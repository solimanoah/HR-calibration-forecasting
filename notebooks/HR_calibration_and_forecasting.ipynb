{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HR Calibration & Forecasting using PPG-Dalia Dataset\n",
        "\n",
        "Training a model that can predict an individual's ECG-measured HR based on signals measured by a wrist device. These signals include:\n",
        "- Blood Volume Pulse (BVP)\n",
        "- Accelerometer (ACC)\n",
        "- Skin Temperature (TEMP)\n",
        "- Electrodermal Activity (EDA)\n",
        "\n",
        "We will also train the model to simutaneously predict HR in future periods given current HR trends.\n",
        "\n",
        "Forecasting and calibration functions are achieved due to the model architecture: shared encoder, followed by two heads."
      ],
      "metadata": {
        "id": "TVD5206z0pPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Notes\n",
        "\n",
        "\n",
        "### Data Process:\n",
        "- For every 8-second window of PPG-measured features, there is a corresponding ECG-measured HR.\n",
        "- Our shared encoder will look to filter each window into its final state to allow for optimal data representation for calibration and forecasting.\n",
        "- Our calibration and forecasting heads will then train on these final states via HR and HR at *t+H* predictions, in relation to each window's label.\n",
        "\n",
        "### Window Information:\n",
        "- length: 8s | shift: 2s\n",
        "- BVP: 512 samples per window (64Hz)\n",
        "- ACC: 256 samples per window (32Hz)\n",
        "- TEMP: 32 samples per window (4Hz)\n",
        "- EDA: 32 samples per window (4Hz)\n",
        "\n",
        "We will interpolate ACC, TEMP, and EDA to ensure equal input per feature.\n",
        "\n",
        "### Calibration & Forecasting Labels:\n",
        "- `y_now`\n",
        "- `y_fut` to be calculated by taking corresponding `y_now` + H, where H is number of windows ahead\n",
        "\n",
        "### Activities:\n",
        "- Sitting (ID: 1) used as a motion-artefact-free baseline.\n",
        "- Ascending & descending stairs (ID: 2)\n",
        "- Table soccer (ID: 3)\n",
        "- Driving a car (ID: 5)\n",
        "- Lunch break (ID: 6)\n",
        "- Walking (ID: 7)\n",
        "- Working (ID: 8)\n",
        "- Transient (ID: 0)"
      ],
      "metadata": {
        "id": "C8d6tWcH02s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieving the Data"
      ],
      "metadata": {
        "id": "db9JRrIxz2h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwyKHVYQ0fQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/PPG_DaLia\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# path where PPG_DaLia data resides\n",
        "DATASET_PATH = \"/content/drive/MyDrive/PPG_DaLia/subjects\"\n",
        "\n",
        "# dictionary of subject dictionaries\n",
        "all_subjects = {}\n",
        "\n",
        "for subject_id in range(1, 16):\n",
        "  fname = f\"S{subject_id}.pkl\"\n",
        "  fpath = os.path.join(DATASET_PATH, fname)\n",
        "\n",
        "  if not os.path.exists(fpath):\n",
        "    print(f\"File {fname} missing, skipping...\")\n",
        "    continue\n",
        "\n",
        "\n",
        "  with open(fpath, \"rb\") as f:\n",
        "    data = pickle.load(f, encoding=\"latin1\")\n",
        "\n",
        "  print(f\"{subject_id} extracted\")\n",
        "\n",
        "  # extract features\n",
        "  wrist = data[\"signal\"][\"wrist\"]\n",
        "  bvp = wrist[\"BVP\"]\n",
        "  acc = wrist[\"ACC\"]\n",
        "  eda = wrist[\"EDA\"]\n",
        "  temp = wrist[\"TEMP\"]\n",
        "\n",
        "  # extract label\n",
        "  labels = data[\"label\"]\n",
        "\n",
        "  # store in dict format; convert from list into array\n",
        "  all_subjects[subject_id] = {\n",
        "      \"BVP\": np.array(bvp, dtype=np.float32),\n",
        "      \"ACC\": np.array(acc, dtype=np.float32),\n",
        "      \"EDA\": np.array(eda, dtype=np.float32),\n",
        "      \"TEMP\": np.array(temp, dtype=np.float32),\n",
        "      \"HR_labels\": np.array(labels, dtype=np.float32)\n",
        "  }\n",
        "\n",
        "print(\"Loaded subjects:\", list(all_subjects.keys()))"
      ],
      "metadata": {
        "id": "02aJpBg00Lwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981acbcc-eede-482e-fe6c-fc2d173bf60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 extracted\n",
            "2 extracted\n",
            "3 extracted\n",
            "4 extracted\n",
            "5 extracted\n",
            "6 extracted\n",
            "7 extracted\n",
            "8 extracted\n",
            "9 extracted\n",
            "10 extracted\n",
            "11 extracted\n",
            "12 extracted\n",
            "13 extracted\n",
            "14 extracted\n",
            "15 extracted\n",
            "Loaded subjects: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save `all_subjects` in Drive file for easy accessibility in the future..."
      ],
      "metadata": {
        "id": "K9IUkUFbN9P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/PPG_DaLia/all_subjects.pkl\", \"wb\") as f:\n",
        "  pickle.dump(all_subjects, f)"
      ],
      "metadata": {
        "id": "QJWoHo-OKNL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve `all_subjects` dictionary"
      ],
      "metadata": {
        "id": "4mDjAqmbSKT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/PPG_DaLia/all_subjects.pkl\", \"rb\") as f:\n",
        "  all_subjects = pickle.load(f)"
      ],
      "metadata": {
        "id": "v_4e96t3HEO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolating ACC, TEMP, & EDA\n",
        "\n",
        "- To meet standards set by BVP of 64Hz, therefore 512 values per feature per 8s window."
      ],
      "metadata": {
        "id": "UCnwTq-3ZRE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note to self:\n",
        "- numpy's criteria for a dtype=object is when the array holds elements that aren't uniform (e.g. one element string, another integer), of varying length (e.g. [1,2] followed by a [1,3] array), or are custom python objects"
      ],
      "metadata": {
        "id": "RHONxKOBW9MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "TARGET_FS = 64.0\n",
        "\n",
        "# conversion to float32 numpy numeric array\n",
        "def to_float32_np(x):\n",
        "  \"\"\"\n",
        "  In the case dtype=object and array isn't numeric; must convert to numpy float32 array\n",
        "  \"\"\"\n",
        "  a = np.asarray(x)\n",
        "  # if object dtype, try to stack if it's an array of arrays\n",
        "  if a.dtype == object:\n",
        "    try:\n",
        "      a = np.stack(a.tolist(), axis=0) # convert a into list for which it is then stacked\n",
        "    except Exception:\n",
        "      a = a.astype('float32', copy=False) # in case a is not an array of containers\n",
        "\n",
        "  return a.astype(np.float32, copy=False)\n",
        "\n",
        "# ensure all array shapes (signals) are time-major, meaning that (samples, channels)\n",
        "def make_time_major(arr, expect_channels=None):\n",
        "  \"\"\"\n",
        "  Ensure array is time-major\n",
        "  - 1D -> (N, 1)\n",
        "  - If 2D and looks like (C, N) due to C being relatively small, transpose to (N, C)\n",
        "  \"\"\"\n",
        "  a = to_float32_np(arr)\n",
        "  # conditions to determine array shape\n",
        "  if a.ndim == 1:\n",
        "    a = a[:, None] # (N,) -> (N, 1)\n",
        "  elif a.ndim == 2:\n",
        "    N0, N1 = a.shape\n",
        "    # if first dim is small and second much larger, transpose to meet (N, C) requirements\n",
        "    if N0 <= 4 and N1 > N0:\n",
        "      a = a.T\n",
        "\n",
        "  else:\n",
        "    a = a.reshape(a.shape[0], -1) # if array has > 2 dimensions, reshape to (C, all other dims)\n",
        "\n",
        "  # in case where expected_channels of an array doesn't match\n",
        "  if expect_channels is not None and a.shape[1] != expect_channels:\n",
        "    pass # no hard fail; may want to add in logs\n",
        "\n",
        "  return a # (N, C)\n",
        "\n",
        "def interp_to_target_1d(x_1d, in_fs, target_len, target_fs=TARGET_FS):\n",
        "  \"\"\"\n",
        "  Using target_len as reference, interpolate x to ensure 512 values\n",
        "  per feature per window\n",
        "  \"\"\"\n",
        "  # ensure 1-D arrays\n",
        "  x = to_float32_np(x_1d).reshape(-1)\n",
        "\n",
        "  # window length\n",
        "  duration = target_len / target_fs\n",
        "\n",
        "  # set desired distribution\n",
        "  t_out = np.linspace(0.0, duration, target_len, endpoint=False)\n",
        "  # set current distribution on target\n",
        "  t_in = np.linspace(0.0, duration, len(x), endpoint=False)\n",
        "\n",
        "  # interpolate x to match distribution\n",
        "  return np.interp(t_out, t_in, x).astype(np.float32)\n",
        "\n",
        "def resample_subject_to_64hz(subj):\n",
        "  \"\"\"\n",
        "  Uses interp_to_target_1d() to interpolate ACC, EDA, and TEMP\n",
        "  \"\"\"\n",
        "  # convert object into np.float32 as tensor prep\n",
        "  bvp = to_float32_np(subj[\"BVP\"]).reshape(-1)\n",
        "  target_len = len(bvp)\n",
        "\n",
        "  # ACC - could be in format (N,3), (3,N), (N,) ---> (256, 3)\n",
        "  acc_raw = make_time_major(subj[\"ACC\"])\n",
        "  C = acc_raw.shape[1]\n",
        "\n",
        "  acc_64 = np.stack(\n",
        "      [interp_to_target_1d(acc_raw[:, i], in_fs=32.0, target_len=target_len) for i in range(C)],\n",
        "      axis=1\n",
        "  )\n",
        "\n",
        "  # EDA\n",
        "  eda_raw = to_float32_np(subj[\"EDA\"]).reshape(-1)\n",
        "  eda_64 = interp_to_target_1d(eda_raw, in_fs=4.0, target_len=target_len)\n",
        "\n",
        "  # TEMP\n",
        "  temp_raw = to_float32_np(subj[\"TEMP\"]).reshape(-1)\n",
        "  temp_64 = interp_to_target_1d(temp_raw, in_fs=4.0, target_len=target_len)\n",
        "\n",
        "  # updated dictionary with interpolated values\n",
        "  return {\n",
        "      \"BVP\": bvp.astype(np.float32),\n",
        "      \"ACC\": acc_64.astype(np.float32),\n",
        "      \"EDA\": eda_64.astype(np.float32),\n",
        "      \"TEMP\": temp_64.astype(np.float32),\n",
        "      \"HR_labels\": to_float32_np(subj[\"HR_labels\"]).reshape(-1)\n",
        "  }\n",
        "\n",
        "# create a new dict of similar format, just with interpolated signals\n",
        "all_subjects_64hz = {}\n",
        "\n",
        "for sid, subj in all_subjects.items():\n",
        "  try:\n",
        "    out = resample_subject_to_64hz(subj)\n",
        "    all_subjects_64hz[sid] = out\n",
        "    # shape of signal arrays\n",
        "    print(f\"S{sid}: BVP {out['BVP'].shape}, ACC {out['ACC'].shape}, \"\n",
        "          f\"EDA {out['EDA'].shape}, TEMP {out['TEMP'].shape}, HR {out['HR_labels'].shape}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Resample failed for subject {sid}: {e}\")"
      ],
      "metadata": {
        "id": "BGY9lOfkSeK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f596def-fb21-4969-8079-9ca9fffce93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1: BVP (589568,), ACC (589568, 3), EDA (589568,), TEMP (589568,), HR (4603,)\n",
            "S2: BVP (525120,), ACC (525120, 3), EDA (525120,), TEMP (525120,), HR (4099,)\n",
            "S3: BVP (559424,), ACC (559424, 3), EDA (559424,), TEMP (559424,), HR (4367,)\n",
            "S4: BVP (585600,), ACC (585600, 3), EDA (585600,), TEMP (585600,), HR (4572,)\n",
            "S5: BVP (595520,), ACC (595520, 3), EDA (595520,), TEMP (595520,), HR (4649,)\n",
            "S6: BVP (336000,), ACC (336000, 3), EDA (336000,), TEMP (336000,), HR (2622,)\n",
            "S7: BVP (597952,), ACC (597952, 3), EDA (597952,), TEMP (597952,), HR (4668,)\n",
            "S8: BVP (517120,), ACC (517120, 3), EDA (517120,), TEMP (517120,), HR (4037,)\n",
            "S9: BVP (547840,), ACC (547840, 3), EDA (547840,), TEMP (547840,), HR (4277,)\n",
            "S10: BVP (681472,), ACC (681472, 3), EDA (681472,), TEMP (681472,), HR (5321,)\n",
            "S11: BVP (579072,), ACC (579072, 3), EDA (579072,), TEMP (579072,), HR (4521,)\n",
            "S12: BVP (506496,), ACC (506496, 3), EDA (506496,), TEMP (506496,), HR (3954,)\n",
            "S13: BVP (584704,), ACC (584704, 3), EDA (584704,), TEMP (584704,), HR (4565,)\n",
            "S14: BVP (573312,), ACC (573312, 3), EDA (573312,), TEMP (573312,), HR (4476,)\n",
            "S15: BVP (508096,), ACC (508096, 3), EDA (508096,), TEMP (508096,), HR (3966,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, signal data is currently not split into its respective windows, which needs to be done for corresponding HR values."
      ],
      "metadata": {
        "id": "oJgUQidyu1Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val/Test Split\n",
        "\n",
        "### Leave-One-Subject-Out (LOSO)\n",
        "\n",
        "- With the split, we will take a LOSO approach, for which we train/val/test as many times as there are subjects.\n",
        "- Each fold (run) will have a different subject as the test set, so we can get a good understanding of the model's generalizability - e.g. to fit/unfit men/women.\n",
        "- Average performance over all folds.\n",
        "\n",
        "### Validation Set\n",
        "\n",
        "- The validation set will contain the final 20% of windows for every activity for every subject (except for testing subject).\n",
        "- This ensures variety in the HR of the validation set, while also maintaining chronology.\n",
        "- An embargo of 8 seconds (1 window) will be included between the train and validation data to prevent overlap leakage."
      ],
      "metadata": {
        "id": "_XeYGeS1U8Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup\n",
        "\n",
        "### Signal Reformatting, Normalization, and Window Formation"
      ],
      "metadata": {
        "id": "7FCdBNJzufAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "FS = 64\n",
        "WIN_S = 8\n",
        "SHIFT_S = 2 # with every window shifting 2 seconds, 6 out 8 seconds (therefore samples) will overlap with previous window\n",
        "T = WIN_S * FS # 512 samples per window\n",
        "STRIDE = SHIFT_S * FS # number of unique samples per window relative to previous window\n",
        "\n",
        "# feature order used everywhere\n",
        "FEATURE_ORDER = [\"BVP\", \"ACCx\", \"ACCy\", \"ACCz\", \"TEMP\", \"EDA\"]\n",
        "\n",
        "\n",
        "# reformats each subject dictionary so all signals are within a single array of shape [N, 6]\n",
        "def assemble_features_subject(subj_dict):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "  X_cont: (N, F=6) in FEATURE_ORDER\n",
        "  labels: (M,) average ECG-HR per 8s window\n",
        "  \"\"\"\n",
        "  # reshape to 1D and retrieve row shape for N\n",
        "  bvp = subj_dict[\"BVP\"].astype(np.float32).reshape(-1)\n",
        "  N = bvp.shape[0]\n",
        "\n",
        "  # ACC - (N,) OR (N, 1) --> (N, 3)\n",
        "  acc = subj_dict[\"ACC\"].astype(np.float32)\n",
        "\n",
        "  if acc.ndim == 1:\n",
        "    acc = acc[:, None]\n",
        "  if acc.shape[1] == 1:\n",
        "    acc = np.repeat(acc, 3, axis=1)\n",
        "  # case if neither\n",
        "  elif acc.shape[1] != 3:\n",
        "    raise ValueError(f\"ACC has {acc.shape[1]} channels; expected 1 or 3\")\n",
        "\n",
        "  # for TEMP and EDA\n",
        "  temp = subj_dict[\"TEMP\"].astype(np.float32).reshape(-1)\n",
        "  eda = subj_dict[\"EDA\"].astype(np.float32).reshape(-1)\n",
        "\n",
        "  # sanity check\n",
        "  assert acc.shape[0] == N == temp.shape[0] == eda.shape[0], \"Signal length mismatch\"\n",
        "\n",
        "  # stack to form (N, 6)\n",
        "  X_cont = np.stack([bvp, acc[:, 0], acc[:, 1], acc[:, 2], temp, eda], axis=1)\n",
        "\n",
        "  # labels to (M,)\n",
        "  labels = subj_dict[\"HR_labels\"].astype(np.float32).reshape(-1)\n",
        "\n",
        "  return X_cont, labels\n",
        "\n",
        "# compute stats for normalization from training subjects only\n",
        "def compute_norm_stats(train_subject_ids, all_subjects_64hz):\n",
        "  \"\"\"Returns: mean (F,), std (F,) for each feature\"\"\"\n",
        "  feats_list = []\n",
        "  # extract signal data for every subject\n",
        "  for sid in train_subject_ids:\n",
        "    X_cont, _ = assemble_features_subject(all_subjects_64hz[sid])\n",
        "    feats_list.append(X_cont)\n",
        "\n",
        "  # combine all signal data across all subjects and then calculate stats\n",
        "  big = np.concatenate(feats_list, axis=0) # (sumN, F)\n",
        "  mean = big.mean(axis=0)\n",
        "  std = big.std(axis=0)\n",
        "  std = np.where(std < 1e-6, 1.0, std) # avoid division by zero during normalization\n",
        "\n",
        "  return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "# normalization\n",
        "def apply_norm(X_cont, mean, std):\n",
        "  return ((X_cont - mean) / std).astype(np.float32)\n",
        "\n",
        "# 8s windows are created by taking the final 6s of the previous window, and adding 2s more\n",
        "def make_windows_for_subject(X_cont, labels, H_segments=1):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "  Xw: all windows stacked in (no. windows, 512, 6) shape\n",
        "  y_now: all corresponding HRs for every window (no. windows,)\n",
        "  y_fut: all corresponding future HRs for all compatible windows (no. compatible windows,)\n",
        "  \"\"\"\n",
        "  # number of windows expected\n",
        "  M = len(labels)\n",
        "  num_seg = M\n",
        "\n",
        "  # windows for signal data, HR data, and future HR data\n",
        "  Xw = []\n",
        "  y_now = []\n",
        "  y_fut = []\n",
        "\n",
        "  # for every window k...\n",
        "  for k in range(num_seg):\n",
        "\n",
        "    # define the data range for one window\n",
        "    start = int(k * STRIDE)\n",
        "    end = start + T\n",
        "\n",
        "    if end > X_cont.shape[0]:\n",
        "      break # unlikely to happen as num_seg should correspond with signal data\n",
        "\n",
        "    # forecasting criteria - final few windows not included due to t + H not existing in labels\n",
        "    if (k + H_segments) >= M:\n",
        "      break\n",
        "\n",
        "    # addition of data for each window\n",
        "    Xw.append(X_cont[start:end, :]) # (512, F)\n",
        "    y_now.append(labels[k]) # ECG-HR @ k\n",
        "    y_fut.append(labels[k + H_segments]) # ECG-HR @ segment k+H\n",
        "\n",
        "  # format\n",
        "  Xw = np.stack(Xw, axis=0).astype(np.float32) # (num_kept, 512, 6)\n",
        "  y_now = np.array(y_now, dtype=np.float32) # (num_kept,)\n",
        "  y_fut = np.array(y_fut, dtype=np.float32) # (num_kept,)\n",
        "\n",
        "  return Xw, y_now, y_fut"
      ],
      "metadata": {
        "id": "MO07GOyvOMs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV Extraction and Activity Masks"
      ],
      "metadata": {
        "id": "0O4TXX28rchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# extract per-activity window information for each subject\n",
        "def load_activity_markers(csv_path):\n",
        "  \"\"\"\n",
        "  Returns a list of (name, start_idx) sorted by start_idx, where 'name' is the activity\n",
        "  and 'start_idx' is the window for which the activity begins.\n",
        "  \"\"\"\n",
        "  markers = []\n",
        "  # read path as csv\n",
        "  with open(csv_path, \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "\n",
        "    for row in reader:\n",
        "      if not row or len(row) < 2:\n",
        "        continue\n",
        "\n",
        "      # extract activity (or subject if row 0)\n",
        "      name = str(row[0]).strip()\n",
        "\n",
        "      # skip if row 0\n",
        "      if \"SUBJECT\" in name.upper():\n",
        "        continue\n",
        "\n",
        "      # extract start window\n",
        "      try:\n",
        "        start_idx = int(str(row[1]).strip()) # this would only work if only integer was in row[1]\n",
        "      except ValueError:\n",
        "        # in case in format \"NAME, 1234\" in row[1]\n",
        "        parts = name.split(\",\")\n",
        "        if len(parts) == 2:\n",
        "          name, start_idx = parts[0].strip(), int(parts[1].strip())\n",
        "        else:\n",
        "          continue # ignore otherwise\n",
        "\n",
        "      # normalize name\n",
        "      if name.startswith(\"#\"):\n",
        "        name = name[1:].strip() # remove #\n",
        "\n",
        "      markers.append((name, start_idx))\n",
        "\n",
        "  # keep and store only rows that have numeric start indices\n",
        "  markers = [(n, i) for (n, i) in markers if isinstance(i, int)]\n",
        "  markers.sort(key=lambda x: x[1])\n",
        "  return markers\n",
        "\n",
        "# assign windows either train, validation, or embargo\n",
        "def build_activity_masks_from_markers(markers, num_segments, val_frac=0.2, embargo_segments=4):\n",
        "  \"\"\"\n",
        "  Returns: train_mask (num_segments,) (boolean), val_mask (num_segments,) (boolean), per_seg_activity (num_segments,) (activity)\n",
        "  \"\"\"\n",
        "  # set up arrays that will hold train/val/activity information per window\n",
        "  train_mask = np.ones(num_segments, dtype=bool)\n",
        "  val_mask = np.zeros(num_segments, dtype=bool)\n",
        "  per_seg_activity = np.array([\"UNKNOWN\"] * num_segments, dtype=object)\n",
        "\n",
        "  # determine start and end window for all activities\n",
        "  for j, (name, start_k) in enumerate(markers):\n",
        "    # the end window is the next marker's start window (assuming a window is available)\n",
        "    end_k = markers[j+1][1] if (j + 1) < len(markers) else num_segments\n",
        "    start_k = max(0, min(start_k, num_segments))\n",
        "    end_k = max(0, min(end_k, num_segments))\n",
        "    # in case of malfunction\n",
        "    if end_k <= start_k:\n",
        "      continue\n",
        "\n",
        "    # set up index range for corresponding activity windows\n",
        "    idx = np.arange(start_k, end_k)\n",
        "\n",
        "    # fill corresponding windows with activity name\n",
        "    per_seg_activity[idx] = name\n",
        "\n",
        "    # extract validation (final 20%) of windows\n",
        "    n = len(idx)\n",
        "    n_val = max(1, int(np.floor(val_frac * n)))\n",
        "    val_idx = idx[-n_val:]\n",
        "\n",
        "    # determine embargo windows; final 4 (default) windows of training data\n",
        "    emb_start = max(start_k, end_k - n_val - embargo_segments)\n",
        "    emb_idx = np.arange(emb_start, end_k - n_val) if (end_k - n_val) > emb_start else np.array([], dtype=int)\n",
        "\n",
        "    # apply masks\n",
        "    val_mask[val_idx] = True\n",
        "    train_mask[val_idx] = False\n",
        "    train_mask[emb_idx] = False\n",
        "\n",
        "  return train_mask, val_mask, per_seg_activity"
      ],
      "metadata": {
        "id": "r--cjnEnrmmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LOSO Fold Builder"
      ],
      "metadata": {
        "id": "aUp6QWS34al_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# build loso fold builder by: splitting train & test subjects, computing norm for both,\n",
        "# assigning train/val masks for each activity per subject, and then splitting accordingly\n",
        "# and into tensors and loaders\n",
        "def build_loso_fold_with_activity(\n",
        "    all_subjects_64hz,\n",
        "    activity_dir, # location SX_activity.csv file\n",
        "    test_sid,\n",
        "    H_segments=30, # prediction 60s ahead\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256,\n",
        "    num_workers=2,\n",
        "    pin_memory=True):\n",
        "\n",
        "  # order subject_ids and filter out test subject\n",
        "  subject_ids = sorted(all_subjects_64hz.keys())\n",
        "  train_sids = [s for s in subject_ids if s != test_sid]\n",
        "\n",
        "  # normalization stats for train subjects\n",
        "  mean, std = compute_norm_stats(train_sids, all_subjects_64hz)\n",
        "\n",
        "  # reformat test subject to (N, 6) and (M,)\n",
        "  Xc_test, lab_test = assemble_features_subject(all_subjects_64hz[test_sid])\n",
        "  Xc_test = apply_norm(Xc_test, mean, std)\n",
        "  # reformat to (no. windows, 512, 6), (no. windows,), (no. compatible windows,)\n",
        "  Xw_test, y_now_test, y_fut_test = make_windows_for_subject(Xc_test, lab_test, H_segments)\n",
        "\n",
        "  # number of windows (segments) to be embargoed for 8 second embargo\n",
        "  embargo_segments = int(np.ceil(embargo_seconds / SHIFT_S))\n",
        "\n",
        "  # train and val data following 20% split\n",
        "  X_train_list, y_now_train_list, y_fut_train_list = [], [], []\n",
        "  X_val_list, y_now_val_list, y_fut_val_list, = [], [], []\n",
        "\n",
        "  for sid in train_sids:\n",
        "    Xc, lab = assemble_features_subject(all_subjects_64hz[sid]) # to (N, 6) and (M,)\n",
        "    Xc = apply_norm(Xc, mean, std)\n",
        "    Xw, y_now, y_fut = make_windows_for_subject(Xc, lab, H_segments) # to (no. windows, 512, 6)\n",
        "    num_seg = len(y_now) # total windows\n",
        "\n",
        "    # extract and assign activity markers\n",
        "    act_csv = os.path.join(activity_dir, f\"S{sid}_activity.csv\")\n",
        "    markers = load_activity_markers(act_csv)\n",
        "    tr_mask, va_mask, _ = build_activity_masks_from_markers(\n",
        "        markers, num_segments=num_seg, val_frac=val_frac, embargo_segments=embargo_segments\n",
        "    )\n",
        "\n",
        "    # add train and validation segments to respective array\n",
        "    X_train_list.append(Xw[tr_mask]); y_now_train_list.append(y_now[tr_mask]); y_fut_train_list.append(y_fut[tr_mask])\n",
        "    X_val_list.append(Xw[va_mask]); y_now_val_list.append(y_now[va_mask]); y_fut_val_list.append(y_fut[va_mask])\n",
        "\n",
        "  # concatenate all subjects' train and validation segments along rows\n",
        "  def _catX(lst): return np.concatenate(lst, axis=0).astype(np.float32) if len(lst) else np.empty((0, T, len(FEATURE_ORDER)), np.float32)\n",
        "  def _caty(lst): return np.concatenate(lst, axis=0).astype(np.float32) if len(lst) else np.empty((0,), np.float32)\n",
        "\n",
        "  X_train = _catX(X_train_list); y_now_train = _caty(y_now_train_list); y_fut_train = _caty(y_fut_train_list)\n",
        "  X_val = _catX(X_val_list); y_now_val = _caty(y_now_val_list); y_fut_val = _caty(y_fut_val_list)\n",
        "\n",
        "  # tensors\n",
        "  train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_now_train), torch.from_numpy(y_fut_train))\n",
        "  val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_now_val), torch.from_numpy(y_fut_val))\n",
        "  test_ds = TensorDataset(torch.from_numpy(Xw_test), torch.from_numpy(y_now_test), torch.from_numpy(y_fut_test))\n",
        "\n",
        "  # loaders\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_train, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory)\n",
        "  val_loader = DataLoader(val_ds, batch_size=batch_eval, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "  test_loader = DataLoader(test_ds, batch_size=batch_eval, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "  print(f\"[Fold S{test_sid}] Train {len(train_ds)} | Val {len(val_ds)} | Test {len(test_ds)} | H={H_segments} seg\")\n",
        "\n",
        "  return {\n",
        "      \"mean\": mean, \"std\": std, \"feature_order\": FEATURE_ORDER,\n",
        "      \"train_loader\": train_loader, \"val_loader\": val_loader, \"test_loader\": test_loader,\n",
        "      \"train\": tuple(t for t in train_ds.tensors),\n",
        "      \"val\": tuple(t for t in val_ds.tensors),\n",
        "      \"test\": tuple(t for t in test_ds.tensors)\n",
        "  }"
      ],
      "metadata": {
        "id": "96OX4-XR4eLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model\n",
        "\n",
        "### GRU (shared encoder) --> Head A (calibration) | Head B (forecasting) --> `y_now` | `y_fut`\n",
        "\n",
        "Shared encoder uses a GRU architecture due to the sequential nature of the windows (512 recordings per window). Patterns discovered over that 512-sample timeframe are crucial for determining the `y_now` and the `y_fut`.\n",
        "\n",
        "- calibration (`y_now`) requires GRU due to every sample representing 1/64 of a second, which isn't enough to determine HR at a single point in time.\n",
        "- forecasting (`y_fut`) requires GRU due to the identification of a trend within the 8-second window to then predict future HR.\n",
        "\n",
        "It is the responsibility of the shared encoder to include the best information in the hidden states to allow for the two heads to derive useful patterns."
      ],
      "metadata": {
        "id": "J5J81pYaiHXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiTaskGRU(nn.Module):\n",
        "  \"\"\"\n",
        "  Shared GRU encoder + two regression heads (calibration & forecasting)\n",
        "\n",
        "  Input: x of shape (B, T=512, F=6)\n",
        "  Output: y_now (B,), y_fut (B,)\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int = 6,\n",
        "      hidden_size: int = 96,\n",
        "      num_layers: int = 2,\n",
        "      dropout: float = 0.2,\n",
        "      bidirectional: bool = False,\n",
        "      head_hidden: int = 64,\n",
        "      head_dropout: float = 0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.bidirectional = bidirectional\n",
        "    self.dir_mult = 2 if bidirectional else 1\n",
        "    enc_out_dim = hidden_size * self.dir_mult\n",
        "\n",
        "    # shared encoder\n",
        "    self.gru = nn.GRU(\n",
        "        input_size=input_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout if num_layers > 1 else 0.0,\n",
        "        batch_first=True,\n",
        "        bidirectional=bidirectional\n",
        "    )\n",
        "\n",
        "    # head constructor\n",
        "    def make_head():\n",
        "      if head_hidden and head_hidden > 0:\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(enc_out_dim, head_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(head_dropout),\n",
        "            nn.Linear(head_hidden, 1)\n",
        "        )\n",
        "      else:\n",
        "        return nn.Linear(enc_out_dim, 1)\n",
        "\n",
        "    # creation of calibration and forecasting heads\n",
        "    self.head_now = make_head()\n",
        "    self.head_fut = make_head()\n",
        "\n",
        "    # initialize parameters to 0 to prevent early instability\n",
        "    nn.init.zeros_(self.head_now[-1].weight if isinstance(self.head_now, nn.Sequential) else self.head_now.weight)\n",
        "    nn.init.zeros_(self.head_now[-1].bias if isinstance(self.head_now, nn.Sequential) else self.head_now.bias)\n",
        "    nn.init.zeros_(self.head_fut[-1].weight if isinstance(self.head_fut, nn.Sequential) else self.head_fut.weight)\n",
        "    nn.init.zeros_(self.head_fut[-1].bias if isinstance(self.head_fut, nn.Sequential) else self.head_fut.bias)\n",
        "\n",
        "  def forward(self, x, return_hidden: bool = False):\n",
        "    \"\"\"\n",
        "    x: (B, T, F)\n",
        "    Return: y_now, y_fut, (+ h_last if return_hidden)\n",
        "    \"\"\"\n",
        "    # GRU (B, T, F) --> (B, H*dir)\n",
        "\n",
        "    # GRU outputs: out (B, T, H*dir), h_n (num_layers*dir, B, H)\n",
        "    out, h_n = self.gru(x)\n",
        "\n",
        "    # h_last to retrieve last hidden state for every window\n",
        "    h_last = h_n[self.dir_mult:, :, :] # (dir, B, H)\n",
        "    h_last = h_last.transpose(0, 1).reshape(x.size(0), -1) # (B, H*dir)\n",
        "\n",
        "    # Heads (B, H*dir) --> (B,) and (B,,)\n",
        "\n",
        "    y_now = self.head_now(h_last).squeeze(-1)\n",
        "    y_fut = self.head_fut(h_last).squeeze(-1)\n",
        "\n",
        "    if return_hidden:\n",
        "      return y_now, y_fut, h_last\n",
        "    return y_now, y_fut"
      ],
      "metadata": {
        "id": "jX_vHkHci1ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss, Optimizer, and Training Loop\n",
        "\n",
        "Techniques used:\n",
        "- weight decay: pulls gradients closer to 0 to prevent overfitting\n",
        "- gradient clipping: minimises increase in gradient to prevent gradient explosion\n",
        "- cosine scheduler: beyond warmup, lr decreases per epoch"
      ],
      "metadata": {
        "id": "aZEaoqzst9Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainConfig"
      ],
      "metadata": {
        "id": "oRks5Z3_y9dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# hold all important info\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "  lr: float = 1e-3\n",
        "  weight_decay: float = 1e-4 # strength of 'pull' of gradients towards 0\n",
        "  lambda_fut: float = 1.0\n",
        "  max_epochs: int = 40\n",
        "  grad_clip: float = 1.0 # minimises increase in gradient to prevent grad explosion\n",
        "  use_amp: bool = True # faster training on gpu due to FP16 and FP32 operations\n",
        "  early_stopping: bool = False\n",
        "  patience: int = 6\n",
        "  scheduler: str = \"cosine\" # lr gradually decreases per epoch\n",
        "  warmup_epochs: int = 2\n",
        "  step_size: int = 15 # lr decreases by x0.1 per 15 epochs\n",
        "  gamma: float = 0.1\n",
        "  ckpt_dir: str = \"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        "  device: str = \"cuda\" if torch.cuda.is_available else \"cpu\""
      ],
      "metadata": {
        "id": "rao53a5ortpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAE Loss (per head)\n",
        "\n",
        "L = L_now + Î» * L_fut"
      ],
      "metadata": {
        "id": "AS2A8wNRzBTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = nn.L1Loss()\n",
        "\n",
        "def multitask_loss(y_now_pred, y_fut_pred, y_now_true, y_fut_true, lambda_fut=1.0):\n",
        "  l_now = mae(y_now_pred, y_now_true)\n",
        "  l_fut = mae(y_fut_pred, y_fut_true)\n",
        "  return l_now + lambda_fut * l_fut, l_now.item(), l_fut.item()"
      ],
      "metadata": {
        "id": "55Py2RACzDHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scheduler\n",
        "\n",
        "Cosine, Step, or None"
      ],
      "metadata": {
        "id": "QTsAYedh1VG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_scheduler(optimizer, cfg: TrainConfig, steps_per_epoch: int):\n",
        "  sched_main = None\n",
        "\n",
        "  # cosine\n",
        "  if cfg.scheduler == \"cosine\":\n",
        "    sched_main = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=cfg.max_epochs\n",
        "    )\n",
        "\n",
        "  # step\n",
        "  elif cfg.scheduler == \"step\":\n",
        "    sched_main = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=cfg.step_size, gamma=cfg.gamma\n",
        "    )\n",
        "\n",
        "  # none\n",
        "  elif cfg.scheduler == \"none\":\n",
        "    sched_main = None\n",
        "\n",
        "  else:\n",
        "    raise ValueError(\"scheduler must be one of: 'cosine', 'step', 'none'\")\n",
        "\n",
        "  # lr during warmup\n",
        "  if cfg.warmup_epochs and cfg.warmup_epochs > 0:\n",
        "    # how many potential updates under 'warmup'\n",
        "    total_warm = cfg.warmup_epochs * steps_per_epoch\n",
        "\n",
        "    # LR calculation\n",
        "    def lr_lambda(step):\n",
        "      if step >= total_warm:\n",
        "        return 1.0 # 100% of base LR\n",
        "      return max(1e-8, (step + 1) / float(total_warm)) # proportion of base LR given current step\n",
        "\n",
        "    sched_warm = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "    return sched_warm, sched_main\n",
        "  else:\n",
        "    return None, sched_main"
      ],
      "metadata": {
        "id": "G_7ToSDv1aj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop"
      ],
      "metadata": {
        "id": "TGPOIv2O8lfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, scaler, cfg: TrainConfig):\n",
        "  model.train()\n",
        "  total, sum_loss, sum_now, sum_fut = 0, 0.0, 0.0, 0.0\n",
        "\n",
        "  for xb, y_now, y_fut in loader:\n",
        "\n",
        "    # device agnostic; non_blocking speeds up CPU -> GPU transfer\n",
        "    xb = xb.to(cfg.device, non_blocking=True)\n",
        "    y_now = y_now.to(cfg.device, non_blocking=True)\n",
        "    y_fut = y_fut.to(cfg.device, non_blocking=True)\n",
        "\n",
        "    # zero grad\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # FP16 and FP32 operations\n",
        "    if cfg.use_amp:\n",
        "      with autocast():\n",
        "        # forward\n",
        "        y_now_pred, y_fut_pred = model(xb)\n",
        "        # loss\n",
        "        loss, l_now, l_fut = multitask_loss(y_now_pred, y_fut_pred, y_now, y_fut, cfg.lambda_fut)\n",
        "\n",
        "      # scale gradients up through scaled loss due to FP16 having a worse limit on small numbers\n",
        "      # backprop\n",
        "      scaler.scale(loss).backward()\n",
        "\n",
        "      # bring gradients back down to true scale before clipping (now on FP32)\n",
        "      if cfg.grad_clip is not None:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "\n",
        "      # gradient descent\n",
        "      scaler.step(optimizer)\n",
        "\n",
        "      # adjust scaling factor, assuming gradients are still finite (not NaN or Inf)\n",
        "      scaler.update()\n",
        "\n",
        "    # F32 operations\n",
        "    else:\n",
        "      # forward\n",
        "      y_now_pred, y_fut_pred = model(xb)\n",
        "      # loss\n",
        "      loss, l_now, l_fut = multitask_loss(y_now_pred, y_fut_pred, y_now, y_fut, cfg.lambda_fut)\n",
        "      # backprop\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping\n",
        "      if cfg.grad_clip is not None:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), cfg.grad_clip)\n",
        "\n",
        "      # gradient descent\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    bs = xb.size(0) # number of windows (batch size)\n",
        "    total += bs\n",
        "    sum_loss += loss.item() * bs # as loss takes average, we multiply by no. windows\n",
        "    sum_now += l_now * bs\n",
        "    sum_fut += l_fut * bs\n",
        "\n",
        "  return {\n",
        "      \"loss\": sum_loss / max(1, total),\n",
        "      \"mae_now\": sum_now / max(1, total),\n",
        "      \"mae_fut\": sum_fut / max(1, total)\n",
        "  }"
      ],
      "metadata": {
        "id": "1nLZY1lS8naF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Loop"
      ],
      "metadata": {
        "id": "6t4aeIkTWpX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_one_epoch(model, loader, cfg: TrainConfig):\n",
        "  model.eval()\n",
        "  total, sum_loss, sum_now, sum_fut = 0, 0.0, 0.0, 0.0\n",
        "\n",
        "  for xb, y_now, y_fut in loader:\n",
        "\n",
        "    # device agnostic\n",
        "    xb = xb.to(cfg.device, non_blocking=True)\n",
        "    y_now = y_now.to(cfg.device, non_blocking=True)\n",
        "    y_fut = y_fut.to(cfg.device, non_blocking=True)\n",
        "\n",
        "    # forward\n",
        "    y_now_pred, y_fut_pred = model(xb)\n",
        "    # loss\n",
        "    loss, l_now, l_fut = multitask_loss(y_now_pred, y_fut_pred, y_now, y_fut, cfg.lambda_fut)\n",
        "\n",
        "    bs = xb.size(0) # number of windows (batch size)\n",
        "    total += bs\n",
        "    sum_loss += loss.item() * bs # as loss takes average, we multiply by no. windows\n",
        "    sum_now += l_now * bs\n",
        "    sum_fut += l_fut * bs\n",
        "\n",
        "  return {\n",
        "      \"loss\": sum_loss / max(1, total),\n",
        "      \"mae_now\": sum_now / max(1, total),\n",
        "      \"mae_fut\": sum_fut / max(1, total)\n",
        "  }"
      ],
      "metadata": {
        "id": "0mHgbVr1WrE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOSO Fold\n",
        "\n",
        "15 subjects (as tests) will be looped through, performing `fit_fold()`"
      ],
      "metadata": {
        "id": "aehHAaS1adIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_fold(model, train_loader, val_loader, cfg: TrainConfig, fold_tag=\"Sx\", iterator=0):\n",
        "\n",
        "  # create checkpoint folder\n",
        "  os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
        "\n",
        "  # model and optimizer\n",
        "  model = model.to(cfg.device)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "  scaler = GradScaler(enabled=cfg.use_amp)\n",
        "\n",
        "  # scheduler\n",
        "  steps_per_epoch = max(1, len(train_loader))\n",
        "  warm_sched, main_sched = build_scheduler(optimizer, cfg, steps_per_epoch)\n",
        "  global_step = 0\n",
        "\n",
        "  best_val = float(\"inf\")\n",
        "  best_path = os.path.join(cfg.ckpt_dir, f\"best_{fold_tag}_{iterator}.pt\")\n",
        "\n",
        "  patience_left = cfg.patience\n",
        "\n",
        "  for epoch in range(1, cfg.max_epochs + 1):\n",
        "\n",
        "    # warmup step-level scheduler\n",
        "    if warm_sched is not None:\n",
        "      for _ in range(steps_per_epoch):\n",
        "        warm_sched.step()\n",
        "        global_step += 1\n",
        "\n",
        "    # train and val\n",
        "    tr = train_one_epoch(model, train_loader, optimizer, scaler, cfg)\n",
        "    va = eval_one_epoch(model, val_loader, cfg)\n",
        "\n",
        "    # epoch-level scheduler\n",
        "    if main_sched is not None:\n",
        "      main_sched.step()\n",
        "\n",
        "    # val_MAE\n",
        "    val_metric = va[\"mae_fut\"]\n",
        "    improved = val_metric < best_val - 1e-6\n",
        "    # save and reset patience if improvement\n",
        "    if improved:\n",
        "      best_val = val_metric\n",
        "      torch.save({\"model\": model.state_dict(), \"cfg\": cfg.__dict__}, best_path)\n",
        "      patience_left = cfg.patience\n",
        "\n",
        "    # performance log\n",
        "    print(f\"Epoch {epoch:03d} | {'** best **' if improved else ''} \"\n",
        "          f\"train loss {tr['loss']:.4f} (now {tr['mae_now']:.3f}, fut {tr['mae_fut']:.3f}) | \"\n",
        "          f\"val loss {va['loss']:.4f} (now {va['mae_now']:.3f}, fut {va['mae_fut']:.3f}) | \")\n",
        "\n",
        "    # early stopping\n",
        "    if cfg.early_stopping and (epoch >= cfg.warmup_epochs) and (not improved):\n",
        "      patience_left -= 1\n",
        "      if patience_left <= 0:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "  # load best weights for downstream test\n",
        "  if os.path.exists(best_path):\n",
        "    state = torch.load(best_path, map_location=cfg.device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "  else:\n",
        "    print(\"Warning: best checkpoint not found; using last epoch weights\")\n",
        "\n",
        "  return model, best_val"
      ],
      "metadata": {
        "id": "SuYEBT6RbEOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Folds End-to-End\n",
        "\n",
        "- produce the train/val/test dataloader for a  particular test subject using `build_loso_fold_with_activity()`\n",
        "- instantiate `MultiTaskGRU`\n",
        "- train with `fit_fold()`"
      ],
      "metadata": {
        "id": "fnx0A5RvXNWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (PRACTICE) Running One LOSO Fold [TEST: SUBJECT 1]\n",
        "\n",
        "- Forecasting to be 60 seconds (H_segments=30); good balance between feasibility and usefulness"
      ],
      "metadata": {
        "id": "uzCx92rJbGh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=1,\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=40,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=2,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S1\")\n",
        "print(f\"Best validation forecast MAE (S1 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2TDCF3_XwGB",
        "outputId": "e6ef97de-aa6a-4044-92e4-79af089f77b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S1] Train 47188 | Val 11874 | Test 4573 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-773659923.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 88.7476 (now 43.935, fut 44.813) | val loss 42.0691 (now 21.559, fut 20.510) | \n",
            "Epoch 002 | ** best ** train loss 49.1944 (now 28.575, fut 20.619) | val loss 33.3769 (now 17.212, fut 16.165) | \n",
            "Epoch 003 | ** best ** train loss 34.4689 (now 18.038, fut 16.431) | val loss 31.7877 (now 16.551, fut 15.236) | \n",
            "Epoch 004 | ** best ** train loss 32.1864 (now 16.700, fut 15.487) | val loss 30.1439 (now 15.752, fut 14.391) | \n",
            "Epoch 005 | ** best ** train loss 31.1174 (now 16.179, fut 14.938) | val loss 27.9135 (now 14.604, fut 13.309) | \n",
            "Epoch 006 | ** best ** train loss 29.5468 (now 15.302, fut 14.245) | val loss 25.9939 (now 13.518, fut 12.476) | \n",
            "Epoch 007 |  train loss 27.1259 (now 13.851, fut 13.275) | val loss 26.8550 (now 13.819, fut 13.036) | \n",
            "Epoch 008 | ** best ** train loss 23.3826 (now 11.560, fut 11.822) | val loss 24.3191 (now 12.366, fut 11.953) | \n",
            "Epoch 009 | ** best ** train loss 22.0379 (now 10.885, fut 11.153) | val loss 23.7879 (now 12.294, fut 11.493) | \n",
            "Epoch 010 | ** best ** train loss 20.7458 (now 10.192, fut 10.554) | val loss 22.9266 (now 11.529, fut 11.398) | \n",
            "Epoch 011 |  train loss 19.6690 (now 9.638, fut 10.031) | val loss 23.7296 (now 11.971, fut 11.759) | \n",
            "Epoch 012 | ** best ** train loss 18.9929 (now 9.263, fut 9.730) | val loss 22.4265 (now 11.532, fut 10.894) | \n",
            "Epoch 013 | ** best ** train loss 18.3429 (now 8.939, fut 9.404) | val loss 21.4790 (now 10.885, fut 10.594) | \n",
            "Epoch 014 | ** best ** train loss 17.9393 (now 8.721, fut 9.218) | val loss 21.0387 (now 10.592, fut 10.447) | \n",
            "Epoch 015 |  train loss 17.5274 (now 8.504, fut 9.024) | val loss 21.2386 (now 10.751, fut 10.488) | \n",
            "Epoch 016 | ** best ** train loss 17.1309 (now 8.288, fut 8.843) | val loss 20.6941 (now 10.489, fut 10.205) | \n",
            "Epoch 017 |  train loss 16.8335 (now 8.155, fut 8.678) | val loss 21.2845 (now 10.918, fut 10.366) | \n",
            "Epoch 018 |  train loss 16.6138 (now 8.010, fut 8.604) | val loss 21.3152 (now 10.886, fut 10.429) | \n",
            "Epoch 019 |  train loss 16.3323 (now 7.864, fut 8.468) | val loss 21.4527 (now 10.795, fut 10.658) | \n",
            "Epoch 020 | ** best ** train loss 16.0956 (now 7.760, fut 8.336) | val loss 19.8765 (now 9.998, fut 9.879) | \n",
            "Epoch 021 |  train loss 15.9332 (now 7.674, fut 8.259) | val loss 20.7382 (now 10.668, fut 10.070) | \n",
            "Epoch 022 | ** best ** train loss 15.8169 (now 7.618, fut 8.199) | val loss 20.1175 (now 10.271, fut 9.847) | \n",
            "Epoch 023 |  train loss 15.4060 (now 7.411, fut 7.996) | val loss 20.5807 (now 10.464, fut 10.116) | \n",
            "Epoch 024 |  train loss 15.1359 (now 7.234, fut 7.901) | val loss 20.8165 (now 10.465, fut 10.352) | \n",
            "Epoch 025 |  train loss 15.0519 (now 7.226, fut 7.826) | val loss 20.5918 (now 10.502, fut 10.090) | \n",
            "Epoch 026 |  train loss 14.8784 (now 7.116, fut 7.762) | val loss 20.1035 (now 10.202, fut 9.901) | \n",
            "Epoch 027 |  train loss 14.6866 (now 7.038, fut 7.649) | val loss 20.3114 (now 10.361, fut 9.951) | \n",
            "Epoch 028 |  train loss 14.5768 (now 6.999, fut 7.578) | val loss 20.9483 (now 10.745, fut 10.203) | \n",
            "Epoch 029 |  train loss 14.4503 (now 6.917, fut 7.533) | val loss 20.0564 (now 10.009, fut 10.047) | \n",
            "Epoch 030 |  train loss 14.3111 (now 6.857, fut 7.455) | val loss 20.2113 (now 10.347, fut 9.865) | \n",
            "Epoch 031 | ** best ** train loss 14.1159 (now 6.764, fut 7.352) | val loss 19.6485 (now 9.981, fut 9.667) | \n",
            "Epoch 032 |  train loss 14.1801 (now 6.767, fut 7.413) | val loss 20.0438 (now 10.288, fut 9.756) | \n",
            "Epoch 033 |  train loss 13.9824 (now 6.685, fut 7.297) | val loss 20.1960 (now 10.290, fut 9.906) | \n",
            "Epoch 034 | ** best ** train loss 13.9812 (now 6.665, fut 7.316) | val loss 19.4284 (now 9.813, fut 9.616) | \n",
            "Epoch 035 |  train loss 13.7359 (now 6.548, fut 7.188) | val loss 20.2522 (now 10.502, fut 9.750) | \n",
            "Epoch 036 |  train loss 13.6551 (now 6.515, fut 7.140) | val loss 20.4061 (now 10.417, fut 9.989) | \n",
            "Epoch 037 |  train loss 13.5423 (now 6.487, fut 7.055) | val loss 20.6192 (now 10.506, fut 10.113) | \n",
            "Epoch 038 | ** best ** train loss 13.4885 (now 6.454, fut 7.034) | val loss 19.2846 (now 9.694, fut 9.591) | \n",
            "Epoch 039 |  train loss 13.3662 (now 6.379, fut 6.987) | val loss 20.0635 (now 10.208, fut 9.856) | \n",
            "Epoch 040 | ** best ** train loss 13.3420 (now 6.370, fut 6.972) | val loss 19.7259 (now 10.143, fut 9.583) | \n",
            "Best validation forecast MAE (S1 fold): 9.5832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best val_MAE for S2-15: 19.7259\n",
        "\n",
        "- now val_MAE: 10.143\n",
        "- fut val_MAE: 9.583"
      ],
      "metadata": {
        "id": "3WPMsMx3mHts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run 15 LOSOs w/ 100 epochs\n",
        "\n",
        "notes:\n",
        "- notice from first test subjects that there is overfitting occurring beyond mid-late epochs (50-70).\n",
        "- in response maybe: increase dropout, increase weight decay..."
      ],
      "metadata": {
        "id": "3F8eGo_ZV1oQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 1]\n",
        "\n",
        "Updates:\n",
        "- epochs: 100\n",
        "- warmup_epochs: 5"
      ],
      "metadata": {
        "id": "wVJn6V1Hn7Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=1,\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S1\", iterator=1)\n",
        "print(f\"Best validation forecast MAE (S1 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-A2x8iomXcD",
        "outputId": "e3e9b158-6748-4475-d8f5-31df47bc6a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S1] Train 47188 | Val 11874 | Test 4573 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 150.6533 (now 75.387, fut 75.267) | val loss 99.9169 (now 50.842, fut 49.075) | \n",
            "Epoch 002 | ** best ** train loss 37.9861 (now 18.865, fut 19.121) | val loss 34.0173 (now 17.149, fut 16.869) | \n",
            "Epoch 003 | ** best ** train loss 32.1656 (now 15.800, fut 16.366) | val loss 29.7021 (now 15.078, fut 14.624) | \n",
            "Epoch 004 | ** best ** train loss 26.2636 (now 12.968, fut 13.295) | val loss 25.8470 (now 13.108, fut 12.739) | \n",
            "Epoch 005 | ** best ** train loss 22.4473 (now 11.028, fut 11.419) | val loss 22.8579 (now 11.523, fut 11.335) | \n",
            "Epoch 006 |  train loss 20.8052 (now 10.190, fut 10.615) | val loss 23.2427 (now 11.901, fut 11.342) | \n",
            "Epoch 007 | ** best ** train loss 19.8035 (now 9.646, fut 10.158) | val loss 21.8258 (now 11.143, fut 10.683) | \n",
            "Epoch 008 | ** best ** train loss 18.8489 (now 9.162, fut 9.687) | val loss 21.5510 (now 10.942, fut 10.609) | \n",
            "Epoch 009 | ** best ** train loss 18.1896 (now 8.835, fut 9.355) | val loss 20.5453 (now 10.287, fut 10.258) | \n",
            "Epoch 010 |  train loss 17.9058 (now 8.696, fut 9.210) | val loss 20.7948 (now 10.428, fut 10.367) | \n",
            "Epoch 011 | ** best ** train loss 17.4703 (now 8.461, fut 9.009) | val loss 20.4045 (now 10.275, fut 10.130) | \n",
            "Epoch 012 | ** best ** train loss 17.0862 (now 8.263, fut 8.823) | val loss 19.6581 (now 9.847, fut 9.811) | \n",
            "Epoch 013 |  train loss 16.5445 (now 7.973, fut 8.571) | val loss 20.5201 (now 10.323, fut 10.197) | \n",
            "Epoch 014 | ** best ** train loss 16.0119 (now 7.668, fut 8.344) | val loss 19.3535 (now 9.746, fut 9.607) | \n",
            "Epoch 015 |  train loss 15.6180 (now 7.461, fut 8.157) | val loss 19.9608 (now 9.960, fut 10.001) | \n",
            "Epoch 016 | ** best ** train loss 15.3868 (now 7.325, fut 8.062) | val loss 19.1302 (now 9.624, fut 9.506) | \n",
            "Epoch 017 |  train loss 15.0091 (now 7.130, fut 7.879) | val loss 19.0708 (now 9.499, fut 9.572) | \n",
            "Epoch 018 | ** best ** train loss 14.7979 (now 7.017, fut 7.781) | val loss 18.3891 (now 9.163, fut 9.226) | \n",
            "Epoch 019 |  train loss 14.5518 (now 6.895, fut 7.656) | val loss 19.2183 (now 9.591, fut 9.627) | \n",
            "Epoch 020 |  train loss 14.3467 (now 6.780, fut 7.567) | val loss 18.8832 (now 9.427, fut 9.456) | \n",
            "Epoch 021 | ** best ** train loss 14.1871 (now 6.704, fut 7.483) | val loss 17.8916 (now 8.792, fut 9.099) | \n",
            "Epoch 022 |  train loss 13.9714 (now 6.586, fut 7.385) | val loss 18.1080 (now 8.933, fut 9.175) | \n",
            "Epoch 023 |  train loss 13.8468 (now 6.521, fut 7.325) | val loss 18.7218 (now 9.286, fut 9.436) | \n",
            "Epoch 024 |  train loss 13.7042 (now 6.459, fut 7.245) | val loss 18.1793 (now 8.984, fut 9.196) | \n",
            "Epoch 025 |  train loss 13.5149 (now 6.340, fut 7.175) | val loss 18.5105 (now 9.101, fut 9.410) | \n",
            "Epoch 026 |  train loss 13.4545 (now 6.325, fut 7.130) | val loss 18.7121 (now 9.047, fut 9.665) | \n",
            "Epoch 027 |  train loss 13.2910 (now 6.239, fut 7.052) | val loss 18.9213 (now 9.442, fut 9.479) | \n",
            "Epoch 028 |  train loss 13.1711 (now 6.173, fut 6.998) | val loss 18.3663 (now 9.205, fut 9.162) | \n",
            "Epoch 029 |  train loss 13.1031 (now 6.137, fut 6.966) | val loss 18.5846 (now 9.172, fut 9.413) | \n",
            "Epoch 030 | ** best ** train loss 12.9459 (now 6.064, fut 6.881) | val loss 17.4670 (now 8.468, fut 8.999) | \n",
            "Epoch 031 |  train loss 12.9535 (now 6.075, fut 6.879) | val loss 17.6066 (now 8.472, fut 9.134) | \n",
            "Epoch 032 |  train loss 12.8501 (now 6.011, fut 6.839) | val loss 18.3130 (now 8.864, fut 9.449) | \n",
            "Epoch 033 |  train loss 12.7524 (now 5.957, fut 6.795) | val loss 18.0188 (now 8.700, fut 9.319) | \n",
            "Epoch 034 |  train loss 12.5860 (now 5.879, fut 6.707) | val loss 18.3993 (now 8.953, fut 9.446) | \n",
            "Epoch 035 |  train loss 12.5305 (now 5.851, fut 6.679) | val loss 18.1937 (now 8.754, fut 9.440) | \n",
            "Epoch 036 |  train loss 12.4981 (now 5.810, fut 6.688) | val loss 17.9801 (now 8.794, fut 9.186) | \n",
            "Epoch 037 |  train loss 12.4702 (now 5.830, fut 6.640) | val loss 17.6401 (now 8.582, fut 9.058) | \n",
            "Epoch 038 |  train loss 12.3871 (now 5.786, fut 6.602) | val loss 17.9074 (now 8.663, fut 9.245) | \n",
            "Epoch 039 |  train loss 12.3055 (now 5.767, fut 6.539) | val loss 17.4619 (now 8.388, fut 9.074) | \n",
            "Epoch 040 |  train loss 12.2168 (now 5.714, fut 6.503) | val loss 17.8290 (now 8.608, fut 9.221) | \n",
            "Epoch 041 |  train loss 12.1784 (now 5.669, fut 6.510) | val loss 17.8505 (now 8.596, fut 9.254) | \n",
            "Epoch 042 |  train loss 12.1359 (now 5.649, fut 6.487) | val loss 17.5390 (now 8.432, fut 9.107) | \n",
            "Epoch 043 |  train loss 12.1066 (now 5.655, fut 6.452) | val loss 18.2292 (now 8.860, fut 9.370) | \n",
            "Epoch 044 |  train loss 12.0229 (now 5.616, fut 6.407) | val loss 18.0388 (now 8.826, fut 9.213) | \n",
            "Epoch 045 |  train loss 11.9832 (now 5.590, fut 6.394) | val loss 18.7496 (now 9.150, fut 9.599) | \n",
            "Epoch 046 |  train loss 11.8617 (now 5.516, fut 6.346) | val loss 18.4610 (now 9.100, fut 9.360) | \n",
            "Epoch 047 |  train loss 11.8647 (now 5.503, fut 6.362) | val loss 18.3064 (now 8.799, fut 9.507) | \n",
            "Epoch 048 |  train loss 11.8059 (now 5.484, fut 6.321) | val loss 17.9557 (now 8.613, fut 9.342) | \n",
            "Epoch 049 |  train loss 11.8123 (now 5.513, fut 6.299) | val loss 18.0696 (now 8.581, fut 9.488) | \n",
            "Epoch 050 |  train loss 11.8147 (now 5.496, fut 6.319) | val loss 17.3932 (now 8.255, fut 9.138) | \n",
            "Epoch 051 | ** best ** train loss 11.6988 (now 5.432, fut 6.266) | val loss 17.1195 (now 8.145, fut 8.975) | \n",
            "Epoch 052 |  train loss 11.6039 (now 5.423, fut 6.181) | val loss 18.4173 (now 8.826, fut 9.592) | \n",
            "Epoch 053 |  train loss 11.6603 (now 5.425, fut 6.235) | val loss 18.2261 (now 8.748, fut 9.478) | \n",
            "Epoch 054 |  train loss 11.5532 (now 5.339, fut 6.215) | val loss 18.2084 (now 8.691, fut 9.518) | \n",
            "Epoch 055 |  train loss 11.5161 (now 5.348, fut 6.168) | val loss 17.9916 (now 8.604, fut 9.387) | \n",
            "Epoch 056 |  train loss 11.4614 (now 5.325, fut 6.136) | val loss 17.7936 (now 8.401, fut 9.392) | \n",
            "Epoch 057 |  train loss 11.4318 (now 5.316, fut 6.116) | val loss 18.5796 (now 8.885, fut 9.695) | \n",
            "Epoch 058 |  train loss 11.4231 (now 5.306, fut 6.117) | val loss 17.9490 (now 8.592, fut 9.357) | \n",
            "Epoch 059 |  train loss 11.4168 (now 5.317, fut 6.100) | val loss 18.0644 (now 8.544, fut 9.521) | \n",
            "Epoch 060 |  train loss 11.3007 (now 5.253, fut 6.047) | val loss 17.7529 (now 8.461, fut 9.292) | \n",
            "Epoch 061 |  train loss 11.3131 (now 5.257, fut 6.056) | val loss 17.6372 (now 8.301, fut 9.336) | \n",
            "Epoch 062 |  train loss 11.2620 (now 5.234, fut 6.028) | val loss 18.0730 (now 8.676, fut 9.397) | \n",
            "Epoch 063 |  train loss 11.2029 (now 5.206, fut 5.997) | val loss 17.7641 (now 8.394, fut 9.370) | \n",
            "Epoch 064 |  train loss 11.1992 (now 5.195, fut 6.004) | val loss 17.9898 (now 8.609, fut 9.380) | \n",
            "Epoch 065 |  train loss 11.1406 (now 5.185, fut 5.955) | val loss 17.7038 (now 8.467, fut 9.237) | \n",
            "Epoch 066 |  train loss 11.1496 (now 5.176, fut 5.974) | val loss 18.4083 (now 8.776, fut 9.632) | \n",
            "Epoch 067 |  train loss 11.0797 (now 5.141, fut 5.938) | val loss 17.9470 (now 8.511, fut 9.436) | \n",
            "Epoch 068 |  train loss 11.0703 (now 5.147, fut 5.923) | val loss 17.9155 (now 8.385, fut 9.530) | \n",
            "Epoch 069 |  train loss 11.0408 (now 5.129, fut 5.912) | val loss 17.5972 (now 8.337, fut 9.260) | \n",
            "Epoch 070 |  train loss 10.9267 (now 5.065, fut 5.862) | val loss 17.3523 (now 8.103, fut 9.249) | \n",
            "Epoch 071 |  train loss 10.9089 (now 5.065, fut 5.844) | val loss 18.3339 (now 8.595, fut 9.739) | \n",
            "Epoch 072 |  train loss 10.9034 (now 5.047, fut 5.856) | val loss 18.0698 (now 8.396, fut 9.674) | \n",
            "Epoch 073 |  train loss 10.8851 (now 5.057, fut 5.828) | val loss 17.8024 (now 8.379, fut 9.424) | \n",
            "Epoch 074 |  train loss 10.8958 (now 5.057, fut 5.839) | val loss 17.6313 (now 8.209, fut 9.422) | \n",
            "Epoch 075 |  train loss 10.8265 (now 5.024, fut 5.802) | val loss 17.5430 (now 8.143, fut 9.400) | \n",
            "Epoch 076 |  train loss 10.8231 (now 5.020, fut 5.803) | val loss 17.7859 (now 8.286, fut 9.500) | \n",
            "Epoch 077 |  train loss 10.8442 (now 5.055, fut 5.789) | val loss 18.5082 (now 8.688, fut 9.820) | \n",
            "Epoch 078 |  train loss 10.7565 (now 4.983, fut 5.774) | val loss 17.7403 (now 8.223, fut 9.517) | \n",
            "Epoch 079 |  train loss 10.7596 (now 5.003, fut 5.756) | val loss 17.8174 (now 8.316, fut 9.502) | \n",
            "Epoch 080 |  train loss 10.6375 (now 4.919, fut 5.718) | val loss 18.2014 (now 8.694, fut 9.507) | \n",
            "Epoch 081 |  train loss 10.6219 (now 4.927, fut 5.695) | val loss 17.7431 (now 8.242, fut 9.501) | \n",
            "Epoch 082 |  train loss 10.5860 (now 4.929, fut 5.657) | val loss 17.8988 (now 8.301, fut 9.598) | \n",
            "Epoch 083 |  train loss 10.5820 (now 4.912, fut 5.670) | val loss 18.1366 (now 8.407, fut 9.730) | \n",
            "Epoch 084 |  train loss 10.5946 (now 4.928, fut 5.667) | val loss 18.1957 (now 8.371, fut 9.825) | \n",
            "Epoch 085 |  train loss 10.5972 (now 4.923, fut 5.674) | val loss 18.4606 (now 8.668, fut 9.792) | \n",
            "Epoch 086 |  train loss 10.5313 (now 4.893, fut 5.638) | val loss 18.6107 (now 8.906, fut 9.704) | \n",
            "Epoch 087 |  train loss 10.6010 (now 4.935, fut 5.666) | val loss 18.2224 (now 8.439, fut 9.783) | \n",
            "Epoch 088 |  train loss 10.5113 (now 4.883, fut 5.628) | val loss 18.3154 (now 8.618, fut 9.697) | \n",
            "Epoch 089 |  train loss 10.4497 (now 4.865, fut 5.585) | val loss 17.9690 (now 8.376, fut 9.593) | \n",
            "Epoch 090 |  train loss 10.4988 (now 4.888, fut 5.611) | val loss 18.4831 (now 8.909, fut 9.574) | \n",
            "Epoch 091 |  train loss 10.5377 (now 4.905, fut 5.633) | val loss 18.0098 (now 8.279, fut 9.731) | \n",
            "Epoch 092 |  train loss 10.3941 (now 4.848, fut 5.546) | val loss 18.9638 (now 8.896, fut 10.067) | \n",
            "Epoch 093 |  train loss 10.3904 (now 4.833, fut 5.557) | val loss 18.8824 (now 8.987, fut 9.896) | \n",
            "Epoch 094 |  train loss 10.3707 (now 4.813, fut 5.558) | val loss 18.0812 (now 8.437, fut 9.644) | \n",
            "Epoch 095 |  train loss 10.3345 (now 4.794, fut 5.541) | val loss 17.9582 (now 8.189, fut 9.769) | \n",
            "Epoch 096 |  train loss 10.3149 (now 4.799, fut 5.516) | val loss 18.7467 (now 8.799, fut 9.948) | \n",
            "Epoch 097 |  train loss 10.4235 (now 4.863, fut 5.561) | val loss 18.6229 (now 8.768, fut 9.854) | \n",
            "Epoch 098 |  train loss 10.3105 (now 4.799, fut 5.512) | val loss 17.6222 (now 8.044, fut 9.578) | \n",
            "Epoch 099 |  train loss 10.2610 (now 4.771, fut 5.490) | val loss 18.0492 (now 8.414, fut 9.636) | \n",
            "Epoch 100 |  train loss 10.2622 (now 4.769, fut 5.493) | val loss 18.1055 (now 8.506, fut 9.600) | \n",
            "Best validation forecast MAE (S1 fold): 8.9748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 1\n",
        "\n",
        "Best val_MAE for S2-15: 17.1195 (100 EPOCHS; reached at epoch 51)\n",
        "\n",
        "- now val_MAE: 8.145\n",
        "- fut val_MAE: 8.9748\n",
        "\n",
        "notes:\n",
        "- overfitting occurring beyond epoch 51 (decreasing train loss while stagnating val loss)"
      ],
      "metadata": {
        "id": "36-52DkDOZ9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 2]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "DZB-9lyqT8PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=2, # test: subject 2\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S2\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S2 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7uzkkIV_70",
        "outputId": "2a91b854-ec64-4b51-d05f-cf447ec2ee0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S2] Train 47590 | Val 11976 | Test 4069 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 149.8399 (now 75.048, fut 74.792) | val loss 101.6944 (now 51.827, fut 49.868) | \n",
            "Epoch 002 | ** best ** train loss 40.3009 (now 20.120, fut 20.181) | val loss 33.1882 (now 16.719, fut 16.469) | \n",
            "Epoch 003 | ** best ** train loss 33.2284 (now 16.098, fut 17.130) | val loss 32.2436 (now 16.336, fut 15.908) | \n",
            "Epoch 004 | ** best ** train loss 28.0981 (now 13.724, fut 14.374) | val loss 27.4649 (now 13.805, fut 13.660) | \n",
            "Epoch 005 | ** best ** train loss 25.8786 (now 12.573, fut 13.305) | val loss 25.7128 (now 13.051, fut 12.662) | \n",
            "Epoch 006 | ** best ** train loss 24.0947 (now 11.702, fut 12.393) | val loss 23.6987 (now 11.874, fut 11.825) | \n",
            "Epoch 007 |  train loss 23.5538 (now 11.369, fut 12.185) | val loss 23.8237 (now 11.964, fut 11.859) | \n",
            "Epoch 008 | ** best ** train loss 23.2883 (now 11.245, fut 12.044) | val loss 23.2498 (now 11.676, fut 11.574) | \n",
            "Epoch 009 | ** best ** train loss 21.6460 (now 10.481, fut 11.165) | val loss 22.7965 (now 11.455, fut 11.342) | \n",
            "Epoch 010 | ** best ** train loss 20.5556 (now 9.991, fut 10.565) | val loss 22.5596 (now 11.274, fut 11.286) | \n",
            "Epoch 011 | ** best ** train loss 21.0011 (now 10.126, fut 10.875) | val loss 21.7883 (now 10.915, fut 10.874) | \n",
            "Epoch 012 | ** best ** train loss 19.7727 (now 9.554, fut 10.218) | val loss 21.2664 (now 10.580, fut 10.686) | \n",
            "Epoch 013 | ** best ** train loss 18.1095 (now 8.782, fut 9.327) | val loss 21.1914 (now 10.619, fut 10.573) | \n",
            "Epoch 014 | ** best ** train loss 18.5947 (now 8.971, fut 9.624) | val loss 20.6215 (now 10.358, fut 10.264) | \n",
            "Epoch 015 | ** best ** train loss 18.1824 (now 8.758, fut 9.424) | val loss 20.5117 (now 10.315, fut 10.197) | \n",
            "Epoch 016 |  train loss 17.1828 (now 8.264, fut 8.919) | val loss 21.9456 (now 11.023, fut 10.922) | \n",
            "Epoch 017 |  train loss 17.5370 (now 8.482, fut 9.055) | val loss 21.1744 (now 10.699, fut 10.475) | \n",
            "Epoch 018 | ** best ** train loss 17.3312 (now 8.350, fut 8.981) | val loss 20.1540 (now 10.037, fut 10.117) | \n",
            "Epoch 019 | ** best ** train loss 16.6575 (now 8.015, fut 8.643) | val loss 19.7928 (now 9.801, fut 9.992) | \n",
            "Epoch 020 |  train loss 16.5029 (now 7.943, fut 8.560) | val loss 21.4444 (now 10.640, fut 10.804) | \n",
            "Epoch 021 |  train loss 16.3606 (now 7.845, fut 8.516) | val loss 21.4631 (now 10.770, fut 10.694) | \n",
            "Epoch 022 |  train loss 16.2556 (now 7.809, fut 8.447) | val loss 21.1438 (now 10.538, fut 10.606) | \n",
            "Epoch 023 |  train loss 16.2059 (now 7.793, fut 8.413) | val loss 20.6093 (now 10.347, fut 10.262) | \n",
            "Epoch 024 |  train loss 15.7306 (now 7.534, fut 8.196) | val loss 20.7604 (now 10.486, fut 10.274) | \n",
            "Epoch 025 |  train loss 15.4241 (now 7.421, fut 8.003) | val loss 20.2343 (now 10.220, fut 10.015) | \n",
            "Epoch 026 | ** best ** train loss 15.1647 (now 7.256, fut 7.909) | val loss 19.8241 (now 9.863, fut 9.961) | \n",
            "Epoch 027 | ** best ** train loss 14.7183 (now 7.051, fut 7.668) | val loss 19.7055 (now 9.749, fut 9.957) | \n",
            "Epoch 028 |  train loss 14.6704 (now 7.049, fut 7.621) | val loss 20.4659 (now 10.242, fut 10.223) | \n",
            "Epoch 029 |  train loss 14.5699 (now 7.003, fut 7.566) | val loss 20.5850 (now 10.324, fut 10.261) | \n",
            "Epoch 030 |  train loss 14.8548 (now 7.097, fut 7.758) | val loss 19.9953 (now 10.006, fut 9.989) | \n",
            "Epoch 031 |  train loss 14.5160 (now 6.925, fut 7.591) | val loss 20.7320 (now 10.296, fut 10.436) | \n",
            "Epoch 032 |  train loss 15.0189 (now 7.192, fut 7.827) | val loss 20.0938 (now 10.008, fut 10.086) | \n",
            "Epoch 033 |  train loss 17.0839 (now 8.108, fut 8.976) | val loss 21.1373 (now 10.500, fut 10.638) | \n",
            "Epoch 034 |  train loss 16.9491 (now 8.057, fut 8.892) | val loss 20.5389 (now 10.235, fut 10.304) | \n",
            "Epoch 035 |  train loss 15.6414 (now 7.443, fut 8.199) | val loss 20.1001 (now 9.860, fut 10.241) | \n",
            "Epoch 036 | ** best ** train loss 14.9813 (now 7.174, fut 7.807) | val loss 19.9056 (now 9.969, fut 9.936) | \n",
            "Epoch 037 |  train loss 14.6399 (now 6.998, fut 7.642) | val loss 20.2372 (now 10.060, fut 10.177) | \n",
            "Epoch 038 | ** best ** train loss 14.3260 (now 6.820, fut 7.506) | val loss 19.4615 (now 9.650, fut 9.812) | \n",
            "Epoch 039 |  train loss 13.8576 (now 6.598, fut 7.260) | val loss 19.6408 (now 9.807, fut 9.834) | \n",
            "Epoch 040 |  train loss 13.8436 (now 6.622, fut 7.221) | val loss 19.6414 (now 9.820, fut 9.821) | \n",
            "Epoch 041 |  train loss 13.9446 (now 6.677, fut 7.267) | val loss 19.6598 (now 9.626, fut 10.034) | \n",
            "Epoch 042 |  train loss 13.7099 (now 6.583, fut 7.127) | val loss 20.5529 (now 10.201, fut 10.352) | \n",
            "Epoch 043 |  train loss 13.8802 (now 6.675, fut 7.206) | val loss 20.4509 (now 10.223, fut 10.228) | \n",
            "Epoch 044 |  train loss 13.6405 (now 6.538, fut 7.103) | val loss 20.6516 (now 10.362, fut 10.289) | \n",
            "Epoch 045 |  train loss 13.5652 (now 6.517, fut 7.048) | val loss 20.7631 (now 10.266, fut 10.497) | \n",
            "Epoch 046 |  train loss 14.1667 (now 6.805, fut 7.361) | val loss 21.1640 (now 10.527, fut 10.637) | \n",
            "Epoch 047 |  train loss 13.8914 (now 6.680, fut 7.211) | val loss 19.9836 (now 10.087, fut 9.897) | \n",
            "Epoch 048 |  train loss 13.4998 (now 6.483, fut 7.016) | val loss 19.9039 (now 9.886, fut 10.018) | \n",
            "Epoch 049 |  train loss 13.2651 (now 6.359, fut 6.906) | val loss 19.8970 (now 9.896, fut 10.000) | \n",
            "Epoch 050 |  train loss 13.1966 (now 6.316, fut 6.880) | val loss 20.0100 (now 9.866, fut 10.144) | \n",
            "Epoch 051 |  train loss 13.0613 (now 6.268, fut 6.793) | val loss 19.6953 (now 9.788, fut 9.907) | \n",
            "Epoch 052 |  train loss 13.0379 (now 6.228, fut 6.810) | val loss 19.8877 (now 9.888, fut 10.000) | \n",
            "Epoch 053 | ** best ** train loss 12.9312 (now 6.181, fut 6.750) | val loss 19.5133 (now 9.719, fut 9.794) | \n",
            "Epoch 054 |  train loss 12.8031 (now 6.136, fut 6.667) | val loss 19.7579 (now 9.785, fut 9.973) | \n",
            "Epoch 055 |  train loss 12.8023 (now 6.123, fut 6.679) | val loss 19.8405 (now 9.917, fut 9.924) | \n",
            "Epoch 056 |  train loss 12.7367 (now 6.079, fut 6.658) | val loss 20.5466 (now 10.376, fut 10.171) | \n",
            "Epoch 057 |  train loss 12.6535 (now 6.059, fut 6.594) | val loss 20.0769 (now 10.181, fut 9.896) | \n",
            "Epoch 058 |  train loss 12.6714 (now 6.086, fut 6.585) | val loss 19.9759 (now 10.001, fut 9.975) | \n",
            "Epoch 059 |  train loss 13.0214 (now 6.211, fut 6.810) | val loss 20.0274 (now 10.006, fut 10.021) | \n",
            "Epoch 060 |  train loss 12.6308 (now 6.039, fut 6.591) | val loss 19.8542 (now 9.785, fut 10.069) | \n",
            "Epoch 061 |  train loss 12.5291 (now 6.001, fut 6.528) | val loss 20.1007 (now 9.956, fut 10.145) | \n",
            "Epoch 062 |  train loss 12.4829 (now 5.995, fut 6.488) | val loss 19.9144 (now 9.868, fut 10.047) | \n",
            "Epoch 063 | ** best ** train loss 12.3775 (now 5.931, fut 6.447) | val loss 19.5688 (now 9.799, fut 9.769) | \n",
            "Epoch 064 |  train loss 12.5431 (now 6.007, fut 6.536) | val loss 20.5625 (now 10.332, fut 10.230) | \n",
            "Epoch 065 |  train loss 12.4475 (now 5.975, fut 6.472) | val loss 20.3157 (now 10.231, fut 10.085) | \n",
            "Epoch 066 |  train loss 12.4749 (now 5.965, fut 6.510) | val loss 20.6345 (now 10.275, fut 10.359) | \n",
            "Epoch 067 |  train loss 12.4890 (now 5.988, fut 6.501) | val loss 19.8341 (now 9.876, fut 9.958) | \n",
            "Epoch 068 | ** best ** train loss 12.3310 (now 5.928, fut 6.403) | val loss 19.1966 (now 9.473, fut 9.723) | \n",
            "Epoch 069 |  train loss 12.3128 (now 5.934, fut 6.379) | val loss 19.5706 (now 9.794, fut 9.776) | \n",
            "Epoch 070 |  train loss 12.3017 (now 5.917, fut 6.385) | val loss 20.0048 (now 10.128, fut 9.877) | \n",
            "Epoch 071 |  train loss 12.1698 (now 5.830, fut 6.340) | val loss 19.5647 (now 9.704, fut 9.861) | \n",
            "Epoch 072 |  train loss 12.1903 (now 5.857, fut 6.333) | val loss 19.9453 (now 9.896, fut 10.049) | \n",
            "Epoch 073 |  train loss 12.1502 (now 5.834, fut 6.316) | val loss 19.5495 (now 9.702, fut 9.848) | \n",
            "Epoch 074 |  train loss 12.0689 (now 5.805, fut 6.264) | val loss 20.2173 (now 10.131, fut 10.086) | \n",
            "Epoch 075 |  train loss 12.0541 (now 5.757, fut 6.297) | val loss 19.6290 (now 9.725, fut 9.904) | \n",
            "Epoch 076 |  train loss 11.9329 (now 5.740, fut 6.193) | val loss 20.1987 (now 10.064, fut 10.134) | \n",
            "Epoch 077 |  train loss 11.9282 (now 5.760, fut 6.168) | val loss 20.3779 (now 10.246, fut 10.132) | \n",
            "Epoch 078 |  train loss 11.9274 (now 5.733, fut 6.194) | val loss 20.3054 (now 10.127, fut 10.178) | \n",
            "Epoch 079 |  train loss 11.8107 (now 5.685, fut 6.125) | val loss 19.9531 (now 10.099, fut 9.855) | \n",
            "Epoch 080 |  train loss 11.8478 (now 5.689, fut 6.159) | val loss 20.1266 (now 10.102, fut 10.024) | \n",
            "Epoch 081 |  train loss 11.7431 (now 5.634, fut 6.109) | val loss 19.7597 (now 9.805, fut 9.955) | \n",
            "Epoch 082 |  train loss 11.6850 (now 5.596, fut 6.089) | val loss 20.1543 (now 10.066, fut 10.088) | \n",
            "Epoch 083 |  train loss 11.7099 (now 5.625, fut 6.085) | val loss 20.0938 (now 10.166, fut 9.928) | \n",
            "Epoch 084 |  train loss 11.6304 (now 5.585, fut 6.045) | val loss 19.9123 (now 9.899, fut 10.014) | \n",
            "Epoch 085 |  train loss 11.5570 (now 5.577, fut 5.980) | val loss 19.5636 (now 9.709, fut 9.854) | \n",
            "Epoch 086 |  train loss 11.6059 (now 5.576, fut 6.030) | val loss 19.6657 (now 9.724, fut 9.942) | \n",
            "Epoch 087 |  train loss 11.5585 (now 5.544, fut 6.014) | val loss 19.3213 (now 9.523, fut 9.798) | \n",
            "Epoch 088 |  train loss 11.5173 (now 5.531, fut 5.986) | val loss 20.2459 (now 10.174, fut 10.072) | \n",
            "Epoch 089 |  train loss 11.4571 (now 5.490, fut 5.967) | val loss 20.1842 (now 10.095, fut 10.089) | \n",
            "Epoch 090 |  train loss 11.4196 (now 5.478, fut 5.941) | val loss 19.9432 (now 9.846, fut 10.097) | \n",
            "Epoch 091 |  train loss 11.4381 (now 5.504, fut 5.935) | val loss 20.1485 (now 10.057, fut 10.092) | \n",
            "Epoch 092 |  train loss 11.3690 (now 5.466, fut 5.903) | val loss 19.5882 (now 9.722, fut 9.867) | \n",
            "Epoch 093 |  train loss 11.4100 (now 5.496, fut 5.914) | val loss 19.6230 (now 9.811, fut 9.812) | \n",
            "Epoch 094 |  train loss 11.3909 (now 5.477, fut 5.914) | val loss 20.2899 (now 10.046, fut 10.244) | \n",
            "Epoch 095 |  train loss 11.2785 (now 5.436, fut 5.842) | val loss 19.8596 (now 9.822, fut 10.037) | \n",
            "Epoch 096 |  train loss 11.3016 (now 5.423, fut 5.878) | val loss 19.9658 (now 9.785, fut 10.181) | \n",
            "Epoch 097 |  train loss 11.2167 (now 5.412, fut 5.805) | val loss 20.2091 (now 9.897, fut 10.312) | \n",
            "Epoch 098 |  train loss 11.1926 (now 5.400, fut 5.793) | val loss 20.0490 (now 9.982, fut 10.067) | \n",
            "Epoch 099 |  train loss 11.2528 (now 5.399, fut 5.854) | val loss 20.3387 (now 10.191, fut 10.148) | \n",
            "Epoch 100 |  train loss 11.2170 (now 5.377, fut 5.840) | val loss 20.2614 (now 10.124, fut 10.137) | \n",
            "Best validation forecast MAE (S2 fold): 9.7235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 2\n",
        "\n",
        "Best val_MAE for S1, S3-15: 19.1966 (100 EPOCHS; reached at epoch 68)\n",
        "\n",
        "- now val_MAE: 9.473\n",
        "- fut val_MAE: 9.7235\n",
        "\n",
        "notes:\n",
        "- overfitting occurring beyond epoch 68 (decreasing train loss while stagnating val loss)"
      ],
      "metadata": {
        "id": "Src2WEInXBe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 3]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "g58gsCmkW9b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=3, # test: subject 3\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S3\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S3 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUJE91EfWZ26",
        "outputId": "4581fa8b-4b79-48bb-a351-890ff3f33764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S3] Train 47376 | Val 11922 | Test 4337 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.6961 (now 74.631, fut 74.065) | val loss 98.6413 (now 50.466, fut 48.175) | \n",
            "Epoch 002 | ** best ** train loss 39.2944 (now 19.621, fut 19.674) | val loss 32.4865 (now 16.219, fut 16.268) | \n",
            "Epoch 003 | ** best ** train loss 32.8043 (now 15.511, fut 17.293) | val loss 30.9562 (now 15.448, fut 15.508) | \n",
            "Epoch 004 | ** best ** train loss 27.7383 (now 13.423, fut 14.315) | val loss 27.9904 (now 13.954, fut 14.037) | \n",
            "Epoch 005 | ** best ** train loss 26.7333 (now 12.793, fut 13.940) | val loss 26.4039 (now 13.049, fut 13.355) | \n",
            "Epoch 006 | ** best ** train loss 25.9851 (now 12.368, fut 13.617) | val loss 24.6305 (now 12.105, fut 12.526) | \n",
            "Epoch 007 |  train loss 24.1746 (now 11.544, fut 12.631) | val loss 24.9675 (now 12.220, fut 12.748) | \n",
            "Epoch 008 | ** best ** train loss 23.3610 (now 11.107, fut 12.254) | val loss 23.2147 (now 11.539, fut 11.676) | \n",
            "Epoch 009 | ** best ** train loss 22.1782 (now 10.567, fut 11.611) | val loss 22.9645 (now 11.403, fut 11.562) | \n",
            "Epoch 010 |  train loss 21.6785 (now 10.336, fut 11.343) | val loss 23.7702 (now 11.765, fut 12.005) | \n",
            "Epoch 011 | ** best ** train loss 21.4413 (now 10.179, fut 11.262) | val loss 22.3959 (now 11.033, fut 11.363) | \n",
            "Epoch 012 |  train loss 21.2944 (now 10.048, fut 11.247) | val loss 22.5521 (now 11.158, fut 11.394) | \n",
            "Epoch 013 | ** best ** train loss 20.7749 (now 9.863, fut 10.912) | val loss 21.8122 (now 10.730, fut 11.082) | \n",
            "Epoch 014 |  train loss 20.9398 (now 9.876, fut 11.064) | val loss 21.8736 (now 10.741, fut 11.133) | \n",
            "Epoch 015 | ** best ** train loss 20.1815 (now 9.599, fut 10.583) | val loss 21.4449 (now 10.668, fut 10.776) | \n",
            "Epoch 016 |  train loss 20.2290 (now 9.620, fut 10.609) | val loss 21.7651 (now 10.699, fut 11.067) | \n",
            "Epoch 017 |  train loss 19.9672 (now 9.538, fut 10.429) | val loss 21.5786 (now 10.663, fut 10.915) | \n",
            "Epoch 018 |  train loss 19.7222 (now 9.373, fut 10.349) | val loss 22.0718 (now 10.846, fut 11.225) | \n",
            "Epoch 019 |  train loss 19.6256 (now 9.289, fut 10.337) | val loss 21.6902 (now 10.758, fut 10.932) | \n",
            "Epoch 020 |  train loss 19.1538 (now 9.045, fut 10.109) | val loss 21.3733 (now 10.577, fut 10.796) | \n",
            "Epoch 021 | ** best ** train loss 19.2358 (now 9.047, fut 10.189) | val loss 20.9613 (now 10.327, fut 10.634) | \n",
            "Epoch 022 | ** best ** train loss 19.0116 (now 8.939, fut 10.072) | val loss 20.6266 (now 10.115, fut 10.511) | \n",
            "Epoch 023 |  train loss 19.4209 (now 9.114, fut 10.307) | val loss 21.5793 (now 10.579, fut 11.000) | \n",
            "Epoch 024 |  train loss 19.3351 (now 9.108, fut 10.227) | val loss 20.8739 (now 10.214, fut 10.660) | \n",
            "Epoch 025 |  train loss 19.1467 (now 8.989, fut 10.158) | val loss 20.9110 (now 10.353, fut 10.558) | \n",
            "Epoch 026 |  train loss 18.6616 (now 8.738, fut 9.924) | val loss 20.5087 (now 9.935, fut 10.574) | \n",
            "Epoch 027 |  train loss 18.7576 (now 8.828, fut 9.930) | val loss 20.6375 (now 10.079, fut 10.559) | \n",
            "Epoch 028 |  train loss 19.1639 (now 8.965, fut 10.199) | val loss 21.3709 (now 10.511, fut 10.860) | \n",
            "Epoch 029 | ** best ** train loss 19.1165 (now 9.021, fut 10.096) | val loss 20.3746 (now 9.943, fut 10.431) | \n",
            "Epoch 030 |  train loss 18.8673 (now 8.877, fut 9.990) | val loss 20.4418 (now 9.820, fut 10.622) | \n",
            "Epoch 031 | ** best ** train loss 18.5004 (now 8.719, fut 9.781) | val loss 20.2328 (now 9.874, fut 10.359) | \n",
            "Epoch 032 |  train loss 18.3953 (now 8.616, fut 9.780) | val loss 20.5965 (now 10.142, fut 10.454) | \n",
            "Epoch 033 |  train loss 18.7335 (now 8.790, fut 9.944) | val loss 21.2071 (now 10.323, fut 10.884) | \n",
            "Epoch 034 |  train loss 19.2354 (now 8.990, fut 10.245) | val loss 20.6496 (now 9.948, fut 10.701) | \n",
            "Epoch 035 | ** best ** train loss 18.8599 (now 8.801, fut 10.059) | val loss 19.8555 (now 9.680, fut 10.175) | \n",
            "Epoch 036 |  train loss 19.1903 (now 8.962, fut 10.229) | val loss 20.1927 (now 9.760, fut 10.432) | \n",
            "Epoch 037 |  train loss 18.5495 (now 8.688, fut 9.861) | val loss 20.1788 (now 9.819, fut 10.360) | \n",
            "Epoch 038 |  train loss 18.4648 (now 8.636, fut 9.829) | val loss 19.9613 (now 9.709, fut 10.253) | \n",
            "Epoch 039 | ** best ** train loss 18.0576 (now 8.476, fut 9.581) | val loss 19.7425 (now 9.581, fut 10.162) | \n",
            "Epoch 040 |  train loss 17.8343 (now 8.332, fut 9.502) | val loss 20.4426 (now 10.024, fut 10.419) | \n",
            "Epoch 041 |  train loss 17.9008 (now 8.409, fut 9.491) | val loss 20.2432 (now 9.749, fut 10.494) | \n",
            "Epoch 042 |  train loss 17.7880 (now 8.324, fut 9.464) | val loss 19.9463 (now 9.683, fut 10.264) | \n",
            "Epoch 043 |  train loss 17.7714 (now 8.304, fut 9.467) | val loss 20.2154 (now 9.718, fut 10.497) | \n",
            "Epoch 044 |  train loss 17.7742 (now 8.340, fut 9.435) | val loss 20.0242 (now 9.546, fut 10.478) | \n",
            "Epoch 045 |  train loss 17.5397 (now 8.254, fut 9.286) | val loss 20.5212 (now 9.913, fut 10.608) | \n",
            "Epoch 046 |  train loss 17.4450 (now 8.158, fut 9.287) | val loss 20.5773 (now 9.923, fut 10.654) | \n",
            "Epoch 047 |  train loss 18.2534 (now 8.579, fut 9.675) | val loss 20.4849 (now 9.643, fut 10.842) | \n",
            "Epoch 048 |  train loss 19.1732 (now 8.910, fut 10.263) | val loss 21.1679 (now 10.391, fut 10.777) | \n",
            "Epoch 049 |  train loss 20.0002 (now 9.256, fut 10.744) | val loss 21.1127 (now 10.142, fut 10.971) | \n",
            "Epoch 050 |  train loss 19.6087 (now 9.068, fut 10.540) | val loss 20.8801 (now 9.859, fut 11.022) | \n",
            "Epoch 051 |  train loss 19.1393 (now 8.906, fut 10.234) | val loss 20.5736 (now 9.832, fut 10.741) | \n",
            "Epoch 052 |  train loss 19.2317 (now 8.902, fut 10.330) | val loss 21.1166 (now 9.858, fut 11.259) | \n",
            "Epoch 053 |  train loss 18.7679 (now 8.712, fut 10.056) | val loss 21.3356 (now 10.187, fut 11.149) | \n",
            "Epoch 054 |  train loss 18.0882 (now 8.443, fut 9.645) | val loss 20.9404 (now 10.094, fut 10.846) | \n",
            "Epoch 055 |  train loss 17.8032 (now 8.246, fut 9.557) | val loss 20.1790 (now 9.738, fut 10.441) | \n",
            "Epoch 056 |  train loss 17.7449 (now 8.252, fut 9.493) | val loss 19.9587 (now 9.395, fut 10.564) | \n",
            "Epoch 057 |  train loss 17.6321 (now 8.176, fut 9.456) | val loss 20.2712 (now 9.673, fut 10.598) | \n",
            "Epoch 058 |  train loss 17.7774 (now 8.255, fut 9.522) | val loss 20.3948 (now 9.692, fut 10.703) | \n",
            "Epoch 059 |  train loss 17.7888 (now 8.238, fut 9.551) | val loss 20.3472 (now 9.725, fut 10.622) | \n",
            "Epoch 060 |  train loss 18.1628 (now 8.407, fut 9.756) | val loss 20.4315 (now 9.834, fut 10.598) | \n",
            "Epoch 061 |  train loss 18.0634 (now 8.335, fut 9.729) | val loss 20.4761 (now 9.879, fut 10.597) | \n",
            "Epoch 062 |  train loss 18.2176 (now 8.470, fut 9.747) | val loss 20.5279 (now 9.683, fut 10.844) | \n",
            "Epoch 063 |  train loss 18.0155 (now 8.334, fut 9.681) | val loss 20.8293 (now 9.906, fut 10.923) | \n",
            "Epoch 064 |  train loss 18.2741 (now 8.508, fut 9.766) | val loss 20.5806 (now 9.923, fut 10.658) | \n",
            "Epoch 065 |  train loss 17.5489 (now 8.174, fut 9.374) | val loss 20.0306 (now 9.561, fut 10.469) | \n",
            "Epoch 066 |  train loss 16.8935 (now 7.889, fut 9.005) | val loss 20.3703 (now 9.799, fut 10.572) | \n",
            "Epoch 067 |  train loss 16.9234 (now 7.884, fut 9.039) | val loss 20.4405 (now 9.893, fut 10.547) | \n",
            "Epoch 068 |  train loss 17.2607 (now 7.971, fut 9.290) | val loss 21.0879 (now 10.269, fut 10.819) | \n",
            "Epoch 069 |  train loss 17.1500 (now 7.958, fut 9.192) | val loss 20.5398 (now 9.670, fut 10.870) | \n",
            "Epoch 070 |  train loss 17.4955 (now 8.123, fut 9.373) | val loss 20.5380 (now 9.761, fut 10.777) | \n",
            "Epoch 071 |  train loss 16.5908 (now 7.767, fut 8.824) | val loss 20.7010 (now 9.978, fut 10.723) | \n",
            "Epoch 072 |  train loss 16.0578 (now 7.569, fut 8.489) | val loss 19.8557 (now 9.577, fut 10.279) | \n",
            "Epoch 073 |  train loss 16.1642 (now 7.586, fut 8.578) | val loss 20.0552 (now 9.562, fut 10.493) | \n",
            "Epoch 074 |  train loss 16.3233 (now 7.613, fut 8.710) | val loss 19.8371 (now 9.399, fut 10.438) | \n",
            "Epoch 075 |  train loss 16.1487 (now 7.571, fut 8.578) | val loss 20.0265 (now 9.587, fut 10.439) | \n",
            "Epoch 076 |  train loss 16.5428 (now 7.762, fut 8.781) | val loss 20.3477 (now 9.751, fut 10.597) | \n",
            "Epoch 077 |  train loss 16.7965 (now 7.834, fut 8.963) | val loss 20.7767 (now 10.003, fut 10.773) | \n",
            "Epoch 078 |  train loss 17.0810 (now 7.950, fut 9.131) | val loss 20.4962 (now 9.702, fut 10.794) | \n",
            "Epoch 079 |  train loss 16.9265 (now 7.871, fut 9.055) | val loss 20.0798 (now 9.568, fut 10.512) | \n",
            "Epoch 080 |  train loss 16.4690 (now 7.700, fut 8.770) | val loss 20.4386 (now 9.725, fut 10.713) | \n",
            "Epoch 081 |  train loss 16.2929 (now 7.615, fut 8.678) | val loss 20.3700 (now 9.661, fut 10.709) | \n",
            "Epoch 082 |  train loss 16.2792 (now 7.575, fut 8.705) | val loss 20.7965 (now 9.907, fut 10.889) | \n",
            "Epoch 083 |  train loss 15.9209 (now 7.464, fut 8.456) | val loss 20.0674 (now 9.424, fut 10.643) | \n",
            "Epoch 084 |  train loss 15.4001 (now 7.279, fut 8.121) | val loss 20.0146 (now 9.590, fut 10.425) | \n",
            "Epoch 085 |  train loss 15.3152 (now 7.140, fut 8.175) | val loss 20.2269 (now 9.724, fut 10.502) | \n",
            "Epoch 086 |  train loss 15.2552 (now 7.147, fut 8.108) | val loss 20.4392 (now 9.680, fut 10.759) | \n",
            "Epoch 087 |  train loss 15.0385 (now 7.055, fut 7.984) | val loss 19.8472 (now 9.407, fut 10.440) | \n",
            "Epoch 088 |  train loss 15.1367 (now 7.090, fut 8.047) | val loss 19.5480 (now 9.360, fut 10.188) | \n",
            "Epoch 089 | ** best ** train loss 14.5666 (now 6.875, fut 7.692) | val loss 19.4216 (now 9.330, fut 10.091) | \n",
            "Epoch 090 |  train loss 14.2454 (now 6.677, fut 7.568) | val loss 19.5277 (now 9.384, fut 10.144) | \n",
            "Epoch 091 |  train loss 14.0765 (now 6.646, fut 7.430) | val loss 19.6323 (now 9.500, fut 10.132) | \n",
            "Epoch 092 |  train loss 14.2098 (now 6.694, fut 7.516) | val loss 19.8346 (now 9.496, fut 10.339) | \n",
            "Epoch 093 |  train loss 14.3625 (now 6.784, fut 7.579) | val loss 20.0879 (now 9.555, fut 10.532) | \n",
            "Epoch 094 |  train loss 13.8670 (now 6.522, fut 7.345) | val loss 19.5100 (now 9.307, fut 10.203) | \n",
            "Epoch 095 |  train loss 13.5383 (now 6.399, fut 7.140) | val loss 19.5751 (now 9.463, fut 10.112) | \n",
            "Epoch 096 | ** best ** train loss 13.4754 (now 6.368, fut 7.107) | val loss 19.2010 (now 9.294, fut 9.907) | \n",
            "Epoch 097 |  train loss 13.4475 (now 6.332, fut 7.115) | val loss 19.5652 (now 9.357, fut 10.208) | \n",
            "Epoch 098 |  train loss 13.5179 (now 6.361, fut 7.157) | val loss 19.5316 (now 9.371, fut 10.161) | \n",
            "Epoch 099 |  train loss 13.5018 (now 6.370, fut 7.132) | val loss 20.0238 (now 9.659, fut 10.364) | \n",
            "Epoch 100 |  train loss 13.9916 (now 6.558, fut 7.434) | val loss 19.8829 (now 9.593, fut 10.290) | \n",
            "Best validation forecast MAE (S3 fold): 9.9069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 3\n",
        "\n",
        "Best val_MAE for S1-2, S4-15: 19.2010 (100 EPOCHS; reached at epoch 96)\n",
        "\n",
        "- now val_MAE: 9.294\n",
        "- fut val_MAE: 9.9069\n",
        "\n",
        "notes:\n",
        "- subject 3's data seems a little bit more difficult to decipher compared to 2 and 3 given the greater epochs spent in the mid 10s train_loss range.\n",
        "- slightly stronger overfitting than s1 and s2"
      ],
      "metadata": {
        "id": "INFKqIXBXLWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 4]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "lsAxmVHQag9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=4, # test: subject 4\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S4\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S4 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWb3btmWXPfz",
        "outputId": "e239394a-ce16-484b-ec04-6f8c819c91bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S4] Train 47212 | Val 11881 | Test 4542 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.9909 (now 74.213, fut 74.778) | val loss 99.2200 (now 49.710, fut 49.510) | \n",
            "Epoch 002 | ** best ** train loss 39.0780 (now 19.312, fut 19.766) | val loss 32.9716 (now 16.429, fut 16.542) | \n",
            "Epoch 003 | ** best ** train loss 35.7216 (now 17.538, fut 18.184) | val loss 29.1901 (now 14.673, fut 14.518) | \n",
            "Epoch 004 | ** best ** train loss 30.0897 (now 15.037, fut 15.053) | val loss 28.6126 (now 14.432, fut 14.180) | \n",
            "Epoch 005 | ** best ** train loss 28.9440 (now 14.544, fut 14.400) | val loss 26.3681 (now 13.257, fut 13.111) | \n",
            "Epoch 006 | ** best ** train loss 28.0555 (now 14.069, fut 13.987) | val loss 25.5146 (now 12.925, fut 12.590) | \n",
            "Epoch 007 | ** best ** train loss 26.4790 (now 13.269, fut 13.210) | val loss 25.5437 (now 13.159, fut 12.385) | \n",
            "Epoch 008 | ** best ** train loss 25.8299 (now 12.874, fut 12.956) | val loss 24.7512 (now 12.516, fut 12.235) | \n",
            "Epoch 009 | ** best ** train loss 24.7055 (now 12.350, fut 12.356) | val loss 23.7347 (now 12.018, fut 11.717) | \n",
            "Epoch 010 |  train loss 25.5770 (now 12.719, fut 12.858) | val loss 24.1471 (now 12.429, fut 11.718) | \n",
            "Epoch 011 |  train loss 24.9616 (now 12.466, fut 12.495) | val loss 24.7054 (now 12.529, fut 12.177) | \n",
            "Epoch 012 | ** best ** train loss 25.4361 (now 12.739, fut 12.697) | val loss 23.5168 (now 12.149, fut 11.368) | \n",
            "Epoch 013 |  train loss 25.4737 (now 12.826, fut 12.648) | val loss 23.6598 (now 11.975, fut 11.685) | \n",
            "Epoch 014 |  train loss 25.0770 (now 12.537, fut 12.540) | val loss 23.7357 (now 12.147, fut 11.589) | \n",
            "Epoch 015 |  train loss 26.7213 (now 13.431, fut 13.290) | val loss 26.0528 (now 13.325, fut 12.727) | \n",
            "Epoch 016 |  train loss 27.1942 (now 13.618, fut 13.576) | val loss 25.5468 (now 13.414, fut 12.133) | \n",
            "Epoch 017 |  train loss 26.1832 (now 13.119, fut 13.064) | val loss 24.7388 (now 12.634, fut 12.105) | \n",
            "Epoch 018 |  train loss 26.6147 (now 13.378, fut 13.236) | val loss 24.5823 (now 12.361, fut 12.221) | \n",
            "Epoch 019 |  train loss 26.3282 (now 13.234, fut 13.094) | val loss 24.2353 (now 12.488, fut 11.747) | \n",
            "Epoch 020 |  train loss 25.5452 (now 12.743, fut 12.802) | val loss 24.1312 (now 12.465, fut 11.666) | \n",
            "Epoch 021 |  train loss 25.5309 (now 12.824, fut 12.707) | val loss 23.4041 (now 11.973, fut 11.431) | \n",
            "Epoch 022 |  train loss 26.0475 (now 13.059, fut 12.988) | val loss 24.7501 (now 12.726, fut 12.024) | \n",
            "Epoch 023 |  train loss 25.9341 (now 12.967, fut 12.967) | val loss 24.1542 (now 12.275, fut 11.879) | \n",
            "Epoch 024 |  train loss 25.9142 (now 12.996, fut 12.918) | val loss 23.8221 (now 12.023, fut 11.799) | \n",
            "Epoch 025 |  train loss 26.3116 (now 13.194, fut 13.118) | val loss 23.9373 (now 12.131, fut 11.806) | \n",
            "Epoch 026 |  train loss 25.9293 (now 13.046, fut 12.883) | val loss 23.9313 (now 12.044, fut 11.888) | \n",
            "Epoch 027 |  train loss 24.6281 (now 12.310, fut 12.318) | val loss 23.9635 (now 12.074, fut 11.890) | \n",
            "Epoch 028 |  train loss 25.5579 (now 12.713, fut 12.845) | val loss 24.6889 (now 12.727, fut 11.962) | \n",
            "Epoch 029 |  train loss 25.4043 (now 12.609, fut 12.795) | val loss 24.9703 (now 12.825, fut 12.145) | \n",
            "Epoch 030 |  train loss 24.3479 (now 12.117, fut 12.231) | val loss 24.5315 (now 12.585, fut 11.946) | \n",
            "Epoch 031 |  train loss 24.1708 (now 11.997, fut 12.174) | val loss 24.9739 (now 12.770, fut 12.204) | \n",
            "Epoch 032 |  train loss 23.7853 (now 11.823, fut 11.962) | val loss 24.2320 (now 12.237, fut 11.995) | \n",
            "Epoch 033 |  train loss 23.5518 (now 11.671, fut 11.881) | val loss 23.7897 (now 12.208, fut 11.582) | \n",
            "Epoch 034 |  train loss 23.2840 (now 11.525, fut 11.759) | val loss 23.2219 (now 11.816, fut 11.406) | \n",
            "Epoch 035 |  train loss 22.3900 (now 11.103, fut 11.287) | val loss 23.3018 (now 11.793, fut 11.508) | \n",
            "Epoch 036 | ** best ** train loss 20.6594 (now 10.153, fut 10.506) | val loss 22.0618 (now 10.980, fut 11.082) | \n",
            "Epoch 037 |  train loss 19.4828 (now 9.540, fut 9.943) | val loss 22.6827 (now 11.479, fut 11.204) | \n",
            "Epoch 038 | ** best ** train loss 19.0074 (now 9.306, fut 9.702) | val loss 21.8394 (now 11.001, fut 10.838) | \n",
            "Epoch 039 | ** best ** train loss 18.8199 (now 9.198, fut 9.622) | val loss 21.7402 (now 10.933, fut 10.807) | \n",
            "Epoch 040 |  train loss 18.9714 (now 9.254, fut 9.718) | val loss 22.1903 (now 11.165, fut 11.025) | \n",
            "Epoch 041 | ** best ** train loss 18.3952 (now 8.999, fut 9.397) | val loss 21.5362 (now 10.862, fut 10.674) | \n",
            "Epoch 042 |  train loss 18.1659 (now 8.868, fut 9.298) | val loss 22.2160 (now 11.311, fut 10.905) | \n",
            "Epoch 043 | ** best ** train loss 17.4532 (now 8.509, fut 8.944) | val loss 20.7190 (now 10.334, fut 10.385) | \n",
            "Epoch 044 | ** best ** train loss 17.2688 (now 8.402, fut 8.866) | val loss 20.9562 (now 10.577, fut 10.379) | \n",
            "Epoch 045 | ** best ** train loss 16.9089 (now 8.209, fut 8.700) | val loss 20.8069 (now 10.568, fut 10.239) | \n",
            "Epoch 046 |  train loss 16.3647 (now 7.912, fut 8.453) | val loss 20.4753 (now 10.230, fut 10.246) | \n",
            "Epoch 047 | ** best ** train loss 16.0215 (now 7.779, fut 8.242) | val loss 20.0386 (now 10.043, fut 9.995) | \n",
            "Epoch 048 | ** best ** train loss 15.7617 (now 7.603, fut 8.159) | val loss 19.8229 (now 9.908, fut 9.915) | \n",
            "Epoch 049 |  train loss 15.4942 (now 7.472, fut 8.023) | val loss 20.3375 (now 10.372, fut 9.965) | \n",
            "Epoch 050 |  train loss 15.1419 (now 7.304, fut 7.838) | val loss 20.0101 (now 10.038, fut 9.972) | \n",
            "Epoch 051 |  train loss 15.1379 (now 7.283, fut 7.855) | val loss 20.3621 (now 10.249, fut 10.113) | \n",
            "Epoch 052 |  train loss 15.0838 (now 7.234, fut 7.849) | val loss 20.4435 (now 10.354, fut 10.089) | \n",
            "Epoch 053 |  train loss 14.8857 (now 7.169, fut 7.717) | val loss 20.3488 (now 10.283, fut 10.065) | \n",
            "Epoch 054 |  train loss 14.9132 (now 7.196, fut 7.717) | val loss 20.3719 (now 10.144, fut 10.228) | \n",
            "Epoch 055 |  train loss 15.0303 (now 7.237, fut 7.793) | val loss 19.9473 (now 9.781, fut 10.167) | \n",
            "Epoch 056 |  train loss 14.9039 (now 7.157, fut 7.747) | val loss 20.2270 (now 10.140, fut 10.087) | \n",
            "Epoch 057 | ** best ** train loss 14.8337 (now 7.133, fut 7.701) | val loss 19.5576 (now 9.732, fut 9.825) | \n",
            "Epoch 058 |  train loss 14.8307 (now 7.142, fut 7.688) | val loss 20.3624 (now 10.230, fut 10.133) | \n",
            "Epoch 059 |  train loss 14.7086 (now 7.089, fut 7.620) | val loss 20.4715 (now 10.306, fut 10.166) | \n",
            "Epoch 060 |  train loss 14.6017 (now 7.000, fut 7.602) | val loss 20.1353 (now 9.945, fut 10.190) | \n",
            "Epoch 061 |  train loss 14.5470 (now 6.991, fut 7.556) | val loss 19.8042 (now 9.835, fut 9.969) | \n",
            "Epoch 062 |  train loss 14.6829 (now 7.078, fut 7.605) | val loss 20.6165 (now 10.261, fut 10.356) | \n",
            "Epoch 063 |  train loss 14.3572 (now 6.916, fut 7.442) | val loss 20.8341 (now 10.346, fut 10.489) | \n",
            "Epoch 064 |  train loss 14.2488 (now 6.849, fut 7.400) | val loss 20.1729 (now 10.007, fut 10.166) | \n",
            "Epoch 065 |  train loss 14.1115 (now 6.771, fut 7.340) | val loss 20.2425 (now 10.039, fut 10.204) | \n",
            "Epoch 066 |  train loss 13.9926 (now 6.721, fut 7.271) | val loss 20.3820 (now 10.152, fut 10.230) | \n",
            "Epoch 067 |  train loss 13.9552 (now 6.695, fut 7.260) | val loss 20.6769 (now 10.400, fut 10.277) | \n",
            "Epoch 068 |  train loss 13.7635 (now 6.628, fut 7.135) | val loss 19.7948 (now 9.860, fut 9.935) | \n",
            "Epoch 069 |  train loss 13.5722 (now 6.513, fut 7.059) | val loss 19.8445 (now 9.798, fut 10.047) | \n",
            "Epoch 070 | ** best ** train loss 13.5132 (now 6.499, fut 7.014) | val loss 19.3839 (now 9.633, fut 9.751) | \n",
            "Epoch 071 |  train loss 13.3738 (now 6.408, fut 6.966) | val loss 20.3576 (now 10.215, fut 10.142) | \n",
            "Epoch 072 |  train loss 13.2888 (now 6.383, fut 6.906) | val loss 20.1195 (now 9.969, fut 10.151) | \n",
            "Epoch 073 |  train loss 13.2888 (now 6.383, fut 6.906) | val loss 20.2194 (now 10.073, fut 10.146) | \n",
            "Epoch 074 |  train loss 13.1172 (now 6.288, fut 6.829) | val loss 19.5769 (now 9.633, fut 9.943) | \n",
            "Epoch 075 |  train loss 13.0333 (now 6.250, fut 6.783) | val loss 19.5268 (now 9.681, fut 9.846) | \n",
            "Epoch 076 |  train loss 12.9441 (now 6.210, fut 6.734) | val loss 19.8098 (now 9.750, fut 10.060) | \n",
            "Epoch 077 |  train loss 12.9166 (now 6.181, fut 6.735) | val loss 19.6172 (now 9.634, fut 9.983) | \n",
            "Epoch 078 |  train loss 12.7902 (now 6.141, fut 6.650) | val loss 20.2357 (now 10.158, fut 10.077) | \n",
            "Epoch 079 |  train loss 12.7463 (now 6.106, fut 6.641) | val loss 19.6662 (now 9.805, fut 9.862) | \n",
            "Epoch 080 |  train loss 12.6594 (now 6.049, fut 6.610) | val loss 19.7839 (now 9.771, fut 10.013) | \n",
            "Epoch 081 |  train loss 12.6385 (now 6.053, fut 6.585) | val loss 20.4586 (now 10.154, fut 10.305) | \n",
            "Epoch 082 |  train loss 12.6320 (now 6.061, fut 6.571) | val loss 19.5283 (now 9.679, fut 9.849) | \n",
            "Epoch 083 |  train loss 12.5548 (now 6.024, fut 6.531) | val loss 19.6005 (now 9.663, fut 9.938) | \n",
            "Epoch 084 |  train loss 12.5127 (now 6.000, fut 6.513) | val loss 19.6963 (now 9.733, fut 9.963) | \n",
            "Epoch 085 |  train loss 12.4460 (now 5.955, fut 6.491) | val loss 19.9046 (now 9.872, fut 10.033) | \n",
            "Epoch 086 |  train loss 12.3683 (now 5.916, fut 6.452) | val loss 20.1911 (now 10.198, fut 9.993) | \n",
            "Epoch 087 |  train loss 12.3415 (now 5.904, fut 6.438) | val loss 19.6959 (now 9.630, fut 10.066) | \n",
            "Epoch 088 |  train loss 12.3405 (now 5.904, fut 6.436) | val loss 19.5637 (now 9.722, fut 9.842) | \n",
            "Epoch 089 |  train loss 12.3199 (now 5.896, fut 6.424) | val loss 19.2589 (now 9.413, fut 9.846) | \n",
            "Epoch 090 |  train loss 12.2100 (now 5.852, fut 6.358) | val loss 19.8069 (now 9.928, fut 9.879) | \n",
            "Epoch 091 |  train loss 12.0916 (now 5.805, fut 6.287) | val loss 19.8114 (now 9.840, fut 9.972) | \n",
            "Epoch 092 |  train loss 12.0698 (now 5.810, fut 6.260) | val loss 19.2507 (now 9.464, fut 9.787) | \n",
            "Epoch 093 |  train loss 12.0351 (now 5.761, fut 6.275) | val loss 19.5425 (now 9.588, fut 9.955) | \n",
            "Epoch 094 |  train loss 11.9633 (now 5.737, fut 6.227) | val loss 19.4160 (now 9.585, fut 9.831) | \n",
            "Epoch 095 |  train loss 11.9296 (now 5.717, fut 6.212) | val loss 19.8496 (now 9.883, fut 9.966) | \n",
            "Epoch 096 |  train loss 12.3642 (now 5.945, fut 6.419) | val loss 19.8235 (now 9.856, fut 9.968) | \n",
            "Epoch 097 |  train loss 12.3935 (now 5.975, fut 6.418) | val loss 19.9206 (now 9.842, fut 10.078) | \n",
            "Epoch 098 |  train loss 11.8152 (now 5.656, fut 6.159) | val loss 19.6077 (now 9.727, fut 9.880) | \n",
            "Epoch 099 |  train loss 11.7882 (now 5.637, fut 6.151) | val loss 19.7846 (now 9.765, fut 10.020) | \n",
            "Epoch 100 |  train loss 11.7686 (now 5.635, fut 6.134) | val loss 19.5546 (now 9.717, fut 9.838) | \n",
            "Best validation forecast MAE (S4 fold): 9.7505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 4\n",
        "\n",
        "Best val_MAE for S1-3, S5-15: 19.3839 (100 EPOCHS; reached at epoch 70)\n",
        "\n",
        "- now val_MAE: 9.633\n",
        "- fut val_MAE: 9.7505\n",
        "\n",
        "notes:\n",
        "- overfitting problem once more\n",
        "- data seems to be more difficult to decipher compared to 1 and 2, however easier than 3."
      ],
      "metadata": {
        "id": "ESA3M-nExaVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 5]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "KtbBJ6WhmUWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=5, # test: subject 5\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S5\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S5 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "id": "7cV3C9p3yCEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93aff316-ad00-464a-ca79-bd07919b14d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S5] Train 47153 | Val 11867 | Test 4619 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 143.7757 (now 71.393, fut 72.383) | val loss 94.5351 (now 46.668, fut 47.867) | \n",
            "Epoch 002 | ** best ** train loss 34.6932 (now 16.977, fut 17.716) | val loss 31.2007 (now 15.616, fut 15.585) | \n",
            "Epoch 003 | ** best ** train loss 33.5203 (now 18.061, fut 15.459) | val loss 26.2170 (now 13.217, fut 13.000) | \n",
            "Epoch 004 | ** best ** train loss 25.9132 (now 13.035, fut 12.878) | val loss 25.5372 (now 12.842, fut 12.695) | \n",
            "Epoch 005 | ** best ** train loss 23.7485 (now 11.848, fut 11.900) | val loss 23.1579 (now 11.619, fut 11.539) | \n",
            "Epoch 006 | ** best ** train loss 21.6733 (now 10.728, fut 10.946) | val loss 22.4039 (now 11.239, fut 11.165) | \n",
            "Epoch 007 |  train loss 21.0493 (now 10.379, fut 10.670) | val loss 25.6169 (now 13.223, fut 12.394) | \n",
            "Epoch 008 |  train loss 21.6648 (now 10.833, fut 10.832) | val loss 23.8985 (now 12.180, fut 11.718) | \n",
            "Epoch 009 | ** best ** train loss 20.7026 (now 10.297, fut 10.406) | val loss 22.0668 (now 11.431, fut 10.636) | \n",
            "Epoch 010 | ** best ** train loss 19.1011 (now 9.362, fut 9.739) | val loss 20.9588 (now 10.508, fut 10.451) | \n",
            "Epoch 011 |  train loss 19.2186 (now 9.461, fut 9.758) | val loss 21.4668 (now 10.783, fut 10.683) | \n",
            "Epoch 012 |  train loss 19.3888 (now 9.608, fut 9.780) | val loss 21.6585 (now 11.134, fut 10.524) | \n",
            "Epoch 013 | ** best ** train loss 18.7667 (now 9.195, fut 9.571) | val loss 20.3764 (now 10.147, fut 10.229) | \n",
            "Epoch 014 | ** best ** train loss 17.8556 (now 8.712, fut 9.144) | val loss 20.0005 (now 9.942, fut 10.058) | \n",
            "Epoch 015 | ** best ** train loss 17.6327 (now 8.550, fut 9.083) | val loss 19.6882 (now 9.835, fut 9.853) | \n",
            "Epoch 016 | ** best ** train loss 16.9448 (now 8.160, fut 8.784) | val loss 19.0202 (now 9.343, fut 9.677) | \n",
            "Epoch 017 | ** best ** train loss 16.4884 (now 7.869, fut 8.619) | val loss 18.1688 (now 8.759, fut 9.410) | \n",
            "Epoch 018 | ** best ** train loss 15.8189 (now 7.468, fut 8.351) | val loss 18.3418 (now 9.113, fut 9.229) | \n",
            "Epoch 019 |  train loss 15.3069 (now 7.175, fut 8.132) | val loss 17.7521 (now 8.515, fut 9.237) | \n",
            "Epoch 020 | ** best ** train loss 14.6183 (now 6.791, fut 7.828) | val loss 17.3306 (now 8.126, fut 9.204) | \n",
            "Epoch 021 | ** best ** train loss 14.2783 (now 6.567, fut 7.711) | val loss 17.1765 (now 8.057, fut 9.119) | \n",
            "Epoch 022 | ** best ** train loss 14.0091 (now 6.425, fut 7.584) | val loss 16.9453 (now 7.899, fut 9.046) | \n",
            "Epoch 023 |  train loss 13.7939 (now 6.299, fut 7.495) | val loss 16.8757 (now 7.826, fut 9.050) | \n",
            "Epoch 024 | ** best ** train loss 13.6308 (now 6.218, fut 7.413) | val loss 17.1848 (now 8.187, fut 8.998) | \n",
            "Epoch 025 | ** best ** train loss 13.4457 (now 6.085, fut 7.361) | val loss 16.7716 (now 7.804, fut 8.968) | \n",
            "Epoch 026 | ** best ** train loss 13.2717 (now 6.016, fut 7.255) | val loss 16.5950 (now 7.655, fut 8.940) | \n",
            "Epoch 027 | ** best ** train loss 13.2338 (now 5.992, fut 7.242) | val loss 15.8260 (now 7.220, fut 8.606) | \n",
            "Epoch 028 |  train loss 13.1265 (now 5.961, fut 7.165) | val loss 16.1682 (now 7.427, fut 8.741) | \n",
            "Epoch 029 |  train loss 12.9783 (now 5.880, fut 7.099) | val loss 16.1512 (now 7.435, fut 8.716) | \n",
            "Epoch 030 |  train loss 12.8338 (now 5.815, fut 7.019) | val loss 17.1989 (now 8.116, fut 9.083) | \n",
            "Epoch 031 |  train loss 12.8222 (now 5.807, fut 7.015) | val loss 15.8667 (now 7.234, fut 8.633) | \n",
            "Epoch 032 | ** best ** train loss 12.6814 (now 5.723, fut 6.958) | val loss 15.9548 (now 7.364, fut 8.590) | \n",
            "Epoch 033 |  train loss 12.6702 (now 5.709, fut 6.961) | val loss 15.9773 (now 7.341, fut 8.636) | \n",
            "Epoch 034 |  train loss 12.5125 (now 5.657, fut 6.856) | val loss 16.4850 (now 7.603, fut 8.882) | \n",
            "Epoch 035 |  train loss 12.4780 (now 5.628, fut 6.850) | val loss 16.5862 (now 7.636, fut 8.951) | \n",
            "Epoch 036 |  train loss 12.3796 (now 5.582, fut 6.798) | val loss 16.0225 (now 7.282, fut 8.740) | \n",
            "Epoch 037 |  train loss 12.3440 (now 5.542, fut 6.802) | val loss 15.9479 (now 7.188, fut 8.760) | \n",
            "Epoch 038 |  train loss 12.2385 (now 5.500, fut 6.738) | val loss 16.4577 (now 7.534, fut 8.924) | \n",
            "Epoch 039 |  train loss 12.1584 (now 5.435, fut 6.723) | val loss 16.2322 (now 7.401, fut 8.832) | \n",
            "Epoch 040 |  train loss 12.1622 (now 5.475, fut 6.687) | val loss 16.0708 (now 7.318, fut 8.753) | \n",
            "Epoch 041 |  train loss 12.0831 (now 5.435, fut 6.648) | val loss 16.8829 (now 7.946, fut 8.937) | \n",
            "Epoch 042 |  train loss 11.9828 (now 5.365, fut 6.618) | val loss 16.0046 (now 7.255, fut 8.750) | \n",
            "Epoch 043 |  train loss 11.9722 (now 5.364, fut 6.608) | val loss 16.0669 (now 7.192, fut 8.875) | \n",
            "Epoch 044 |  train loss 11.9512 (now 5.379, fut 6.572) | val loss 16.2909 (now 7.185, fut 9.106) | \n",
            "Epoch 045 |  train loss 11.8622 (now 5.323, fut 6.540) | val loss 16.2217 (now 7.269, fut 8.953) | \n",
            "Epoch 046 |  train loss 11.8298 (now 5.298, fut 6.532) | val loss 15.9497 (now 7.208, fut 8.742) | \n",
            "Epoch 047 |  train loss 11.7956 (now 5.279, fut 6.516) | val loss 16.3744 (now 7.483, fut 8.891) | \n",
            "Epoch 048 |  train loss 11.9160 (now 5.359, fut 6.557) | val loss 16.1284 (now 7.317, fut 8.811) | \n",
            "Epoch 049 |  train loss 11.6905 (now 5.226, fut 6.465) | val loss 16.6627 (now 7.563, fut 9.100) | \n",
            "Epoch 050 |  train loss 11.7018 (now 5.231, fut 6.471) | val loss 16.1759 (now 7.174, fut 9.002) | \n",
            "Epoch 051 |  train loss 11.6520 (now 5.220, fut 6.432) | val loss 16.0723 (now 7.059, fut 9.013) | \n",
            "Epoch 052 |  train loss 11.5532 (now 5.181, fut 6.372) | val loss 16.0438 (now 7.071, fut 8.973) | \n",
            "Epoch 053 |  train loss 11.5163 (now 5.156, fut 6.360) | val loss 15.9492 (now 7.168, fut 8.781) | \n",
            "Epoch 054 |  train loss 11.5231 (now 5.165, fut 6.358) | val loss 16.2915 (now 7.115, fut 9.176) | \n",
            "Epoch 055 |  train loss 11.4768 (now 5.151, fut 6.325) | val loss 16.4864 (now 7.469, fut 9.018) | \n",
            "Epoch 056 |  train loss 11.4489 (now 5.144, fut 6.305) | val loss 16.7078 (now 7.625, fut 9.083) | \n",
            "Epoch 057 |  train loss 11.4362 (now 5.136, fut 6.301) | val loss 16.2255 (now 7.090, fut 9.136) | \n",
            "Epoch 058 |  train loss 11.3519 (now 5.088, fut 6.264) | val loss 16.3019 (now 7.300, fut 9.002) | \n",
            "Epoch 059 |  train loss 11.3090 (now 5.052, fut 6.257) | val loss 16.1525 (now 7.090, fut 9.063) | \n",
            "Epoch 060 |  train loss 11.3225 (now 5.086, fut 6.236) | val loss 16.0887 (now 7.191, fut 8.897) | \n",
            "Epoch 061 |  train loss 11.2518 (now 5.038, fut 6.214) | val loss 16.9389 (now 7.629, fut 9.310) | \n",
            "Epoch 062 |  train loss 11.2336 (now 5.028, fut 6.206) | val loss 16.0015 (now 7.006, fut 8.995) | \n",
            "Epoch 063 |  train loss 11.2046 (now 4.988, fut 6.216) | val loss 16.5506 (now 7.357, fut 9.194) | \n",
            "Epoch 064 |  train loss 11.1682 (now 4.994, fut 6.174) | val loss 16.2923 (now 7.149, fut 9.143) | \n",
            "Epoch 065 |  train loss 11.1149 (now 4.961, fut 6.154) | val loss 16.5000 (now 7.388, fut 9.112) | \n",
            "Epoch 066 |  train loss 11.1318 (now 4.974, fut 6.157) | val loss 16.5840 (now 7.490, fut 9.094) | \n",
            "Epoch 067 |  train loss 11.0972 (now 4.978, fut 6.119) | val loss 16.2770 (now 7.175, fut 9.102) | \n",
            "Epoch 068 |  train loss 11.0275 (now 4.953, fut 6.075) | val loss 16.1991 (now 7.098, fut 9.101) | \n",
            "Epoch 069 |  train loss 11.0488 (now 4.948, fut 6.101) | val loss 16.0780 (now 6.947, fut 9.131) | \n",
            "Epoch 070 |  train loss 11.0149 (now 4.922, fut 6.093) | val loss 16.5379 (now 7.144, fut 9.394) | \n",
            "Epoch 071 |  train loss 10.9231 (now 4.892, fut 6.031) | val loss 16.1308 (now 7.003, fut 9.128) | \n",
            "Epoch 072 |  train loss 10.8764 (now 4.858, fut 6.018) | val loss 15.8915 (now 6.918, fut 8.973) | \n",
            "Epoch 073 |  train loss 10.8356 (now 4.855, fut 5.981) | val loss 16.3511 (now 7.101, fut 9.250) | \n",
            "Epoch 074 |  train loss 10.8449 (now 4.852, fut 5.993) | val loss 16.3902 (now 7.226, fut 9.164) | \n",
            "Epoch 075 |  train loss 10.8034 (now 4.825, fut 5.978) | val loss 16.4211 (now 7.277, fut 9.144) | \n",
            "Epoch 076 |  train loss 10.8474 (now 4.871, fut 5.976) | val loss 16.1127 (now 6.928, fut 9.185) | \n",
            "Epoch 077 |  train loss 10.7717 (now 4.805, fut 5.967) | val loss 16.2767 (now 7.050, fut 9.227) | \n",
            "Epoch 078 |  train loss 10.7588 (now 4.797, fut 5.962) | val loss 16.3612 (now 7.066, fut 9.295) | \n",
            "Epoch 079 |  train loss 10.7528 (now 4.807, fut 5.946) | val loss 16.0971 (now 7.002, fut 9.095) | \n",
            "Epoch 080 |  train loss 10.7364 (now 4.805, fut 5.932) | val loss 16.8512 (now 7.460, fut 9.391) | \n",
            "Epoch 081 |  train loss 10.6309 (now 4.768, fut 5.863) | val loss 16.3011 (now 6.989, fut 9.312) | \n",
            "Epoch 082 |  train loss 10.6542 (now 4.778, fut 5.876) | val loss 16.5642 (now 7.315, fut 9.249) | \n",
            "Epoch 083 |  train loss 10.5874 (now 4.741, fut 5.846) | val loss 16.2714 (now 7.114, fut 9.158) | \n",
            "Epoch 084 |  train loss 10.5871 (now 4.745, fut 5.842) | val loss 17.3161 (now 7.817, fut 9.499) | \n",
            "Epoch 085 |  train loss 10.5748 (now 4.763, fut 5.812) | val loss 16.3461 (now 7.166, fut 9.180) | \n",
            "Epoch 086 |  train loss 10.5785 (now 4.725, fut 5.854) | val loss 16.6724 (now 7.272, fut 9.400) | \n",
            "Epoch 087 |  train loss 10.5183 (now 4.699, fut 5.819) | val loss 16.2914 (now 7.145, fut 9.146) | \n",
            "Epoch 088 |  train loss 10.4733 (now 4.697, fut 5.777) | val loss 16.2559 (now 7.235, fut 9.021) | \n",
            "Epoch 089 |  train loss 10.4575 (now 4.689, fut 5.769) | val loss 16.2040 (now 7.127, fut 9.077) | \n",
            "Epoch 090 |  train loss 10.4095 (now 4.671, fut 5.738) | val loss 16.9587 (now 7.472, fut 9.486) | \n",
            "Epoch 091 |  train loss 10.4468 (now 4.694, fut 5.752) | val loss 16.5292 (now 7.439, fut 9.090) | \n",
            "Epoch 092 |  train loss 10.4677 (now 4.698, fut 5.770) | val loss 16.2548 (now 7.104, fut 9.151) | \n",
            "Epoch 093 |  train loss 10.3786 (now 4.644, fut 5.734) | val loss 16.8425 (now 7.697, fut 9.146) | \n",
            "Epoch 094 |  train loss 10.3227 (now 4.628, fut 5.695) | val loss 16.2201 (now 6.965, fut 9.255) | \n",
            "Epoch 095 |  train loss 10.3013 (now 4.610, fut 5.691) | val loss 16.1794 (now 6.959, fut 9.221) | \n",
            "Epoch 096 |  train loss 10.2969 (now 4.624, fut 5.673) | val loss 16.2740 (now 7.094, fut 9.180) | \n",
            "Epoch 097 |  train loss 10.3351 (now 4.650, fut 5.685) | val loss 16.9231 (now 7.410, fut 9.513) | \n",
            "Epoch 098 |  train loss 10.2528 (now 4.579, fut 5.674) | val loss 16.9193 (now 7.576, fut 9.343) | \n",
            "Epoch 099 |  train loss 10.3400 (now 4.637, fut 5.703) | val loss 16.6533 (now 7.435, fut 9.218) | \n",
            "Epoch 100 |  train loss 10.2994 (now 4.617, fut 5.682) | val loss 16.4035 (now 7.042, fut 9.362) | \n",
            "Best validation forecast MAE (S5 fold): 8.5903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 5\n",
        "\n",
        "Best val_MAE for S1-4, S6-15: 15.9548 (100 EPOCHS; reached at epoch 31)\n",
        "\n",
        "- now val_MAE: 7.364\n",
        "- fut val_MAE: 8.5903\n",
        "\n",
        "notes:\n",
        "- this dataset seems to be the most decipherable by the model (so far), given the low train and val loss relative to previous 4 (2-3 val loss points lower). potentially indicating that subject 5's data is particularly difficult...(differing physiology, mortion artefacts, activity effort)?\n",
        "- again overfitting beyond epoch 31..."
      ],
      "metadata": {
        "id": "gyxx5uHOmj8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 6] - note S6 only has 1.5/2.5 hours worth of recorded data\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "FVME-k_vm71O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=6, # test: subject 6\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S6\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S6 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g83fRV2JmzDK",
        "outputId": "2a2007d5-f48b-4ddf-bebf-e5c05a9cbfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S6] Train 48762 | Val 12269 | Test 2592 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 146.0683 (now 73.131, fut 72.938) | val loss 95.3617 (now 49.047, fut 46.315) | \n",
            "Epoch 002 | ** best ** train loss 37.5699 (now 18.820, fut 18.750) | val loss 43.3735 (now 21.669, fut 21.705) | \n",
            "Epoch 003 | ** best ** train loss 30.9593 (now 15.203, fut 15.756) | val loss 28.9218 (now 14.539, fut 14.383) | \n",
            "Epoch 004 | ** best ** train loss 26.2245 (now 12.859, fut 13.366) | val loss 25.1385 (now 12.679, fut 12.460) | \n",
            "Epoch 005 | ** best ** train loss 22.6684 (now 11.120, fut 11.549) | val loss 22.8228 (now 11.580, fut 11.243) | \n",
            "Epoch 006 | ** best ** train loss 21.5624 (now 10.521, fut 11.041) | val loss 21.0827 (now 10.570, fut 10.513) | \n",
            "Epoch 007 | ** best ** train loss 20.3672 (now 9.930, fut 10.437) | val loss 20.9132 (now 10.486, fut 10.427) | \n",
            "Epoch 008 | ** best ** train loss 20.6663 (now 10.067, fut 10.600) | val loss 20.4775 (now 10.334, fut 10.144) | \n",
            "Epoch 009 | ** best ** train loss 21.3337 (now 10.374, fut 10.960) | val loss 19.8737 (now 10.006, fut 9.868) | \n",
            "Epoch 010 |  train loss 22.0229 (now 10.650, fut 11.373) | val loss 20.7868 (now 10.472, fut 10.315) | \n",
            "Epoch 011 |  train loss 21.7778 (now 10.507, fut 11.271) | val loss 20.1464 (now 10.030, fut 10.116) | \n",
            "Epoch 012 | ** best ** train loss 21.5801 (now 10.419, fut 11.161) | val loss 19.6589 (now 9.860, fut 9.799) | \n",
            "Epoch 013 | ** best ** train loss 21.4748 (now 10.346, fut 11.129) | val loss 19.5593 (now 9.867, fut 9.693) | \n",
            "Epoch 014 |  train loss 21.7424 (now 10.445, fut 11.297) | val loss 19.6984 (now 9.969, fut 9.730) | \n",
            "Epoch 015 | ** best ** train loss 21.9919 (now 10.563, fut 11.429) | val loss 19.1622 (now 9.878, fut 9.284) | \n",
            "Epoch 016 |  train loss 21.0961 (now 10.091, fut 11.005) | val loss 19.6686 (now 9.941, fut 9.727) | \n",
            "Epoch 017 |  train loss 21.9835 (now 10.525, fut 11.459) | val loss 19.0008 (now 9.452, fut 9.549) | \n",
            "Epoch 018 |  train loss 21.4738 (now 10.279, fut 11.194) | val loss 19.0113 (now 9.270, fut 9.741) | \n",
            "Epoch 019 |  train loss 20.9544 (now 10.003, fut 10.951) | val loss 19.1079 (now 9.665, fut 9.443) | \n",
            "Epoch 020 |  train loss 21.0731 (now 10.083, fut 10.991) | val loss 19.0215 (now 9.357, fut 9.665) | \n",
            "Epoch 021 |  train loss 21.2223 (now 10.123, fut 11.099) | val loss 19.3017 (now 9.684, fut 9.618) | \n",
            "Epoch 022 |  train loss 21.1781 (now 10.148, fut 11.030) | val loss 19.7181 (now 9.930, fut 9.788) | \n",
            "Epoch 023 |  train loss 21.2698 (now 10.163, fut 11.107) | val loss 18.5989 (now 9.266, fut 9.333) | \n",
            "Epoch 024 |  train loss 20.7325 (now 9.838, fut 10.895) | val loss 18.6474 (now 9.090, fut 9.557) | \n",
            "Epoch 025 |  train loss 20.8468 (now 9.891, fut 10.956) | val loss 18.6949 (now 9.305, fut 9.390) | \n",
            "Epoch 026 | ** best ** train loss 20.6980 (now 9.869, fut 10.829) | val loss 18.2832 (now 9.094, fut 9.190) | \n",
            "Epoch 027 |  train loss 20.3513 (now 9.714, fut 10.638) | val loss 18.8385 (now 9.415, fut 9.424) | \n",
            "Epoch 028 | ** best ** train loss 19.7557 (now 9.381, fut 10.375) | val loss 18.0722 (now 9.094, fut 8.978) | \n",
            "Epoch 029 |  train loss 19.6453 (now 9.371, fut 10.274) | val loss 18.5650 (now 9.297, fut 9.268) | \n",
            "Epoch 030 |  train loss 19.6645 (now 9.369, fut 10.295) | val loss 17.4984 (now 8.450, fut 9.048) | \n",
            "Epoch 031 |  train loss 19.8257 (now 9.433, fut 10.393) | val loss 18.2116 (now 8.948, fut 9.264) | \n",
            "Epoch 032 |  train loss 19.8315 (now 9.472, fut 10.359) | val loss 18.3452 (now 9.180, fut 9.166) | \n",
            "Epoch 033 |  train loss 19.5488 (now 9.318, fut 10.231) | val loss 17.9920 (now 8.758, fut 9.234) | \n",
            "Epoch 034 |  train loss 19.0434 (now 9.073, fut 9.970) | val loss 18.5446 (now 9.169, fut 9.375) | \n",
            "Epoch 035 |  train loss 19.3046 (now 9.200, fut 10.104) | val loss 19.0691 (now 9.322, fut 9.747) | \n",
            "Epoch 036 |  train loss 19.1745 (now 9.149, fut 10.026) | val loss 18.2356 (now 9.040, fut 9.195) | \n",
            "Epoch 037 |  train loss 19.0138 (now 9.070, fut 9.944) | val loss 17.9828 (now 8.815, fut 9.168) | \n",
            "Epoch 038 |  train loss 18.9930 (now 9.034, fut 9.959) | val loss 17.9171 (now 8.761, fut 9.156) | \n",
            "Epoch 039 |  train loss 18.7278 (now 8.906, fut 9.822) | val loss 18.3242 (now 8.982, fut 9.342) | \n",
            "Epoch 040 |  train loss 18.8058 (now 8.961, fut 9.845) | val loss 18.8181 (now 9.205, fut 9.613) | \n",
            "Epoch 041 |  train loss 18.7173 (now 8.896, fut 9.821) | val loss 18.1165 (now 8.798, fut 9.318) | \n",
            "Epoch 042 | ** best ** train loss 19.0554 (now 9.108, fut 9.948) | val loss 17.5053 (now 8.597, fut 8.908) | \n",
            "Epoch 043 |  train loss 18.8573 (now 8.987, fut 9.870) | val loss 18.2784 (now 9.051, fut 9.227) | \n",
            "Epoch 044 | ** best ** train loss 18.5258 (now 8.783, fut 9.743) | val loss 17.3926 (now 8.552, fut 8.840) | \n",
            "Epoch 045 |  train loss 18.3942 (now 8.740, fut 9.654) | val loss 18.8723 (now 9.443, fut 9.430) | \n",
            "Epoch 046 |  train loss 18.3874 (now 8.693, fut 9.694) | val loss 18.2433 (now 9.000, fut 9.243) | \n",
            "Epoch 047 |  train loss 18.2306 (now 8.655, fut 9.576) | val loss 17.9774 (now 8.859, fut 9.119) | \n",
            "Epoch 048 |  train loss 18.3277 (now 8.728, fut 9.600) | val loss 18.6691 (now 9.246, fut 9.423) | \n",
            "Epoch 049 |  train loss 18.7498 (now 8.915, fut 9.834) | val loss 18.1614 (now 9.056, fut 9.106) | \n",
            "Epoch 050 |  train loss 19.7276 (now 9.355, fut 10.373) | val loss 18.0938 (now 8.702, fut 9.392) | \n",
            "Epoch 051 |  train loss 19.4963 (now 9.255, fut 10.242) | val loss 18.6853 (now 9.167, fut 9.519) | \n",
            "Epoch 052 |  train loss 20.1340 (now 9.616, fut 10.518) | val loss 18.0969 (now 8.788, fut 9.309) | \n",
            "Epoch 053 |  train loss 20.2256 (now 9.604, fut 10.621) | val loss 19.1438 (now 9.167, fut 9.977) | \n",
            "Epoch 054 |  train loss 20.1344 (now 9.616, fut 10.518) | val loss 18.7130 (now 9.262, fut 9.451) | \n",
            "Epoch 055 |  train loss 20.0409 (now 9.562, fut 10.479) | val loss 19.2027 (now 9.548, fut 9.655) | \n",
            "Epoch 056 |  train loss 19.9721 (now 9.496, fut 10.476) | val loss 18.8865 (now 9.358, fut 9.529) | \n",
            "Epoch 057 |  train loss 19.8313 (now 9.441, fut 10.391) | val loss 18.5696 (now 8.998, fut 9.571) | \n",
            "Epoch 058 |  train loss 19.7903 (now 9.420, fut 10.370) | val loss 18.0822 (now 9.006, fut 9.077) | \n",
            "Epoch 059 |  train loss 19.7058 (now 9.361, fut 10.345) | val loss 18.4525 (now 8.927, fut 9.526) | \n",
            "Epoch 060 |  train loss 19.5906 (now 9.322, fut 10.268) | val loss 18.7567 (now 9.184, fut 9.573) | \n",
            "Epoch 061 |  train loss 19.6287 (now 9.387, fut 10.242) | val loss 18.0093 (now 8.640, fut 9.369) | \n",
            "Epoch 062 |  train loss 19.5538 (now 9.325, fut 10.228) | val loss 17.8239 (now 8.775, fut 9.049) | \n",
            "Epoch 063 |  train loss 19.4885 (now 9.275, fut 10.213) | val loss 18.3248 (now 8.987, fut 9.338) | \n",
            "Epoch 064 |  train loss 19.3767 (now 9.225, fut 10.152) | val loss 17.9689 (now 8.793, fut 9.176) | \n",
            "Epoch 065 |  train loss 19.3595 (now 9.249, fut 10.110) | val loss 18.5153 (now 9.109, fut 9.407) | \n",
            "Epoch 066 |  train loss 19.2498 (now 9.139, fut 10.111) | val loss 18.2705 (now 9.000, fut 9.271) | \n",
            "Epoch 067 |  train loss 19.1462 (now 9.126, fut 10.021) | val loss 18.3524 (now 8.992, fut 9.361) | \n",
            "Epoch 068 |  train loss 19.1377 (now 9.077, fut 10.061) | val loss 18.6297 (now 9.145, fut 9.485) | \n",
            "Epoch 069 |  train loss 19.1451 (now 9.084, fut 10.061) | val loss 18.5963 (now 9.216, fut 9.380) | \n",
            "Epoch 070 |  train loss 19.0395 (now 9.071, fut 9.969) | val loss 18.2675 (now 8.829, fut 9.439) | \n",
            "Epoch 071 |  train loss 18.9439 (now 9.019, fut 9.924) | val loss 18.3072 (now 8.990, fut 9.317) | \n",
            "Epoch 072 |  train loss 18.9510 (now 9.011, fut 9.940) | val loss 18.9666 (now 9.393, fut 9.573) | \n",
            "Epoch 073 |  train loss 18.9867 (now 9.032, fut 9.954) | val loss 19.0715 (now 9.411, fut 9.660) | \n",
            "Epoch 074 |  train loss 18.9550 (now 9.040, fut 9.915) | val loss 18.7273 (now 9.306, fut 9.421) | \n",
            "Epoch 075 |  train loss 18.7963 (now 8.960, fut 9.836) | val loss 18.1171 (now 8.881, fut 9.237) | \n",
            "Epoch 076 |  train loss 18.7569 (now 8.963, fut 9.794) | val loss 17.5870 (now 8.695, fut 8.892) | \n",
            "Epoch 077 |  train loss 18.6195 (now 8.902, fut 9.717) | val loss 17.7951 (now 8.795, fut 9.000) | \n",
            "Epoch 078 |  train loss 18.5959 (now 8.826, fut 9.770) | val loss 18.0739 (now 8.711, fut 9.363) | \n",
            "Epoch 079 |  train loss 18.5408 (now 8.797, fut 9.744) | val loss 19.3666 (now 9.695, fut 9.672) | \n",
            "Epoch 080 |  train loss 18.5393 (now 8.831, fut 9.708) | val loss 18.4707 (now 9.185, fut 9.286) | \n",
            "Epoch 081 |  train loss 18.4540 (now 8.799, fut 9.655) | val loss 18.3818 (now 9.145, fut 9.237) | \n",
            "Epoch 082 |  train loss 18.4655 (now 8.835, fut 9.630) | val loss 18.1819 (now 9.080, fut 9.102) | \n",
            "Epoch 083 |  train loss 18.3250 (now 8.734, fut 9.591) | val loss 18.2232 (now 8.865, fut 9.358) | \n",
            "Epoch 084 |  train loss 18.1522 (now 8.644, fut 9.508) | val loss 18.3173 (now 9.046, fut 9.271) | \n",
            "Epoch 085 |  train loss 18.2244 (now 8.655, fut 9.569) | val loss 18.0089 (now 8.747, fut 9.262) | \n",
            "Epoch 086 |  train loss 18.2043 (now 8.667, fut 9.538) | val loss 17.8829 (now 8.611, fut 9.272) | \n",
            "Epoch 087 |  train loss 18.6266 (now 8.851, fut 9.775) | val loss 18.4431 (now 8.990, fut 9.453) | \n",
            "Epoch 088 |  train loss 18.3637 (now 8.730, fut 9.633) | val loss 18.4302 (now 9.094, fut 9.336) | \n",
            "Epoch 089 |  train loss 18.4019 (now 8.725, fut 9.677) | val loss 18.7719 (now 9.162, fut 9.610) | \n",
            "Epoch 090 |  train loss 18.1803 (now 8.635, fut 9.545) | val loss 18.2900 (now 9.000, fut 9.290) | \n",
            "Epoch 091 |  train loss 18.1736 (now 8.612, fut 9.562) | val loss 18.1104 (now 8.909, fut 9.202) | \n",
            "Epoch 092 |  train loss 18.1467 (now 8.643, fut 9.504) | val loss 18.8456 (now 9.437, fut 9.408) | \n",
            "Epoch 093 |  train loss 17.9757 (now 8.523, fut 9.453) | val loss 18.4392 (now 9.016, fut 9.423) | \n",
            "Epoch 094 |  train loss 17.8822 (now 8.484, fut 9.398) | val loss 18.0019 (now 8.882, fut 9.120) | \n",
            "Epoch 095 |  train loss 17.9022 (now 8.539, fut 9.363) | val loss 18.2664 (now 8.782, fut 9.485) | \n",
            "Epoch 096 |  train loss 17.7285 (now 8.423, fut 9.306) | val loss 18.5159 (now 9.157, fut 9.359) | \n",
            "Epoch 097 |  train loss 17.8453 (now 8.464, fut 9.382) | val loss 18.4754 (now 9.120, fut 9.355) | \n",
            "Epoch 098 |  train loss 17.8030 (now 8.447, fut 9.356) | val loss 18.6803 (now 9.004, fut 9.676) | \n",
            "Epoch 099 |  train loss 17.8449 (now 8.527, fut 9.318) | val loss 19.1111 (now 9.553, fut 9.558) | \n",
            "Epoch 100 |  train loss 17.7828 (now 8.438, fut 9.345) | val loss 18.3304 (now 8.978, fut 9.353) | \n",
            "Best validation forecast MAE (S6 fold): 8.8401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 6\n",
        "\n",
        "Best val_MAE for S1-5, S7-15: 17.3926 (100 EPOCHS; reached at epoch 44)\n",
        "\n",
        "- now val_MAE: 8.552\n",
        "- fut val_MAE: 8.8401\n",
        "\n",
        "notes:\n",
        "- overfitting occurring again (beyond circa epoch 65)"
      ],
      "metadata": {
        "id": "48Qu0oT6nAge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 7]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "BsdcvrE-uXZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=7, # test: subject 7\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S7\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S7 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIebNii7ueUp",
        "outputId": "58ae5f77-3c2a-4ce3-ee1e-dbabe53dd397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S7] Train 47136 | Val 11861 | Test 4638 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 150.1076 (now 74.593, fut 75.515) | val loss 102.1986 (now 50.896, fut 51.302) | \n",
            "Epoch 002 | ** best ** train loss 41.6237 (now 20.527, fut 21.096) | val loss 46.4067 (now 23.365, fut 23.042) | \n",
            "Epoch 003 | ** best ** train loss 36.8147 (now 18.994, fut 17.820) | val loss 31.8956 (now 16.010, fut 15.886) | \n",
            "Epoch 004 | ** best ** train loss 30.6596 (now 15.478, fut 15.182) | val loss 28.4163 (now 14.315, fut 14.101) | \n",
            "Epoch 005 | ** best ** train loss 28.5025 (now 14.324, fut 14.178) | val loss 27.2175 (now 13.786, fut 13.431) | \n",
            "Epoch 006 |  train loss 26.9465 (now 13.580, fut 13.366) | val loss 28.6623 (now 14.470, fut 14.192) | \n",
            "Epoch 007 | ** best ** train loss 28.2265 (now 14.247, fut 13.979) | val loss 24.8476 (now 12.562, fut 12.286) | \n",
            "Epoch 008 | ** best ** train loss 26.0212 (now 13.171, fut 12.850) | val loss 24.4218 (now 12.519, fut 11.903) | \n",
            "Epoch 009 | ** best ** train loss 26.5258 (now 13.426, fut 13.100) | val loss 23.4540 (now 12.157, fut 11.297) | \n",
            "Epoch 010 |  train loss 26.1049 (now 13.255, fut 12.850) | val loss 22.8839 (now 11.586, fut 11.298) | \n",
            "Epoch 011 | ** best ** train loss 25.1257 (now 12.772, fut 12.354) | val loss 21.8599 (now 11.083, fut 10.777) | \n",
            "Epoch 012 |  train loss 24.9546 (now 12.628, fut 12.327) | val loss 22.5887 (now 11.536, fut 11.053) | \n",
            "Epoch 013 |  train loss 24.6380 (now 12.512, fut 12.126) | val loss 22.2176 (now 11.371, fut 10.847) | \n",
            "Epoch 014 | ** best ** train loss 24.2954 (now 12.377, fut 11.918) | val loss 21.5919 (now 11.017, fut 10.574) | \n",
            "Epoch 015 |  train loss 24.1536 (now 12.290, fut 11.863) | val loss 21.4724 (now 10.747, fut 10.725) | \n",
            "Epoch 016 |  train loss 23.8390 (now 12.095, fut 11.744) | val loss 22.3849 (now 11.444, fut 10.941) | \n",
            "Epoch 017 | ** best ** train loss 22.8394 (now 11.553, fut 11.287) | val loss 21.2472 (now 10.898, fut 10.349) | \n",
            "Epoch 018 | ** best ** train loss 22.8908 (now 11.616, fut 11.275) | val loss 20.4161 (now 10.427, fut 9.989) | \n",
            "Epoch 019 |  train loss 22.2944 (now 11.235, fut 11.059) | val loss 20.7082 (now 10.496, fut 10.212) | \n",
            "Epoch 020 |  train loss 22.1660 (now 11.204, fut 10.962) | val loss 20.5863 (now 10.512, fut 10.074) | \n",
            "Epoch 021 |  train loss 20.9825 (now 10.488, fut 10.494) | val loss 20.3288 (now 10.303, fut 10.026) | \n",
            "Epoch 022 |  train loss 20.4369 (now 10.213, fut 10.224) | val loss 20.8234 (now 10.556, fut 10.267) | \n",
            "Epoch 023 | ** best ** train loss 20.7286 (now 10.338, fut 10.391) | val loss 20.3141 (now 10.396, fut 9.918) | \n",
            "Epoch 024 |  train loss 21.0731 (now 10.604, fut 10.469) | val loss 20.4102 (now 10.183, fut 10.228) | \n",
            "Epoch 025 |  train loss 20.9802 (now 10.538, fut 10.442) | val loss 20.5218 (now 10.373, fut 10.149) | \n",
            "Epoch 026 |  train loss 21.5912 (now 10.902, fut 10.689) | val loss 20.6911 (now 10.535, fut 10.156) | \n",
            "Epoch 027 |  train loss 21.5046 (now 10.855, fut 10.649) | val loss 20.1958 (now 10.254, fut 9.941) | \n",
            "Epoch 028 |  train loss 21.9077 (now 11.081, fut 10.827) | val loss 20.7656 (now 10.788, fut 9.977) | \n",
            "Epoch 029 |  train loss 20.9740 (now 10.584, fut 10.390) | val loss 20.2663 (now 10.201, fut 10.066) | \n",
            "Epoch 030 |  train loss 20.3224 (now 10.192, fut 10.131) | val loss 20.6965 (now 10.540, fut 10.157) | \n",
            "Epoch 031 | ** best ** train loss 19.8320 (now 9.922, fut 9.910) | val loss 19.5852 (now 9.830, fut 9.755) | \n",
            "Epoch 032 |  train loss 19.9077 (now 9.994, fut 9.914) | val loss 20.0799 (now 10.152, fut 9.927) | \n",
            "Epoch 033 |  train loss 19.8222 (now 9.925, fut 9.897) | val loss 19.7959 (now 10.029, fut 9.767) | \n",
            "Epoch 034 | ** best ** train loss 19.3955 (now 9.711, fut 9.685) | val loss 18.9813 (now 9.463, fut 9.518) | \n",
            "Epoch 035 |  train loss 19.0608 (now 9.519, fut 9.542) | val loss 19.3428 (now 9.576, fut 9.767) | \n",
            "Epoch 036 |  train loss 18.4220 (now 9.148, fut 9.274) | val loss 19.5872 (now 9.815, fut 9.772) | \n",
            "Epoch 037 |  train loss 17.5361 (now 8.658, fut 8.878) | val loss 18.9042 (now 9.366, fut 9.538) | \n",
            "Epoch 038 |  train loss 18.0325 (now 8.941, fut 9.092) | val loss 19.1482 (now 9.564, fut 9.585) | \n",
            "Epoch 039 |  train loss 18.1142 (now 8.989, fut 9.125) | val loss 20.7470 (now 10.678, fut 10.069) | \n",
            "Epoch 040 |  train loss 18.0849 (now 8.981, fut 9.104) | val loss 19.6252 (now 9.775, fut 9.850) | \n",
            "Epoch 041 |  train loss 18.0503 (now 8.977, fut 9.073) | val loss 19.3286 (now 9.671, fut 9.657) | \n",
            "Epoch 042 |  train loss 17.7850 (now 8.852, fut 8.933) | val loss 19.5267 (now 9.830, fut 9.697) | \n",
            "Epoch 043 |  train loss 17.0581 (now 8.391, fut 8.667) | val loss 20.3588 (now 10.474, fut 9.885) | \n",
            "Epoch 044 |  train loss 17.0017 (now 8.374, fut 8.628) | val loss 19.3810 (now 9.610, fut 9.771) | \n",
            "Epoch 045 |  train loss 16.6783 (now 8.192, fut 8.487) | val loss 19.7100 (now 9.998, fut 9.712) | \n",
            "Epoch 046 | ** best ** train loss 16.5587 (now 8.106, fut 8.452) | val loss 18.8216 (now 9.351, fut 9.471) | \n",
            "Epoch 047 |  train loss 15.6575 (now 7.628, fut 8.029) | val loss 19.0133 (now 9.505, fut 9.508) | \n",
            "Epoch 048 |  train loss 15.6210 (now 7.604, fut 8.017) | val loss 19.4300 (now 9.702, fut 9.728) | \n",
            "Epoch 049 |  train loss 15.3175 (now 7.416, fut 7.901) | val loss 19.3795 (now 9.758, fut 9.622) | \n",
            "Epoch 050 | ** best ** train loss 15.2954 (now 7.447, fut 7.849) | val loss 18.7720 (now 9.371, fut 9.401) | \n",
            "Epoch 051 | ** best ** train loss 14.8758 (now 7.171, fut 7.705) | val loss 18.6568 (now 9.306, fut 9.350) | \n",
            "Epoch 052 |  train loss 15.6986 (now 7.667, fut 8.031) | val loss 19.1041 (now 9.516, fut 9.588) | \n",
            "Epoch 053 |  train loss 15.7005 (now 7.631, fut 8.070) | val loss 18.8432 (now 9.324, fut 9.519) | \n",
            "Epoch 054 |  train loss 15.7397 (now 7.681, fut 8.059) | val loss 19.5613 (now 9.829, fut 9.732) | \n",
            "Epoch 055 |  train loss 16.2025 (now 7.940, fut 8.262) | val loss 18.7780 (now 9.302, fut 9.476) | \n",
            "Epoch 056 |  train loss 17.0095 (now 8.391, fut 8.618) | val loss 20.5067 (now 10.167, fut 10.339) | \n",
            "Epoch 057 |  train loss 17.5850 (now 8.622, fut 8.963) | val loss 21.8639 (now 10.904, fut 10.959) | \n",
            "Epoch 058 |  train loss 18.3716 (now 9.060, fut 9.312) | val loss 21.1267 (now 10.561, fut 10.566) | \n",
            "Epoch 059 |  train loss 21.8196 (now 10.915, fut 10.904) | val loss 24.4026 (now 12.577, fut 11.826) | \n",
            "Epoch 060 |  train loss 24.3298 (now 12.324, fut 12.006) | val loss 23.2807 (now 11.840, fut 11.440) | \n",
            "Epoch 061 |  train loss 25.8000 (now 13.101, fut 12.699) | val loss 24.4532 (now 12.626, fut 11.827) | \n",
            "Epoch 062 |  train loss 26.4059 (now 13.436, fut 12.970) | val loss 24.3281 (now 12.343, fut 11.985) | \n",
            "Epoch 063 |  train loss 26.0452 (now 13.205, fut 12.841) | val loss 24.5834 (now 12.580, fut 12.004) | \n",
            "Epoch 064 |  train loss 26.0369 (now 13.095, fut 12.941) | val loss 27.1252 (now 13.639, fut 13.487) | \n",
            "Epoch 065 |  train loss 26.2681 (now 13.047, fut 13.221) | val loss 27.6344 (now 13.811, fut 13.824) | \n",
            "Epoch 066 |  train loss 26.8598 (now 13.308, fut 13.552) | val loss 27.6530 (now 13.814, fut 13.839) | \n",
            "Epoch 067 |  train loss 27.0622 (now 13.513, fut 13.549) | val loss 26.0932 (now 13.172, fut 12.921) | \n",
            "Epoch 068 |  train loss 26.2824 (now 13.187, fut 13.096) | val loss 25.7186 (now 13.200, fut 12.519) | \n",
            "Epoch 069 |  train loss 26.1443 (now 13.154, fut 12.991) | val loss 24.7505 (now 12.598, fut 12.153) | \n",
            "Epoch 070 |  train loss 26.0876 (now 13.173, fut 12.915) | val loss 24.6464 (now 12.420, fut 12.227) | \n",
            "Epoch 071 |  train loss 25.6885 (now 12.933, fut 12.756) | val loss 24.0372 (now 12.110, fut 11.927) | \n",
            "Epoch 072 |  train loss 24.8367 (now 12.485, fut 12.352) | val loss 23.8784 (now 11.882, fut 11.996) | \n",
            "Epoch 073 |  train loss 24.3727 (now 12.321, fut 12.052) | val loss 23.9908 (now 12.138, fut 11.853) | \n",
            "Epoch 074 |  train loss 24.0215 (now 12.154, fut 11.867) | val loss 24.2280 (now 12.265, fut 11.963) | \n",
            "Epoch 075 |  train loss 23.8117 (now 12.110, fut 11.701) | val loss 22.8962 (now 11.674, fut 11.223) | \n",
            "Epoch 076 |  train loss 23.4193 (now 11.948, fut 11.472) | val loss 22.5247 (now 11.281, fut 11.243) | \n",
            "Epoch 077 |  train loss 23.1964 (now 11.830, fut 11.366) | val loss 23.3607 (now 11.962, fut 11.399) | \n",
            "Epoch 078 |  train loss 22.8101 (now 11.623, fut 11.187) | val loss 21.9125 (now 10.997, fut 10.916) | \n",
            "Epoch 079 |  train loss 22.3884 (now 11.394, fut 10.995) | val loss 21.5050 (now 10.913, fut 10.591) | \n",
            "Epoch 080 |  train loss 22.1492 (now 11.319, fut 10.831) | val loss 21.7888 (now 11.073, fut 10.716) | \n",
            "Epoch 081 |  train loss 22.0179 (now 11.122, fut 10.895) | val loss 21.8990 (now 11.194, fut 10.705) | \n",
            "Epoch 082 |  train loss 21.5414 (now 10.843, fut 10.698) | val loss 21.1071 (now 10.725, fut 10.382) | \n",
            "Epoch 083 |  train loss 21.3913 (now 10.785, fut 10.606) | val loss 21.0203 (now 10.666, fut 10.354) | \n",
            "Epoch 084 |  train loss 20.9385 (now 10.565, fut 10.373) | val loss 21.0318 (now 10.632, fut 10.400) | \n",
            "Epoch 085 |  train loss 20.7850 (now 10.431, fut 10.354) | val loss 20.9742 (now 10.502, fut 10.472) | \n",
            "Epoch 086 |  train loss 20.2897 (now 10.146, fut 10.144) | val loss 21.2633 (now 10.763, fut 10.500) | \n",
            "Epoch 087 |  train loss 20.1508 (now 10.057, fut 10.094) | val loss 20.9979 (now 10.678, fut 10.320) | \n",
            "Epoch 088 |  train loss 19.9108 (now 9.977, fut 9.934) | val loss 20.5438 (now 10.310, fut 10.233) | \n",
            "Epoch 089 |  train loss 19.5401 (now 9.840, fut 9.700) | val loss 20.9320 (now 10.567, fut 10.365) | \n",
            "Epoch 090 |  train loss 19.2710 (now 9.700, fut 9.571) | val loss 20.6138 (now 10.203, fut 10.411) | \n",
            "Epoch 091 |  train loss 19.1966 (now 9.655, fut 9.542) | val loss 20.7196 (now 10.499, fut 10.221) | \n",
            "Epoch 092 |  train loss 18.9866 (now 9.651, fut 9.335) | val loss 21.0848 (now 10.472, fut 10.613) | \n",
            "Epoch 093 |  train loss 18.8057 (now 9.512, fut 9.294) | val loss 20.4477 (now 10.248, fut 10.199) | \n",
            "Epoch 094 |  train loss 18.6985 (now 9.476, fut 9.222) | val loss 20.2124 (now 10.082, fut 10.130) | \n",
            "Epoch 095 |  train loss 18.4877 (now 9.362, fut 9.126) | val loss 20.0243 (now 10.010, fut 10.015) | \n",
            "Epoch 096 |  train loss 18.2641 (now 9.156, fut 9.108) | val loss 20.4068 (now 10.269, fut 10.138) | \n",
            "Epoch 097 |  train loss 17.8566 (now 9.001, fut 8.856) | val loss 21.3481 (now 10.753, fut 10.595) | \n",
            "Epoch 098 |  train loss 17.3299 (now 8.632, fut 8.698) | val loss 20.2251 (now 10.145, fut 10.081) | \n",
            "Epoch 099 |  train loss 17.2163 (now 8.556, fut 8.660) | val loss 20.4402 (now 10.221, fut 10.220) | \n",
            "Epoch 100 |  train loss 17.0555 (now 8.390, fut 8.666) | val loss 19.7259 (now 9.875, fut 9.851) | \n",
            "Best validation forecast MAE (S7 fold): 9.3504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 7\n",
        "\n",
        "Best val_MAE for S1-6, S8-15: 18.6568 (100 EPOCHS; reached at epoch 51)\n",
        "\n",
        "- now val_MAE: 9.306\n",
        "- fut val_MAE: 9.3504\n",
        "\n",
        "notes:\n",
        "- weird spike in loss at circa epoch 58, however eventually began to decrease again. potentially due to gradient overshooting on outlier windows...?"
      ],
      "metadata": {
        "id": "GmW5bFCpuh5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 8]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "fNC55IIU1peB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=8, # test: subject 8\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S8\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S8 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XjK5k7h1rSC",
        "outputId": "227fa707-acde-40e6-82bc-e99ca1682f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S8] Train 47641 | Val 11987 | Test 4007 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 149.8617 (now 75.122, fut 74.740) | val loss 98.9670 (now 50.775, fut 48.192) | \n",
            "Epoch 002 | ** best ** train loss 39.6837 (now 19.898, fut 19.786) | val loss 32.4760 (now 16.237, fut 16.239) | \n",
            "Epoch 003 |  train loss 35.6752 (now 16.113, fut 19.562) | val loss 32.3265 (now 16.060, fut 16.266) | \n",
            "Epoch 004 | ** best ** train loss 30.5491 (now 14.744, fut 15.805) | val loss 28.7630 (now 14.200, fut 14.563) | \n",
            "Epoch 005 | ** best ** train loss 26.6215 (now 12.727, fut 13.895) | val loss 24.5898 (now 12.278, fut 12.311) | \n",
            "Epoch 006 | ** best ** train loss 23.5019 (now 11.335, fut 12.167) | val loss 23.3568 (now 11.502, fut 11.855) | \n",
            "Epoch 007 | ** best ** train loss 22.1649 (now 10.629, fut 11.536) | val loss 23.3406 (now 11.497, fut 11.844) | \n",
            "Epoch 008 | ** best ** train loss 21.0653 (now 10.125, fut 10.940) | val loss 20.6984 (now 10.198, fut 10.500) | \n",
            "Epoch 009 |  train loss 19.9689 (now 9.526, fut 10.443) | val loss 21.0938 (now 10.261, fut 10.833) | \n",
            "Epoch 010 |  train loss 21.0043 (now 9.867, fut 11.138) | val loss 21.1670 (now 10.498, fut 10.669) | \n",
            "Epoch 011 | ** best ** train loss 18.8136 (now 8.874, fut 9.940) | val loss 19.8993 (now 9.638, fut 10.261) | \n",
            "Epoch 012 | ** best ** train loss 18.6915 (now 8.758, fut 9.934) | val loss 19.7893 (now 9.552, fut 10.237) | \n",
            "Epoch 013 | ** best ** train loss 17.9703 (now 8.398, fut 9.572) | val loss 19.0950 (now 9.095, fut 10.000) | \n",
            "Epoch 014 | ** best ** train loss 18.0475 (now 8.321, fut 9.726) | val loss 19.1431 (now 9.249, fut 9.895) | \n",
            "Epoch 015 |  train loss 18.0087 (now 8.314, fut 9.695) | val loss 18.9543 (now 9.040, fut 9.914) | \n",
            "Epoch 016 | ** best ** train loss 17.1748 (now 7.929, fut 9.246) | val loss 18.4054 (now 8.755, fut 9.651) | \n",
            "Epoch 017 | ** best ** train loss 16.7373 (now 7.727, fut 9.010) | val loss 17.8573 (now 8.371, fut 9.487) | \n",
            "Epoch 018 | ** best ** train loss 16.4609 (now 7.587, fut 8.874) | val loss 17.1537 (now 8.000, fut 9.154) | \n",
            "Epoch 019 |  train loss 15.8081 (now 7.252, fut 8.556) | val loss 17.6924 (now 8.203, fut 9.490) | \n",
            "Epoch 020 |  train loss 15.6045 (now 7.166, fut 8.439) | val loss 17.1029 (now 7.836, fut 9.267) | \n",
            "Epoch 021 |  train loss 15.7451 (now 7.167, fut 8.578) | val loss 17.6409 (now 8.164, fut 9.477) | \n",
            "Epoch 022 |  train loss 16.0366 (now 7.304, fut 8.733) | val loss 17.6857 (now 8.124, fut 9.562) | \n",
            "Epoch 023 |  train loss 15.7635 (now 7.179, fut 8.585) | val loss 17.1979 (now 7.791, fut 9.407) | \n",
            "Epoch 024 |  train loss 15.0914 (now 6.835, fut 8.257) | val loss 16.7250 (now 7.508, fut 9.217) | \n",
            "Epoch 025 | ** best ** train loss 14.3441 (now 6.556, fut 7.788) | val loss 16.7419 (now 7.762, fut 8.980) | \n",
            "Epoch 026 | ** best ** train loss 14.0497 (now 6.405, fut 7.645) | val loss 16.5406 (now 7.589, fut 8.952) | \n",
            "Epoch 027 |  train loss 13.6830 (now 6.217, fut 7.466) | val loss 16.9042 (now 7.660, fut 9.244) | \n",
            "Epoch 028 | ** best ** train loss 13.4631 (now 6.107, fut 7.356) | val loss 16.5549 (now 7.645, fut 8.910) | \n",
            "Epoch 029 | ** best ** train loss 13.2628 (now 6.062, fut 7.201) | val loss 16.3923 (now 7.509, fut 8.884) | \n",
            "Epoch 030 | ** best ** train loss 13.1091 (now 5.958, fut 7.151) | val loss 16.2020 (now 7.379, fut 8.823) | \n",
            "Epoch 031 |  train loss 12.9280 (now 5.881, fut 7.047) | val loss 16.4300 (now 7.499, fut 8.931) | \n",
            "Epoch 032 |  train loss 12.8677 (now 5.848, fut 7.020) | val loss 16.6115 (now 7.662, fut 8.950) | \n",
            "Epoch 033 | ** best ** train loss 12.7676 (now 5.776, fut 6.992) | val loss 15.7261 (now 7.026, fut 8.700) | \n",
            "Epoch 034 |  train loss 12.7041 (now 5.753, fut 6.951) | val loss 16.3578 (now 7.598, fut 8.759) | \n",
            "Epoch 035 |  train loss 12.5599 (now 5.662, fut 6.898) | val loss 16.3776 (now 7.443, fut 8.934) | \n",
            "Epoch 036 |  train loss 12.4643 (now 5.632, fut 6.833) | val loss 16.1600 (now 7.179, fut 8.981) | \n",
            "Epoch 037 |  train loss 12.4588 (now 5.631, fut 6.828) | val loss 16.2096 (now 7.366, fut 8.844) | \n",
            "Epoch 038 |  train loss 12.4164 (now 5.633, fut 6.783) | val loss 16.0475 (now 7.056, fut 8.992) | \n",
            "Epoch 039 |  train loss 12.2948 (now 5.574, fut 6.721) | val loss 16.5584 (now 7.463, fut 9.095) | \n",
            "Epoch 040 |  train loss 12.1956 (now 5.493, fut 6.702) | val loss 15.5724 (now 6.785, fut 8.787) | \n",
            "Epoch 041 |  train loss 12.1819 (now 5.509, fut 6.673) | val loss 15.9482 (now 6.997, fut 8.952) | \n",
            "Epoch 042 | ** best ** train loss 12.0174 (now 5.413, fut 6.604) | val loss 15.5903 (now 6.896, fut 8.694) | \n",
            "Epoch 043 |  train loss 12.0876 (now 5.461, fut 6.627) | val loss 16.5149 (now 7.326, fut 9.189) | \n",
            "Epoch 044 |  train loss 11.9764 (now 5.384, fut 6.593) | val loss 16.2129 (now 7.282, fut 8.931) | \n",
            "Epoch 045 |  train loss 12.0043 (now 5.392, fut 6.613) | val loss 15.9785 (now 7.097, fut 8.882) | \n",
            "Epoch 046 |  train loss 11.8561 (now 5.321, fut 6.535) | val loss 16.2331 (now 7.245, fut 8.988) | \n",
            "Epoch 047 |  train loss 11.8366 (now 5.342, fut 6.495) | val loss 16.3621 (now 7.272, fut 9.090) | \n",
            "Epoch 048 |  train loss 11.8054 (now 5.296, fut 6.510) | val loss 15.8555 (now 6.985, fut 8.871) | \n",
            "Epoch 049 |  train loss 11.8203 (now 5.348, fut 6.472) | val loss 15.6704 (now 6.858, fut 8.813) | \n",
            "Epoch 050 |  train loss 11.7126 (now 5.281, fut 6.432) | val loss 15.7869 (now 6.854, fut 8.932) | \n",
            "Epoch 051 |  train loss 11.7441 (now 5.288, fut 6.456) | val loss 15.5597 (now 6.713, fut 8.846) | \n",
            "Epoch 052 |  train loss 11.6536 (now 5.254, fut 6.399) | val loss 16.2729 (now 7.473, fut 8.800) | \n",
            "Epoch 053 |  train loss 11.5487 (now 5.180, fut 6.369) | val loss 15.8717 (now 6.920, fut 8.952) | \n",
            "Epoch 054 |  train loss 11.4863 (now 5.168, fut 6.318) | val loss 16.1093 (now 6.964, fut 9.145) | \n",
            "Epoch 055 |  train loss 11.4684 (now 5.147, fut 6.321) | val loss 15.7015 (now 6.751, fut 8.950) | \n",
            "Epoch 056 |  train loss 11.4538 (now 5.149, fut 6.304) | val loss 16.2651 (now 7.252, fut 9.013) | \n",
            "Epoch 057 |  train loss 11.4156 (now 5.148, fut 6.267) | val loss 15.7049 (now 6.801, fut 8.904) | \n",
            "Epoch 058 |  train loss 11.3595 (now 5.117, fut 6.243) | val loss 15.6336 (now 6.775, fut 8.859) | \n",
            "Epoch 059 |  train loss 11.3791 (now 5.092, fut 6.288) | val loss 15.6985 (now 6.651, fut 9.047) | \n",
            "Epoch 060 |  train loss 11.2892 (now 5.077, fut 6.212) | val loss 16.1946 (now 7.185, fut 9.010) | \n",
            "Epoch 061 |  train loss 11.2627 (now 5.058, fut 6.205) | val loss 15.5974 (now 6.656, fut 8.941) | \n",
            "Epoch 062 |  train loss 11.2418 (now 5.065, fut 6.177) | val loss 16.6959 (now 7.447, fut 9.249) | \n",
            "Epoch 063 |  train loss 11.2043 (now 5.023, fut 6.181) | val loss 15.9309 (now 6.905, fut 9.026) | \n",
            "Epoch 064 |  train loss 11.1490 (now 5.016, fut 6.133) | val loss 15.6236 (now 6.691, fut 8.932) | \n",
            "Epoch 065 |  train loss 11.0846 (now 4.981, fut 6.104) | val loss 15.6900 (now 6.849, fut 8.841) | \n",
            "Epoch 066 |  train loss 11.0801 (now 5.003, fut 6.077) | val loss 15.3565 (now 6.587, fut 8.770) | \n",
            "Epoch 067 |  train loss 11.0532 (now 4.982, fut 6.071) | val loss 15.5118 (now 6.604, fut 8.908) | \n",
            "Epoch 068 |  train loss 10.9671 (now 4.927, fut 6.040) | val loss 15.9033 (now 6.913, fut 8.990) | \n",
            "Epoch 069 |  train loss 10.9591 (now 4.919, fut 6.040) | val loss 15.6287 (now 6.745, fut 8.884) | \n",
            "Epoch 070 |  train loss 11.0130 (now 4.976, fut 6.037) | val loss 16.4807 (now 7.100, fut 9.380) | \n",
            "Epoch 071 |  train loss 10.9713 (now 4.946, fut 6.026) | val loss 15.8128 (now 6.755, fut 9.058) | \n",
            "Epoch 072 |  train loss 10.9610 (now 4.956, fut 6.005) | val loss 15.3638 (now 6.471, fut 8.892) | \n",
            "Epoch 073 |  train loss 10.8521 (now 4.894, fut 5.958) | val loss 15.8669 (now 6.993, fut 8.874) | \n",
            "Epoch 074 |  train loss 10.9291 (now 4.931, fut 5.998) | val loss 15.9984 (now 6.815, fut 9.184) | \n",
            "Epoch 075 |  train loss 10.8274 (now 4.889, fut 5.939) | val loss 16.1423 (now 6.935, fut 9.207) | \n",
            "Epoch 076 |  train loss 10.7875 (now 4.863, fut 5.925) | val loss 15.6359 (now 6.606, fut 9.030) | \n",
            "Epoch 077 |  train loss 10.7151 (now 4.840, fut 5.875) | val loss 15.4056 (now 6.503, fut 8.903) | \n",
            "Epoch 078 |  train loss 10.7520 (now 4.843, fut 5.909) | val loss 15.5271 (now 6.573, fut 8.954) | \n",
            "Epoch 079 |  train loss 10.7040 (now 4.825, fut 5.879) | val loss 15.7908 (now 6.788, fut 9.003) | \n",
            "Epoch 080 |  train loss 10.6289 (now 4.806, fut 5.823) | val loss 16.0572 (now 6.958, fut 9.099) | \n",
            "Epoch 081 |  train loss 10.6643 (now 4.824, fut 5.840) | val loss 16.1615 (now 7.149, fut 9.013) | \n",
            "Epoch 082 |  train loss 10.6503 (now 4.800, fut 5.851) | val loss 15.9302 (now 6.769, fut 9.161) | \n",
            "Epoch 083 |  train loss 10.5538 (now 4.767, fut 5.787) | val loss 15.8544 (now 6.832, fut 9.023) | \n",
            "Epoch 084 |  train loss 10.5684 (now 4.787, fut 5.782) | val loss 16.1815 (now 6.798, fut 9.384) | \n",
            "Epoch 085 |  train loss 10.5636 (now 4.784, fut 5.780) | val loss 16.3781 (now 7.106, fut 9.272) | \n",
            "Epoch 086 |  train loss 10.5137 (now 4.741, fut 5.772) | val loss 15.6512 (now 6.788, fut 8.863) | \n",
            "Epoch 087 |  train loss 10.4432 (now 4.708, fut 5.735) | val loss 15.4472 (now 6.574, fut 8.873) | \n",
            "Epoch 088 |  train loss 10.4925 (now 4.752, fut 5.740) | val loss 15.5831 (now 6.616, fut 8.968) | \n",
            "Epoch 089 |  train loss 10.4408 (now 4.718, fut 5.723) | val loss 15.8135 (now 6.706, fut 9.108) | \n",
            "Epoch 090 |  train loss 10.4036 (now 4.722, fut 5.682) | val loss 15.4130 (now 6.596, fut 8.817) | \n",
            "Epoch 091 |  train loss 10.3980 (now 4.683, fut 5.715) | val loss 16.5038 (now 7.226, fut 9.278) | \n",
            "Epoch 092 |  train loss 10.3255 (now 4.689, fut 5.636) | val loss 16.0721 (now 6.765, fut 9.307) | \n",
            "Epoch 093 |  train loss 10.3167 (now 4.647, fut 5.670) | val loss 15.8307 (now 6.694, fut 9.136) | \n",
            "Epoch 094 |  train loss 10.3486 (now 4.685, fut 5.664) | val loss 16.1151 (now 6.803, fut 9.312) | \n",
            "Epoch 095 |  train loss 10.3130 (now 4.656, fut 5.657) | val loss 15.3894 (now 6.527, fut 8.862) | \n",
            "Epoch 096 |  train loss 10.2906 (now 4.628, fut 5.663) | val loss 16.2191 (now 7.050, fut 9.169) | \n",
            "Epoch 097 |  train loss 10.2505 (now 4.630, fut 5.620) | val loss 15.3551 (now 6.464, fut 8.891) | \n",
            "Epoch 098 |  train loss 10.2153 (now 4.621, fut 5.594) | val loss 15.9507 (now 6.718, fut 9.232) | \n",
            "Epoch 099 |  train loss 10.2159 (now 4.623, fut 5.593) | val loss 15.7544 (now 6.758, fut 8.997) | \n",
            "Epoch 100 |  train loss 10.1305 (now 4.590, fut 5.540) | val loss 15.7698 (now 6.645, fut 9.125) | \n",
            "Best validation forecast MAE (S8 fold): 8.6939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 8\n",
        "\n",
        "Best val_MAE for S1-7, S9-15: 15.5903 (100 EPOCHS; reached at epoch 42)\n",
        "\n",
        "- now val_MAE: 6.896\n",
        "- fut val_MAE: 8.6939\n",
        "\n",
        "notes:\n",
        "- dataset without subject 8 is well decipherable and best so far - similar to results from subject 5.\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "25xwKL3G2HVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 9]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "00EQYJt65oFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=9, # test: subject 9\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S9\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S9 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHPx-Xti5pv-",
        "outputId": "fbd0b09a-1003-497b-91ad-9c0ef7f38d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S9] Train 47449 | Val 11939 | Test 4247 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 149.3989 (now 74.341, fut 75.058) | val loss 99.7991 (now 49.976, fut 49.823) | \n",
            "Epoch 002 | ** best ** train loss 40.0179 (now 19.835, fut 20.183) | val loss 36.8562 (now 18.535, fut 18.321) | \n",
            "Epoch 003 | ** best ** train loss 31.8574 (now 15.890, fut 15.967) | val loss 31.6492 (now 15.846, fut 15.803) | \n",
            "Epoch 004 | ** best ** train loss 28.5001 (now 14.051, fut 14.449) | val loss 31.2060 (now 15.724, fut 15.482) | \n",
            "Epoch 005 | ** best ** train loss 29.7726 (now 14.738, fut 15.035) | val loss 30.1728 (now 15.154, fut 15.019) | \n",
            "Epoch 006 | ** best ** train loss 26.8577 (now 13.270, fut 13.588) | val loss 25.7356 (now 13.064, fut 12.672) | \n",
            "Epoch 007 | ** best ** train loss 24.5772 (now 12.181, fut 12.396) | val loss 24.2538 (now 12.196, fut 12.058) | \n",
            "Epoch 008 | ** best ** train loss 24.2524 (now 12.027, fut 12.226) | val loss 23.8139 (now 12.060, fut 11.754) | \n",
            "Epoch 009 | ** best ** train loss 22.9945 (now 11.410, fut 11.584) | val loss 22.3662 (now 11.302, fut 11.065) | \n",
            "Epoch 010 | ** best ** train loss 21.6742 (now 10.714, fut 10.960) | val loss 21.6925 (now 10.886, fut 10.807) | \n",
            "Epoch 011 | ** best ** train loss 20.9183 (now 10.317, fut 10.602) | val loss 20.9641 (now 10.505, fut 10.459) | \n",
            "Epoch 012 | ** best ** train loss 19.7959 (now 9.723, fut 10.073) | val loss 20.9405 (now 10.493, fut 10.447) | \n",
            "Epoch 013 | ** best ** train loss 19.9096 (now 9.784, fut 10.126) | val loss 20.9555 (now 10.580, fut 10.375) | \n",
            "Epoch 014 | ** best ** train loss 19.2507 (now 9.408, fut 9.843) | val loss 20.0659 (now 9.995, fut 10.071) | \n",
            "Epoch 015 | ** best ** train loss 18.3013 (now 8.925, fut 9.377) | val loss 19.0314 (now 9.539, fut 9.493) | \n",
            "Epoch 016 |  train loss 17.3888 (now 8.389, fut 9.000) | val loss 18.7779 (now 9.268, fut 9.510) | \n",
            "Epoch 017 |  train loss 16.4052 (now 7.834, fut 8.571) | val loss 18.7530 (now 9.214, fut 9.539) | \n",
            "Epoch 018 | ** best ** train loss 15.6944 (now 7.426, fut 8.268) | val loss 17.8104 (now 8.550, fut 9.261) | \n",
            "Epoch 019 |  train loss 16.1069 (now 7.627, fut 8.480) | val loss 18.9051 (now 9.291, fut 9.614) | \n",
            "Epoch 020 |  train loss 16.6467 (now 7.924, fut 8.722) | val loss 18.3758 (now 8.900, fut 9.476) | \n",
            "Epoch 021 |  train loss 16.4734 (now 7.823, fut 8.650) | val loss 18.5072 (now 8.946, fut 9.561) | \n",
            "Epoch 022 | ** best ** train loss 15.5922 (now 7.338, fut 8.254) | val loss 17.4747 (now 8.328, fut 9.147) | \n",
            "Epoch 023 | ** best ** train loss 14.1739 (now 6.533, fut 7.641) | val loss 16.5722 (now 7.693, fut 8.880) | \n",
            "Epoch 024 |  train loss 13.7517 (now 6.278, fut 7.473) | val loss 17.2562 (now 8.129, fut 9.127) | \n",
            "Epoch 025 |  train loss 13.5773 (now 6.204, fut 7.373) | val loss 17.0227 (now 7.983, fut 9.039) | \n",
            "Epoch 026 | ** best ** train loss 13.3751 (now 6.082, fut 7.293) | val loss 16.2021 (now 7.490, fut 8.712) | \n",
            "Epoch 027 |  train loss 13.2650 (now 6.019, fut 7.246) | val loss 16.0552 (now 7.260, fut 8.796) | \n",
            "Epoch 028 | ** best ** train loss 13.0454 (now 5.906, fut 7.139) | val loss 16.4862 (now 7.806, fut 8.680) | \n",
            "Epoch 029 |  train loss 12.8468 (now 5.782, fut 7.064) | val loss 15.8696 (now 7.181, fut 8.688) | \n",
            "Epoch 030 |  train loss 12.8051 (now 5.775, fut 7.030) | val loss 15.9479 (now 7.241, fut 8.707) | \n",
            "Epoch 031 |  train loss 12.6612 (now 5.682, fut 6.979) | val loss 16.2173 (now 7.447, fut 8.770) | \n",
            "Epoch 032 |  train loss 12.6355 (now 5.672, fut 6.963) | val loss 16.2660 (now 7.507, fut 8.759) | \n",
            "Epoch 033 |  train loss 12.5198 (now 5.593, fut 6.926) | val loss 16.2489 (now 7.446, fut 8.803) | \n",
            "Epoch 034 | ** best ** train loss 12.3734 (now 5.530, fut 6.843) | val loss 16.1358 (now 7.458, fut 8.678) | \n",
            "Epoch 035 |  train loss 12.3008 (now 5.499, fut 6.802) | val loss 16.4343 (now 7.599, fut 8.835) | \n",
            "Epoch 036 |  train loss 12.2574 (now 5.462, fut 6.795) | val loss 16.2965 (now 7.374, fut 8.923) | \n",
            "Epoch 037 |  train loss 12.2026 (now 5.454, fut 6.748) | val loss 16.5460 (now 7.398, fut 9.148) | \n",
            "Epoch 038 |  train loss 12.0354 (now 5.359, fut 6.676) | val loss 16.1717 (now 7.302, fut 8.869) | \n",
            "Epoch 039 |  train loss 12.0531 (now 5.379, fut 6.674) | val loss 16.0345 (now 7.351, fut 8.684) | \n",
            "Epoch 040 | ** best ** train loss 11.9296 (now 5.310, fut 6.619) | val loss 15.6343 (now 6.988, fut 8.646) | \n",
            "Epoch 041 |  train loss 11.8452 (now 5.288, fut 6.557) | val loss 15.6652 (now 7.001, fut 8.664) | \n",
            "Epoch 042 |  train loss 11.7532 (now 5.230, fut 6.523) | val loss 15.7723 (now 7.118, fut 8.655) | \n",
            "Epoch 043 | ** best ** train loss 11.7218 (now 5.219, fut 6.503) | val loss 15.5022 (now 7.017, fut 8.485) | \n",
            "Epoch 044 |  train loss 11.7279 (now 5.221, fut 6.507) | val loss 15.3958 (now 6.779, fut 8.617) | \n",
            "Epoch 045 |  train loss 11.6382 (now 5.180, fut 6.458) | val loss 15.8106 (now 7.123, fut 8.687) | \n",
            "Epoch 046 |  train loss 11.6055 (now 5.149, fut 6.457) | val loss 15.5987 (now 6.870, fut 8.729) | \n",
            "Epoch 047 |  train loss 11.5445 (now 5.131, fut 6.413) | val loss 15.6133 (now 6.896, fut 8.718) | \n",
            "Epoch 048 |  train loss 11.4329 (now 5.078, fut 6.355) | val loss 15.7940 (now 7.018, fut 8.776) | \n",
            "Epoch 049 |  train loss 11.3831 (now 5.035, fut 6.348) | val loss 15.5292 (now 6.829, fut 8.700) | \n",
            "Epoch 050 |  train loss 11.4295 (now 5.067, fut 6.362) | val loss 16.1747 (now 7.258, fut 8.917) | \n",
            "Epoch 051 |  train loss 11.4143 (now 5.069, fut 6.345) | val loss 15.5747 (now 6.753, fut 8.821) | \n",
            "Epoch 052 |  train loss 11.3050 (now 5.001, fut 6.304) | val loss 15.5967 (now 6.797, fut 8.800) | \n",
            "Epoch 053 |  train loss 11.2994 (now 5.019, fut 6.280) | val loss 15.5792 (now 6.734, fut 8.845) | \n",
            "Epoch 054 |  train loss 11.2719 (now 4.990, fut 6.282) | val loss 15.6068 (now 6.780, fut 8.827) | \n",
            "Epoch 055 |  train loss 11.2151 (now 4.965, fut 6.250) | val loss 15.6560 (now 6.801, fut 8.855) | \n",
            "Epoch 056 |  train loss 11.1126 (now 4.929, fut 6.184) | val loss 16.0098 (now 6.872, fut 9.138) | \n",
            "Epoch 057 |  train loss 11.2263 (now 4.983, fut 6.244) | val loss 15.7174 (now 6.929, fut 8.789) | \n",
            "Epoch 058 |  train loss 11.0355 (now 4.892, fut 6.144) | val loss 15.2702 (now 6.456, fut 8.815) | \n",
            "Epoch 059 |  train loss 10.9928 (now 4.895, fut 6.097) | val loss 15.6498 (now 6.671, fut 8.979) | \n",
            "Epoch 060 |  train loss 10.9989 (now 4.895, fut 6.104) | val loss 15.6159 (now 6.638, fut 8.978) | \n",
            "Epoch 061 |  train loss 10.9748 (now 4.885, fut 6.089) | val loss 15.6899 (now 6.807, fut 8.883) | \n",
            "Epoch 062 |  train loss 10.8610 (now 4.825, fut 6.036) | val loss 16.2169 (now 6.978, fut 9.238) | \n",
            "Epoch 063 |  train loss 10.8844 (now 4.838, fut 6.047) | val loss 16.1526 (now 7.112, fut 9.041) | \n",
            "Epoch 064 |  train loss 10.8231 (now 4.857, fut 5.966) | val loss 15.5690 (now 6.689, fut 8.880) | \n",
            "Epoch 065 |  train loss 10.7167 (now 4.765, fut 5.952) | val loss 16.0343 (now 6.798, fut 9.236) | \n",
            "Epoch 066 |  train loss 10.6763 (now 4.758, fut 5.918) | val loss 16.0723 (now 7.101, fut 8.971) | \n",
            "Epoch 067 |  train loss 10.6763 (now 4.738, fut 5.939) | val loss 15.4909 (now 6.682, fut 8.809) | \n",
            "Epoch 068 |  train loss 10.6605 (now 4.751, fut 5.910) | val loss 15.5622 (now 6.655, fut 8.907) | \n",
            "Epoch 069 |  train loss 10.6227 (now 4.732, fut 5.891) | val loss 15.6628 (now 6.738, fut 8.925) | \n",
            "Epoch 070 |  train loss 10.6123 (now 4.721, fut 5.892) | val loss 15.5833 (now 6.667, fut 8.917) | \n",
            "Epoch 071 |  train loss 10.6102 (now 4.728, fut 5.882) | val loss 16.1854 (now 6.852, fut 9.334) | \n",
            "Epoch 072 |  train loss 10.5671 (now 4.707, fut 5.860) | val loss 15.9122 (now 6.836, fut 9.076) | \n",
            "Epoch 073 |  train loss 10.5103 (now 4.666, fut 5.845) | val loss 15.4587 (now 6.526, fut 8.933) | \n",
            "Epoch 074 |  train loss 10.5136 (now 4.679, fut 5.835) | val loss 15.6518 (now 6.767, fut 8.885) | \n",
            "Epoch 075 |  train loss 10.4471 (now 4.669, fut 5.778) | val loss 15.3931 (now 6.489, fut 8.904) | \n",
            "Epoch 076 |  train loss 10.4370 (now 4.664, fut 5.773) | val loss 15.3975 (now 6.460, fut 8.938) | \n",
            "Epoch 077 |  train loss 10.3944 (now 4.633, fut 5.762) | val loss 15.8062 (now 6.711, fut 9.096) | \n",
            "Epoch 078 |  train loss 10.2869 (now 4.577, fut 5.710) | val loss 15.9327 (now 6.907, fut 9.026) | \n",
            "Epoch 079 |  train loss 10.3478 (now 4.628, fut 5.720) | val loss 15.6808 (now 6.766, fut 8.914) | \n",
            "Epoch 080 |  train loss 10.2614 (now 4.556, fut 5.705) | val loss 15.6706 (now 6.527, fut 9.144) | \n",
            "Epoch 081 |  train loss 10.2837 (now 4.593, fut 5.690) | val loss 15.7607 (now 6.633, fut 9.127) | \n",
            "Epoch 082 |  train loss 10.1912 (now 4.543, fut 5.648) | val loss 15.8556 (now 6.927, fut 8.929) | \n",
            "Epoch 083 |  train loss 10.2481 (now 4.570, fut 5.678) | val loss 15.7853 (now 6.810, fut 8.976) | \n",
            "Epoch 084 |  train loss 10.1782 (now 4.527, fut 5.652) | val loss 16.1044 (now 6.998, fut 9.106) | \n",
            "Epoch 085 |  train loss 10.1507 (now 4.551, fut 5.599) | val loss 15.8700 (now 6.743, fut 9.127) | \n",
            "Epoch 086 |  train loss 10.1116 (now 4.515, fut 5.596) | val loss 15.6538 (now 6.534, fut 9.119) | \n",
            "Epoch 087 |  train loss 10.1290 (now 4.526, fut 5.603) | val loss 16.3136 (now 6.859, fut 9.455) | \n",
            "Epoch 088 |  train loss 10.0915 (now 4.507, fut 5.584) | val loss 16.1071 (now 6.735, fut 9.372) | \n",
            "Epoch 089 |  train loss 10.0603 (now 4.496, fut 5.564) | val loss 15.8661 (now 6.629, fut 9.237) | \n",
            "Epoch 090 |  train loss 10.0882 (now 4.499, fut 5.590) | val loss 15.8234 (now 6.680, fut 9.144) | \n",
            "Epoch 091 |  train loss 10.0631 (now 4.501, fut 5.562) | val loss 16.1223 (now 6.874, fut 9.249) | \n",
            "Epoch 092 |  train loss 9.9875 (now 4.478, fut 5.510) | val loss 15.8975 (now 6.584, fut 9.313) | \n",
            "Epoch 093 |  train loss 10.0284 (now 4.488, fut 5.541) | val loss 15.5610 (now 6.471, fut 9.090) | \n",
            "Epoch 094 |  train loss 9.9295 (now 4.442, fut 5.488) | val loss 15.5112 (now 6.341, fut 9.170) | \n",
            "Epoch 095 |  train loss 9.9255 (now 4.443, fut 5.482) | val loss 15.7576 (now 6.439, fut 9.319) | \n",
            "Epoch 096 |  train loss 9.9024 (now 4.425, fut 5.477) | val loss 15.5781 (now 6.395, fut 9.183) | \n",
            "Epoch 097 |  train loss 9.8467 (now 4.403, fut 5.444) | val loss 15.7468 (now 6.603, fut 9.143) | \n",
            "Epoch 098 |  train loss 9.8754 (now 4.421, fut 5.455) | val loss 15.8971 (now 6.731, fut 9.166) | \n",
            "Epoch 099 |  train loss 9.9242 (now 4.457, fut 5.467) | val loss 15.8061 (now 6.530, fut 9.276) | \n",
            "Epoch 100 |  train loss 9.7867 (now 4.369, fut 5.418) | val loss 15.8875 (now 6.709, fut 9.178) | \n",
            "Best validation forecast MAE (S9 fold): 8.4853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 9\n",
        "\n",
        "Best val_MAE for S1-8, S10-15: 15.5022 (100 EPOCHS; reached at epoch 43)\n",
        "\n",
        "- now val_MAE: 7.017\n",
        "- fut val_MAE: 8.4853\n",
        "\n",
        "notes:\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "AevrzgMD5y1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 10]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "cY3KNXGA9g2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=10, # test: subject 10\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S10\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S10 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLRXZio-9iNz",
        "outputId": "14dd1614-b13a-496c-c930-f7c22d4dd545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S10] Train 46612 | Val 11732 | Test 5291 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.7581 (now 74.676, fut 74.082) | val loss 99.6580 (now 51.681, fut 47.977) | \n",
            "Epoch 002 | ** best ** train loss 38.6867 (now 19.490, fut 19.197) | val loss 33.6226 (now 16.854, fut 16.768) | \n",
            "Epoch 003 |  train loss 33.7758 (now 15.118, fut 18.658) | val loss 38.2374 (now 15.880, fut 22.357) | \n",
            "Epoch 004 | ** best ** train loss 33.5676 (now 15.568, fut 18.000) | val loss 31.2262 (now 14.842, fut 16.385) | \n",
            "Epoch 005 | ** best ** train loss 30.4904 (now 14.225, fut 16.265) | val loss 30.7558 (now 14.895, fut 15.860) | \n",
            "Epoch 006 | ** best ** train loss 29.6940 (now 13.791, fut 15.903) | val loss 27.5895 (now 12.790, fut 14.799) | \n",
            "Epoch 007 | ** best ** train loss 27.4307 (now 12.697, fut 14.733) | val loss 25.8243 (now 12.122, fut 13.702) | \n",
            "Epoch 008 | ** best ** train loss 27.2823 (now 12.492, fut 14.791) | val loss 24.9932 (now 11.490, fut 13.504) | \n",
            "Epoch 009 | ** best ** train loss 27.2090 (now 12.403, fut 14.806) | val loss 23.6349 (now 10.788, fut 12.847) | \n",
            "Epoch 010 | ** best ** train loss 27.1811 (now 12.342, fut 14.839) | val loss 23.1480 (now 10.491, fut 12.657) | \n",
            "Epoch 011 |  train loss 27.2087 (now 12.272, fut 14.937) | val loss 23.3812 (now 10.575, fut 12.806) | \n",
            "Epoch 012 |  train loss 26.9037 (now 12.062, fut 14.842) | val loss 23.6052 (now 10.559, fut 13.046) | \n",
            "Epoch 013 |  train loss 26.9253 (now 12.209, fut 14.717) | val loss 23.6134 (now 10.759, fut 12.855) | \n",
            "Epoch 014 |  train loss 28.7449 (now 12.732, fut 16.013) | val loss 23.9115 (now 10.710, fut 13.202) | \n",
            "Epoch 015 |  train loss 29.1416 (now 12.913, fut 16.229) | val loss 22.7647 (now 10.046, fut 12.718) | \n",
            "Epoch 016 |  train loss 28.6346 (now 12.619, fut 16.016) | val loss 24.0606 (now 10.843, fut 13.217) | \n",
            "Epoch 017 |  train loss 28.2281 (now 12.501, fut 15.727) | val loss 24.7432 (now 11.084, fut 13.659) | \n",
            "Epoch 018 |  train loss 28.0429 (now 12.425, fut 15.618) | val loss 23.8965 (now 10.487, fut 13.409) | \n",
            "Epoch 019 |  train loss 28.9136 (now 12.824, fut 16.090) | val loss 23.4698 (now 10.313, fut 13.157) | \n",
            "Epoch 020 |  train loss 28.1463 (now 12.454, fut 15.692) | val loss 22.7952 (now 10.045, fut 12.750) | \n",
            "Epoch 021 |  train loss 27.6247 (now 12.345, fut 15.279) | val loss 23.4044 (now 10.208, fut 13.197) | \n",
            "Epoch 022 |  train loss 27.4610 (now 12.166, fut 15.295) | val loss 22.9185 (now 10.155, fut 12.763) | \n",
            "Epoch 023 |  train loss 27.2391 (now 12.026, fut 15.213) | val loss 23.8094 (now 10.746, fut 13.063) | \n",
            "Epoch 024 | ** best ** train loss 27.1424 (now 11.961, fut 15.181) | val loss 22.3230 (now 10.032, fut 12.291) | \n",
            "Epoch 025 |  train loss 26.9204 (now 11.974, fut 14.946) | val loss 23.0472 (now 10.124, fut 12.924) | \n",
            "Epoch 026 |  train loss 26.5397 (now 11.790, fut 14.749) | val loss 22.0888 (now 9.791, fut 12.298) | \n",
            "Epoch 027 |  train loss 26.5844 (now 11.703, fut 14.882) | val loss 22.8028 (now 10.156, fut 12.646) | \n",
            "Epoch 028 |  train loss 26.2356 (now 11.588, fut 14.648) | val loss 23.2366 (now 10.382, fut 12.855) | \n",
            "Epoch 029 |  train loss 26.0500 (now 11.569, fut 14.481) | val loss 22.8784 (now 10.533, fut 12.345) | \n",
            "Epoch 030 |  train loss 26.0598 (now 11.457, fut 14.603) | val loss 23.4431 (now 10.283, fut 13.161) | \n",
            "Epoch 031 |  train loss 25.7135 (now 11.301, fut 14.413) | val loss 23.4032 (now 10.408, fut 12.996) | \n",
            "Epoch 032 |  train loss 25.3705 (now 11.348, fut 14.023) | val loss 23.4794 (now 10.671, fut 12.808) | \n",
            "Epoch 033 | ** best ** train loss 25.3189 (now 11.217, fut 14.102) | val loss 22.7110 (now 10.430, fut 12.281) | \n",
            "Epoch 034 | ** best ** train loss 24.8580 (now 11.128, fut 13.730) | val loss 21.9297 (now 9.728, fut 12.202) | \n",
            "Epoch 035 | ** best ** train loss 24.8478 (now 11.066, fut 13.782) | val loss 22.0107 (now 9.847, fut 12.164) | \n",
            "Epoch 036 |  train loss 24.5007 (now 10.948, fut 13.553) | val loss 21.7597 (now 9.550, fut 12.210) | \n",
            "Epoch 037 |  train loss 24.6490 (now 10.945, fut 13.704) | val loss 22.6333 (now 10.218, fut 12.416) | \n",
            "Epoch 038 | ** best ** train loss 23.6977 (now 10.659, fut 13.038) | val loss 21.7192 (now 9.786, fut 11.933) | \n",
            "Epoch 039 | ** best ** train loss 22.5734 (now 10.072, fut 12.501) | val loss 21.5440 (now 10.026, fut 11.518) | \n",
            "Epoch 040 |  train loss 22.7618 (now 10.090, fut 12.672) | val loss 22.6500 (now 10.337, fut 12.313) | \n",
            "Epoch 041 |  train loss 22.6351 (now 10.136, fut 12.499) | val loss 22.7489 (now 10.395, fut 12.354) | \n",
            "Epoch 042 |  train loss 23.2186 (now 10.432, fut 12.786) | val loss 22.6711 (now 10.309, fut 12.362) | \n",
            "Epoch 043 |  train loss 23.9198 (now 10.630, fut 13.290) | val loss 22.6037 (now 10.074, fut 12.529) | \n",
            "Epoch 044 |  train loss 23.9901 (now 10.710, fut 13.280) | val loss 22.9286 (now 10.226, fut 12.703) | \n",
            "Epoch 045 |  train loss 23.9063 (now 10.641, fut 13.265) | val loss 23.8607 (now 10.859, fut 13.001) | \n",
            "Epoch 046 |  train loss 23.8122 (now 10.613, fut 13.199) | val loss 21.9694 (now 9.931, fut 12.038) | \n",
            "Epoch 047 |  train loss 23.6043 (now 10.639, fut 12.965) | val loss 23.5856 (now 10.722, fut 12.864) | \n",
            "Epoch 048 |  train loss 23.8483 (now 10.697, fut 13.151) | val loss 21.4962 (now 9.591, fut 11.905) | \n",
            "Epoch 049 |  train loss 23.8365 (now 10.581, fut 13.256) | val loss 22.1655 (now 9.926, fut 12.239) | \n",
            "Epoch 050 |  train loss 23.5699 (now 10.544, fut 13.026) | val loss 22.6810 (now 10.415, fut 12.266) | \n",
            "Epoch 051 |  train loss 23.8404 (now 10.589, fut 13.252) | val loss 22.2370 (now 9.876, fut 12.361) | \n",
            "Epoch 052 |  train loss 23.6097 (now 10.551, fut 13.059) | val loss 22.2195 (now 10.186, fut 12.034) | \n",
            "Epoch 053 |  train loss 23.2538 (now 10.456, fut 12.797) | val loss 21.5775 (now 9.633, fut 11.945) | \n",
            "Epoch 054 |  train loss 23.1033 (now 10.424, fut 12.679) | val loss 23.0058 (now 10.448, fut 12.558) | \n",
            "Epoch 055 |  train loss 23.3632 (now 10.435, fut 12.928) | val loss 23.2096 (now 10.611, fut 12.599) | \n",
            "Epoch 056 |  train loss 23.2534 (now 10.295, fut 12.958) | val loss 22.4854 (now 10.183, fut 12.303) | \n",
            "Epoch 057 |  train loss 22.9370 (now 10.235, fut 12.702) | val loss 21.7454 (now 9.880, fut 11.865) | \n",
            "Epoch 058 |  train loss 22.9987 (now 10.277, fut 12.722) | val loss 22.2403 (now 9.939, fut 12.301) | \n",
            "Epoch 059 |  train loss 22.9062 (now 10.289, fut 12.617) | val loss 21.9747 (now 9.733, fut 12.242) | \n",
            "Epoch 060 |  train loss 22.8480 (now 10.220, fut 12.628) | val loss 21.5462 (now 9.667, fut 11.880) | \n",
            "Epoch 061 |  train loss 22.7432 (now 10.164, fut 12.580) | val loss 22.5312 (now 9.877, fut 12.654) | \n",
            "Epoch 062 |  train loss 22.7814 (now 10.228, fut 12.553) | val loss 22.7297 (now 10.411, fut 12.319) | \n",
            "Epoch 063 |  train loss 22.5721 (now 10.136, fut 12.437) | val loss 22.3167 (now 10.148, fut 12.169) | \n",
            "Epoch 064 |  train loss 22.5575 (now 10.048, fut 12.510) | val loss 22.2400 (now 9.900, fut 12.340) | \n",
            "Epoch 065 |  train loss 22.4966 (now 10.124, fut 12.373) | val loss 22.4880 (now 10.029, fut 12.459) | \n",
            "Epoch 066 |  train loss 22.2616 (now 10.015, fut 12.247) | val loss 22.5307 (now 10.307, fut 12.224) | \n",
            "Epoch 067 |  train loss 22.1950 (now 10.000, fut 12.195) | val loss 21.7632 (now 9.903, fut 11.860) | \n",
            "Epoch 068 |  train loss 22.0713 (now 9.880, fut 12.191) | val loss 21.9055 (now 9.742, fut 12.164) | \n",
            "Epoch 069 |  train loss 22.0195 (now 9.988, fut 12.031) | val loss 22.7755 (now 10.300, fut 12.475) | \n",
            "Epoch 070 |  train loss 22.1577 (now 9.871, fut 12.287) | val loss 22.2815 (now 9.825, fut 12.457) | \n",
            "Epoch 071 |  train loss 22.0165 (now 9.857, fut 12.160) | val loss 21.9005 (now 10.027, fut 11.874) | \n",
            "Epoch 072 |  train loss 21.7654 (now 9.748, fut 12.017) | val loss 22.7046 (now 10.418, fut 12.286) | \n",
            "Epoch 073 |  train loss 21.8682 (now 9.759, fut 12.109) | val loss 21.8416 (now 9.825, fut 12.016) | \n",
            "Epoch 074 |  train loss 21.6766 (now 9.789, fut 11.888) | val loss 21.4902 (now 9.652, fut 11.838) | \n",
            "Epoch 075 |  train loss 21.6140 (now 9.742, fut 11.872) | val loss 21.8402 (now 9.752, fut 12.089) | \n",
            "Epoch 076 |  train loss 21.4651 (now 9.784, fut 11.681) | val loss 21.6592 (now 9.619, fut 12.040) | \n",
            "Epoch 077 |  train loss 21.3173 (now 9.701, fut 11.616) | val loss 21.5687 (now 10.002, fut 11.567) | \n",
            "Epoch 078 |  train loss 21.3317 (now 9.718, fut 11.614) | val loss 21.8480 (now 9.868, fut 11.980) | \n",
            "Epoch 079 |  train loss 21.5844 (now 9.656, fut 11.928) | val loss 22.4774 (now 10.129, fut 12.348) | \n",
            "Epoch 080 |  train loss 21.5680 (now 9.683, fut 11.885) | val loss 22.4802 (now 10.073, fut 12.407) | \n",
            "Epoch 081 |  train loss 21.2705 (now 9.615, fut 11.655) | val loss 21.4759 (now 9.494, fut 11.982) | \n",
            "Epoch 082 | ** best ** train loss 21.0378 (now 9.566, fut 11.472) | val loss 20.6867 (now 9.333, fut 11.353) | \n",
            "Epoch 083 |  train loss 20.9734 (now 9.491, fut 11.482) | val loss 21.8225 (now 9.799, fut 12.023) | \n",
            "Epoch 084 |  train loss 20.9606 (now 9.436, fut 11.525) | val loss 23.3709 (now 10.848, fut 12.522) | \n",
            "Epoch 085 |  train loss 21.0598 (now 9.439, fut 11.621) | val loss 21.5056 (now 9.738, fut 11.767) | \n",
            "Epoch 086 |  train loss 20.9942 (now 9.382, fut 11.612) | val loss 21.8960 (now 9.725, fut 12.171) | \n",
            "Epoch 087 |  train loss 20.7909 (now 9.408, fut 11.383) | val loss 21.6712 (now 9.778, fut 11.893) | \n",
            "Epoch 088 |  train loss 20.7955 (now 9.351, fut 11.444) | val loss 21.7050 (now 9.833, fut 11.872) | \n",
            "Epoch 089 |  train loss 20.7215 (now 9.330, fut 11.391) | val loss 21.9779 (now 9.937, fut 12.041) | \n",
            "Epoch 090 |  train loss 20.6963 (now 9.263, fut 11.433) | val loss 22.9272 (now 10.509, fut 12.419) | \n",
            "Epoch 091 |  train loss 20.6603 (now 9.269, fut 11.391) | val loss 22.1202 (now 9.933, fut 12.187) | \n",
            "Epoch 092 |  train loss 20.4903 (now 9.204, fut 11.287) | val loss 22.0538 (now 10.135, fut 11.919) | \n",
            "Epoch 093 |  train loss 20.4718 (now 9.203, fut 11.269) | val loss 21.4302 (now 9.528, fut 11.902) | \n",
            "Epoch 094 |  train loss 20.3624 (now 9.176, fut 11.186) | val loss 21.8099 (now 9.812, fut 11.997) | \n",
            "Epoch 095 |  train loss 20.2400 (now 9.125, fut 11.115) | val loss 21.4817 (now 9.388, fut 12.093) | \n",
            "Epoch 096 |  train loss 20.1961 (now 9.097, fut 11.099) | val loss 20.9187 (now 9.361, fut 11.558) | \n",
            "Epoch 097 |  train loss 20.0570 (now 9.106, fut 10.951) | val loss 22.3706 (now 9.861, fut 12.509) | \n",
            "Epoch 098 |  train loss 20.1491 (now 9.047, fut 11.102) | val loss 21.7832 (now 9.873, fut 11.910) | \n",
            "Epoch 099 |  train loss 19.8335 (now 9.047, fut 10.786) | val loss 22.3786 (now 10.051, fut 12.328) | \n",
            "Epoch 100 |  train loss 20.0803 (now 8.969, fut 11.111) | val loss 21.9317 (now 9.969, fut 11.963) | \n",
            "Best validation forecast MAE (S10 fold): 11.3532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 10 - worst performer\n",
        "\n",
        "Best val_MAE for S1-9, S11-15: 20.6867 (100 EPOCHS; reached at epoch 39)\n",
        "\n",
        "- now val_MAE: 9.333\n",
        "- fut val_MAE: 11.3532\n",
        "\n",
        "notes:\n",
        "- worst performer; only val_MAE to break 20 mark\n",
        "- particularly difficult dataset w/o subject 10"
      ],
      "metadata": {
        "id": "BSqtvLG89t2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 11]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "tyMUADbgAqH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=11, # test: subject 11\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S11\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S11 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgeILQnyAlho",
        "outputId": "692d4194-cb5d-4736-fb35-ef9978090ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S11] Train 47257 | Val 11891 | Test 4491 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 145.9332 (now 72.439, fut 73.494) | val loss 94.4938 (now 46.982, fut 47.512) | \n",
            "Epoch 002 | ** best ** train loss 38.1745 (now 18.865, fut 19.310) | val loss 32.2486 (now 16.340, fut 15.909) | \n",
            "Epoch 003 | ** best ** train loss 30.3349 (now 15.646, fut 14.689) | val loss 30.5356 (now 16.390, fut 14.146) | \n",
            "Epoch 004 | ** best ** train loss 26.0386 (now 13.065, fut 12.974) | val loss 25.7547 (now 13.133, fut 12.621) | \n",
            "Epoch 005 | ** best ** train loss 24.5580 (now 12.368, fut 12.190) | val loss 25.2481 (now 12.935, fut 12.313) | \n",
            "Epoch 006 | ** best ** train loss 21.3857 (now 10.571, fut 10.815) | val loss 22.6260 (now 11.381, fut 11.245) | \n",
            "Epoch 007 | ** best ** train loss 19.1336 (now 9.314, fut 9.820) | val loss 21.2969 (now 10.696, fut 10.601) | \n",
            "Epoch 008 | ** best ** train loss 18.8576 (now 9.193, fut 9.665) | val loss 21.0023 (now 10.507, fut 10.495) | \n",
            "Epoch 009 | ** best ** train loss 18.2206 (now 8.849, fut 9.371) | val loss 20.8369 (now 10.525, fut 10.312) | \n",
            "Epoch 010 | ** best ** train loss 17.4631 (now 8.471, fut 8.992) | val loss 20.1708 (now 10.077, fut 10.094) | \n",
            "Epoch 011 |  train loss 17.0341 (now 8.264, fut 8.770) | val loss 20.4797 (now 10.349, fut 10.130) | \n",
            "Epoch 012 |  train loss 16.5379 (now 7.986, fut 8.552) | val loss 21.2479 (now 10.756, fut 10.492) | \n",
            "Epoch 013 |  train loss 16.9227 (now 8.221, fut 8.702) | val loss 22.0155 (now 11.263, fut 10.752) | \n",
            "Epoch 014 | ** best ** train loss 17.3228 (now 8.469, fut 8.854) | val loss 19.7986 (now 9.967, fut 9.831) | \n",
            "Epoch 015 | ** best ** train loss 16.1497 (now 7.773, fut 8.377) | val loss 19.3784 (now 9.728, fut 9.651) | \n",
            "Epoch 016 |  train loss 15.8051 (now 7.607, fut 8.198) | val loss 19.7226 (now 9.812, fut 9.911) | \n",
            "Epoch 017 |  train loss 15.2628 (now 7.327, fut 7.935) | val loss 19.8814 (now 9.958, fut 9.924) | \n",
            "Epoch 018 |  train loss 15.0212 (now 7.195, fut 7.826) | val loss 20.0363 (now 9.964, fut 10.073) | \n",
            "Epoch 019 |  train loss 15.7480 (now 7.583, fut 8.165) | val loss 23.2594 (now 11.641, fut 11.618) | \n",
            "Epoch 020 |  train loss 16.5003 (now 7.970, fut 8.530) | val loss 20.5398 (now 10.298, fut 10.242) | \n",
            "Epoch 021 |  train loss 15.2699 (now 7.347, fut 7.923) | val loss 20.1886 (now 10.119, fut 10.070) | \n",
            "Epoch 022 |  train loss 15.5142 (now 7.457, fut 8.057) | val loss 19.5165 (now 9.822, fut 9.695) | \n",
            "Epoch 023 |  train loss 15.6716 (now 7.564, fut 8.108) | val loss 20.0064 (now 10.142, fut 9.864) | \n",
            "Epoch 024 |  train loss 15.4193 (now 7.417, fut 8.002) | val loss 19.4087 (now 9.688, fut 9.721) | \n",
            "Epoch 025 | ** best ** train loss 15.1288 (now 7.276, fut 7.852) | val loss 19.1793 (now 9.633, fut 9.546) | \n",
            "Epoch 026 |  train loss 15.0086 (now 7.217, fut 7.792) | val loss 19.6948 (now 9.821, fut 9.874) | \n",
            "Epoch 027 |  train loss 14.8611 (now 7.145, fut 7.716) | val loss 19.5289 (now 9.886, fut 9.643) | \n",
            "Epoch 028 |  train loss 14.9718 (now 7.215, fut 7.757) | val loss 19.4160 (now 9.825, fut 9.591) | \n",
            "Epoch 029 |  train loss 14.6970 (now 7.060, fut 7.637) | val loss 19.5094 (now 9.755, fut 9.754) | \n",
            "Epoch 030 |  train loss 14.8860 (now 7.173, fut 7.713) | val loss 19.4653 (now 9.793, fut 9.672) | \n",
            "Epoch 031 |  train loss 14.3626 (now 6.894, fut 7.468) | val loss 19.5132 (now 9.844, fut 9.670) | \n",
            "Epoch 032 | ** best ** train loss 14.1860 (now 6.791, fut 7.395) | val loss 19.0175 (now 9.503, fut 9.514) | \n",
            "Epoch 033 |  train loss 14.2022 (now 6.800, fut 7.402) | val loss 19.2077 (now 9.679, fut 9.529) | \n",
            "Epoch 034 | ** best ** train loss 13.9636 (now 6.696, fut 7.268) | val loss 19.1443 (now 9.701, fut 9.444) | \n",
            "Epoch 035 |  train loss 14.4517 (now 6.886, fut 7.566) | val loss 19.6615 (now 9.712, fut 9.950) | \n",
            "Epoch 036 |  train loss 14.3526 (now 6.867, fut 7.485) | val loss 19.8415 (now 9.880, fut 9.961) | \n",
            "Epoch 037 |  train loss 14.5871 (now 7.036, fut 7.551) | val loss 20.3471 (now 10.310, fut 10.037) | \n",
            "Epoch 038 |  train loss 14.1730 (now 6.814, fut 7.359) | val loss 19.7747 (now 9.847, fut 9.928) | \n",
            "Epoch 039 |  train loss 14.0076 (now 6.722, fut 7.286) | val loss 19.7689 (now 9.965, fut 9.803) | \n",
            "Epoch 040 |  train loss 14.2883 (now 6.877, fut 7.412) | val loss 19.7772 (now 10.152, fut 9.625) | \n",
            "Epoch 041 |  train loss 14.1062 (now 6.765, fut 7.341) | val loss 19.1488 (now 9.567, fut 9.581) | \n",
            "Epoch 042 |  train loss 13.9980 (now 6.726, fut 7.272) | val loss 19.5322 (now 9.714, fut 9.818) | \n",
            "Epoch 043 |  train loss 13.7023 (now 6.579, fut 7.123) | val loss 19.5274 (now 9.860, fut 9.668) | \n",
            "Epoch 044 |  train loss 13.8253 (now 6.628, fut 7.198) | val loss 19.6047 (now 9.760, fut 9.844) | \n",
            "Epoch 045 |  train loss 13.6594 (now 6.514, fut 7.146) | val loss 19.1127 (now 9.410, fut 9.703) | \n",
            "Epoch 046 |  train loss 13.5405 (now 6.458, fut 7.083) | val loss 20.0121 (now 10.073, fut 9.939) | \n",
            "Epoch 047 |  train loss 13.7304 (now 6.573, fut 7.157) | val loss 20.0280 (now 10.104, fut 9.924) | \n",
            "Epoch 048 |  train loss 13.5373 (now 6.484, fut 7.053) | val loss 19.9294 (now 9.971, fut 9.959) | \n",
            "Epoch 049 |  train loss 13.4275 (now 6.406, fut 7.021) | val loss 20.3657 (now 10.032, fut 10.334) | \n",
            "Epoch 050 |  train loss 13.3952 (now 6.386, fut 7.009) | val loss 19.4047 (now 9.665, fut 9.740) | \n",
            "Epoch 051 |  train loss 13.3820 (now 6.396, fut 6.986) | val loss 20.3320 (now 10.138, fut 10.194) | \n",
            "Epoch 052 |  train loss 13.3144 (now 6.376, fut 6.938) | val loss 19.1677 (now 9.497, fut 9.671) | \n",
            "Epoch 053 |  train loss 13.4010 (now 6.397, fut 7.005) | val loss 19.3747 (now 9.690, fut 9.685) | \n",
            "Epoch 054 |  train loss 13.2163 (now 6.285, fut 6.931) | val loss 20.1249 (now 10.075, fut 10.050) | \n",
            "Epoch 055 |  train loss 13.1044 (now 6.272, fut 6.833) | val loss 19.1922 (now 9.509, fut 9.683) | \n",
            "Epoch 056 |  train loss 13.0057 (now 6.186, fut 6.820) | val loss 18.9401 (now 9.313, fut 9.627) | \n",
            "Epoch 057 |  train loss 12.9766 (now 6.190, fut 6.787) | val loss 19.7993 (now 10.069, fut 9.731) | \n",
            "Epoch 058 |  train loss 13.7874 (now 6.596, fut 7.192) | val loss 19.6211 (now 9.750, fut 9.871) | \n",
            "Epoch 059 |  train loss 13.3425 (now 6.388, fut 6.954) | val loss 19.9981 (now 10.052, fut 9.946) | \n",
            "Epoch 060 |  train loss 13.0893 (now 6.265, fut 6.824) | val loss 19.0276 (now 9.461, fut 9.566) | \n",
            "Epoch 061 |  train loss 12.8455 (now 6.099, fut 6.746) | val loss 19.6296 (now 9.896, fut 9.734) | \n",
            "Epoch 062 |  train loss 12.7140 (now 6.052, fut 6.662) | val loss 18.9667 (now 9.344, fut 9.622) | \n",
            "Epoch 063 |  train loss 12.6308 (now 6.018, fut 6.612) | val loss 19.4218 (now 9.594, fut 9.827) | \n",
            "Epoch 064 |  train loss 12.5394 (now 5.980, fut 6.560) | val loss 19.5560 (now 9.768, fut 9.788) | \n",
            "Epoch 065 |  train loss 12.5822 (now 5.987, fut 6.595) | val loss 18.9330 (now 9.416, fut 9.517) | \n",
            "Epoch 066 |  train loss 12.5513 (now 5.972, fut 6.580) | val loss 18.9550 (now 9.451, fut 9.504) | \n",
            "Epoch 067 |  train loss 12.4950 (now 5.964, fut 6.531) | val loss 19.3285 (now 9.626, fut 9.703) | \n",
            "Epoch 068 | ** best ** train loss 12.3524 (now 5.886, fut 6.466) | val loss 18.7392 (now 9.381, fut 9.358) | \n",
            "Epoch 069 |  train loss 12.4977 (now 5.950, fut 6.548) | val loss 18.9229 (now 9.327, fut 9.596) | \n",
            "Epoch 070 |  train loss 12.4404 (now 5.931, fut 6.509) | val loss 19.1613 (now 9.586, fut 9.575) | \n",
            "Epoch 071 |  train loss 12.3843 (now 5.914, fut 6.470) | val loss 19.0226 (now 9.480, fut 9.543) | \n",
            "Epoch 072 |  train loss 12.3022 (now 5.867, fut 6.435) | val loss 18.8889 (now 9.374, fut 9.515) | \n",
            "Epoch 073 |  train loss 12.2300 (now 5.830, fut 6.400) | val loss 19.8493 (now 9.918, fut 9.932) | \n",
            "Epoch 074 |  train loss 12.1867 (now 5.802, fut 6.385) | val loss 19.2862 (now 9.655, fut 9.631) | \n",
            "Epoch 075 |  train loss 12.2749 (now 5.868, fut 6.407) | val loss 19.4982 (now 9.646, fut 9.853) | \n",
            "Epoch 076 |  train loss 12.2799 (now 5.843, fut 6.436) | val loss 19.0367 (now 9.411, fut 9.625) | \n",
            "Epoch 077 |  train loss 12.2665 (now 5.846, fut 6.420) | val loss 18.9510 (now 9.420, fut 9.531) | \n",
            "Epoch 078 |  train loss 12.2756 (now 5.850, fut 6.425) | val loss 19.1826 (now 9.442, fut 9.740) | \n",
            "Epoch 079 |  train loss 12.3055 (now 5.853, fut 6.453) | val loss 19.4170 (now 9.583, fut 9.834) | \n",
            "Epoch 080 |  train loss 12.1742 (now 5.799, fut 6.375) | val loss 19.4018 (now 9.718, fut 9.683) | \n",
            "Epoch 081 |  train loss 12.2500 (now 5.836, fut 6.414) | val loss 18.4853 (now 9.018, fut 9.467) | \n",
            "Epoch 082 |  train loss 12.1220 (now 5.747, fut 6.375) | val loss 19.1818 (now 9.571, fut 9.611) | \n",
            "Epoch 083 |  train loss 11.9875 (now 5.722, fut 6.266) | val loss 18.7524 (now 9.391, fut 9.361) | \n",
            "Epoch 084 |  train loss 11.9280 (now 5.683, fut 6.245) | val loss 18.8291 (now 9.423, fut 9.406) | \n",
            "Epoch 085 |  train loss 11.8790 (now 5.652, fut 6.227) | val loss 19.5280 (now 9.703, fut 9.825) | \n",
            "Epoch 086 | ** best ** train loss 11.7999 (now 5.623, fut 6.177) | val loss 18.5637 (now 9.239, fut 9.325) | \n",
            "Epoch 087 |  train loss 11.7552 (now 5.604, fut 6.151) | val loss 18.9817 (now 9.357, fut 9.624) | \n",
            "Epoch 088 |  train loss 11.7277 (now 5.587, fut 6.141) | val loss 19.2157 (now 9.539, fut 9.677) | \n",
            "Epoch 089 |  train loss 11.6815 (now 5.550, fut 6.131) | val loss 19.0874 (now 9.409, fut 9.678) | \n",
            "Epoch 090 |  train loss 11.6458 (now 5.558, fut 6.088) | val loss 19.2727 (now 9.665, fut 9.608) | \n",
            "Epoch 091 |  train loss 11.5969 (now 5.519, fut 6.078) | val loss 19.4876 (now 9.799, fut 9.689) | \n",
            "Epoch 092 |  train loss 11.6070 (now 5.518, fut 6.089) | val loss 19.3021 (now 9.618, fut 9.684) | \n",
            "Epoch 093 | ** best ** train loss 11.5106 (now 5.467, fut 6.043) | val loss 18.4446 (now 9.150, fut 9.295) | \n",
            "Epoch 094 |  train loss 11.4637 (now 5.448, fut 6.016) | val loss 19.3084 (now 9.639, fut 9.670) | \n",
            "Epoch 095 |  train loss 11.4533 (now 5.451, fut 6.002) | val loss 19.0954 (now 9.469, fut 9.627) | \n",
            "Epoch 096 |  train loss 11.4045 (now 5.443, fut 5.961) | val loss 18.5846 (now 9.222, fut 9.362) | \n",
            "Epoch 097 |  train loss 11.3260 (now 5.404, fut 5.922) | val loss 19.1173 (now 9.456, fut 9.661) | \n",
            "Epoch 098 |  train loss 11.3480 (now 5.394, fut 5.954) | val loss 18.7824 (now 9.303, fut 9.479) | \n",
            "Epoch 099 |  train loss 11.3001 (now 5.383, fut 5.917) | val loss 19.5146 (now 9.777, fut 9.737) | \n",
            "Epoch 100 |  train loss 11.2378 (now 5.357, fut 5.881) | val loss 19.0771 (now 9.524, fut 9.553) | \n",
            "Best validation forecast MAE (S11 fold): 9.2949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 11\n",
        "\n",
        "Best val_MAE for S1-10, S12-15: 18.4446 (100 EPOCHS; reached at epoch 93)\n",
        "\n",
        "- now val_MAE: 9.150\n",
        "- fut val_MAE: 9.2949\n",
        "\n",
        "notes:\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "odAaj1YoA6Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 12] - REDACTED DUE TO HUMAN ERROR\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "-kBxdM4zB8DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=12, # test: subject 12\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S12\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S12 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFyKnGT0B3zS",
        "outputId": "bab5e910-d30a-43a4-d25b-426790d798aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S12] Train 47706 | Val 12005 | Test 3924 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 151.1867 (now 76.038, fut 75.149) | val loss 101.4652 (now 53.094, fut 48.371) | \n",
            "Epoch 002 | ** best ** train loss 41.1511 (now 20.842, fut 20.309) | val loss 63.3922 (now 30.649, fut 32.744) | \n",
            "Epoch 003 | ** best ** train loss 34.2925 (now 15.701, fut 18.591) | val loss 30.3859 (now 15.298, fut 15.087) | \n",
            "Epoch 004 | ** best ** train loss 27.9781 (now 13.372, fut 14.607) | val loss 28.5744 (now 14.250, fut 14.324) | \n",
            "Epoch 005 | ** best ** train loss 24.5729 (now 11.704, fut 12.869) | val loss 25.1586 (now 12.639, fut 12.520) | \n",
            "Epoch 006 | ** best ** train loss 20.7012 (now 10.118, fut 10.584) | val loss 23.5192 (now 11.784, fut 11.735) | \n",
            "Epoch 007 | ** best ** train loss 19.7370 (now 9.624, fut 10.113) | val loss 22.0691 (now 11.026, fut 11.043) | \n",
            "Epoch 008 | ** best ** train loss 19.3201 (now 9.382, fut 9.938) | val loss 21.8305 (now 10.955, fut 10.876) | \n",
            "Epoch 009 | ** best ** train loss 18.6459 (now 9.032, fut 9.613) | val loss 20.7463 (now 10.336, fut 10.410) | \n",
            "Epoch 010 | ** best ** train loss 17.6978 (now 8.625, fut 9.073) | val loss 20.2955 (now 10.123, fut 10.172) | \n",
            "Epoch 011 | ** best ** train loss 17.2329 (now 8.361, fut 8.871) | val loss 19.9438 (now 9.852, fut 10.092) | \n",
            "Epoch 012 |  train loss 17.8721 (now 8.630, fut 9.242) | val loss 20.7431 (now 10.134, fut 10.609) | \n",
            "Epoch 013 |  train loss 17.6525 (now 8.486, fut 9.166) | val loss 20.2722 (now 10.013, fut 10.259) | \n",
            "Epoch 014 | ** best ** train loss 16.6338 (now 8.014, fut 8.620) | val loss 19.6070 (now 9.689, fut 9.918) | \n",
            "Epoch 015 |  train loss 15.9886 (now 7.784, fut 8.204) | val loss 19.6038 (now 9.682, fut 9.922) | \n",
            "Epoch 016 | ** best ** train loss 15.6540 (now 7.596, fut 8.058) | val loss 19.8389 (now 9.953, fut 9.886) | \n",
            "Epoch 017 | ** best ** train loss 15.3870 (now 7.452, fut 7.935) | val loss 19.6963 (now 9.829, fut 9.867) | \n",
            "Epoch 018 | ** best ** train loss 15.5441 (now 7.499, fut 8.046) | val loss 19.4636 (now 9.659, fut 9.805) | \n",
            "Epoch 019 | ** best ** train loss 15.2695 (now 7.366, fut 7.903) | val loss 19.1661 (now 9.504, fut 9.662) | \n",
            "Epoch 020 | ** best ** train loss 14.9035 (now 7.220, fut 7.683) | val loss 19.0275 (now 9.548, fut 9.479) | \n",
            "Epoch 021 |  train loss 14.7017 (now 7.090, fut 7.612) | val loss 19.0249 (now 9.477, fut 9.548) | \n",
            "Epoch 022 |  train loss 14.6079 (now 7.083, fut 7.525) | val loss 19.6474 (now 9.734, fut 9.914) | \n",
            "Epoch 023 | ** best ** train loss 14.4132 (now 6.950, fut 7.463) | val loss 18.6401 (now 9.333, fut 9.307) | \n",
            "Epoch 024 |  train loss 14.4012 (now 6.960, fut 7.441) | val loss 18.6531 (now 9.225, fut 9.428) | \n",
            "Epoch 025 |  train loss 14.1364 (now 6.801, fut 7.335) | val loss 18.9833 (now 9.337, fut 9.647) | \n",
            "Epoch 026 | ** best ** train loss 14.0476 (now 6.751, fut 7.297) | val loss 18.4024 (now 9.100, fut 9.303) | \n",
            "Epoch 027 |  train loss 13.9736 (now 6.721, fut 7.253) | val loss 19.4440 (now 9.819, fut 9.625) | \n",
            "Epoch 028 |  train loss 13.9323 (now 6.716, fut 7.216) | val loss 18.5502 (now 9.213, fut 9.338) | \n",
            "Epoch 029 |  train loss 13.7833 (now 6.637, fut 7.147) | val loss 18.8929 (now 9.383, fut 9.510) | \n",
            "Epoch 030 |  train loss 13.6482 (now 6.584, fut 7.065) | val loss 18.7580 (now 9.363, fut 9.395) | \n",
            "Epoch 031 |  train loss 13.4865 (now 6.485, fut 7.001) | val loss 18.5757 (now 9.212, fut 9.363) | \n",
            "Epoch 032 | ** best ** train loss 13.7048 (now 6.590, fut 7.115) | val loss 18.0676 (now 8.844, fut 9.223) | \n",
            "Epoch 033 |  train loss 13.4314 (now 6.454, fut 6.978) | val loss 18.4644 (now 9.060, fut 9.405) | \n",
            "Epoch 034 |  train loss 13.2992 (now 6.398, fut 6.902) | val loss 18.5258 (now 9.142, fut 9.384) | \n",
            "Epoch 035 | ** best ** train loss 13.3004 (now 6.387, fut 6.913) | val loss 18.0826 (now 8.955, fut 9.128) | \n",
            "Epoch 036 |  train loss 13.2981 (now 6.401, fut 6.897) | val loss 18.1755 (now 8.989, fut 9.186) | \n",
            "Epoch 037 |  train loss 13.2626 (now 6.373, fut 6.890) | val loss 18.7279 (now 9.225, fut 9.503) | \n",
            "Epoch 038 |  train loss 13.0773 (now 6.264, fut 6.813) | val loss 18.9913 (now 9.367, fut 9.625) | \n",
            "Epoch 039 |  train loss 13.0868 (now 6.286, fut 6.801) | val loss 18.5111 (now 9.126, fut 9.385) | \n",
            "Epoch 040 |  train loss 13.0050 (now 6.244, fut 6.761) | val loss 18.9776 (now 9.490, fut 9.488) | \n",
            "Epoch 041 |  train loss 12.9655 (now 6.231, fut 6.735) | val loss 18.3070 (now 9.080, fut 9.227) | \n",
            "Epoch 042 |  train loss 12.9246 (now 6.213, fut 6.712) | val loss 18.1897 (now 8.969, fut 9.220) | \n",
            "Epoch 043 |  train loss 12.8484 (now 6.164, fut 6.684) | val loss 18.8433 (now 9.304, fut 9.539) | \n",
            "Epoch 044 |  train loss 12.7478 (now 6.128, fut 6.620) | val loss 18.5913 (now 9.155, fut 9.437) | \n",
            "Epoch 045 |  train loss 12.6984 (now 6.103, fut 6.595) | val loss 18.6833 (now 9.053, fut 9.631) | \n",
            "Epoch 046 |  train loss 12.6401 (now 6.069, fut 6.572) | val loss 18.0538 (now 8.909, fut 9.144) | \n",
            "Epoch 047 |  train loss 12.6309 (now 6.064, fut 6.567) | val loss 18.3713 (now 9.025, fut 9.347) | \n",
            "Epoch 048 | ** best ** train loss 12.6030 (now 6.060, fut 6.543) | val loss 17.8257 (now 8.769, fut 9.056) | \n",
            "Epoch 049 |  train loss 12.5865 (now 6.045, fut 6.542) | val loss 18.4504 (now 9.158, fut 9.292) | \n",
            "Epoch 050 |  train loss 12.5980 (now 6.066, fut 6.532) | val loss 18.5258 (now 9.196, fut 9.330) | \n",
            "Epoch 051 |  train loss 12.4926 (now 6.004, fut 6.489) | val loss 18.7307 (now 9.260, fut 9.470) | \n",
            "Epoch 052 |  train loss 12.4343 (now 5.985, fut 6.450) | val loss 18.2329 (now 9.077, fut 9.156) | \n",
            "Epoch 053 |  train loss 12.3591 (now 5.922, fut 6.437) | val loss 18.3378 (now 9.100, fut 9.237) | \n",
            "Epoch 054 |  train loss 12.4836 (now 6.003, fut 6.481) | val loss 18.8245 (now 9.329, fut 9.495) | \n",
            "Epoch 055 |  train loss 12.3680 (now 5.935, fut 6.433) | val loss 18.5820 (now 9.191, fut 9.391) | \n",
            "Epoch 056 |  train loss 12.2595 (now 5.866, fut 6.393) | val loss 19.2465 (now 9.680, fut 9.567) | \n",
            "Epoch 057 |  train loss 12.2860 (now 5.896, fut 6.390) | val loss 18.7075 (now 9.356, fut 9.351) | \n",
            "Epoch 058 |  train loss 12.2656 (now 5.880, fut 6.386) | val loss 18.2986 (now 9.006, fut 9.292) | \n",
            "Epoch 059 |  train loss 12.2122 (now 5.866, fut 6.346) | val loss 19.0724 (now 9.331, fut 9.742) | \n",
            "Epoch 060 |  train loss 12.1406 (now 5.823, fut 6.318) | val loss 18.9044 (now 9.374, fut 9.530) | \n",
            "Epoch 061 |  train loss 12.1345 (now 5.812, fut 6.323) | val loss 18.9149 (now 9.419, fut 9.496) | \n",
            "Epoch 062 |  train loss 12.1714 (now 5.879, fut 6.293) | val loss 18.5532 (now 9.118, fut 9.436) | \n",
            "Epoch 063 |  train loss 12.0438 (now 5.788, fut 6.256) | val loss 18.9398 (now 9.262, fut 9.678) | \n",
            "Epoch 064 |  train loss 12.0877 (now 5.813, fut 6.275) | val loss 18.9963 (now 9.307, fut 9.689) | \n",
            "Epoch 065 |  train loss 12.0438 (now 5.798, fut 6.246) | val loss 18.6840 (now 9.208, fut 9.476) | \n",
            "Epoch 066 |  train loss 11.9975 (now 5.748, fut 6.249) | val loss 18.7670 (now 9.203, fut 9.563) | \n",
            "Epoch 067 |  train loss 11.9517 (now 5.757, fut 6.195) | val loss 19.6943 (now 9.774, fut 9.921) | \n",
            "Epoch 068 |  train loss 11.9644 (now 5.764, fut 6.200) | val loss 18.9988 (now 9.229, fut 9.770) | \n",
            "Epoch 069 |  train loss 12.0076 (now 5.802, fut 6.205) | val loss 18.9317 (now 9.209, fut 9.723) | \n",
            "Epoch 070 |  train loss 12.0473 (now 5.802, fut 6.245) | val loss 19.2668 (now 9.593, fut 9.674) | \n",
            "Epoch 071 |  train loss 11.8937 (now 5.729, fut 6.164) | val loss 19.2215 (now 9.503, fut 9.719) | \n",
            "Epoch 072 |  train loss 12.0331 (now 5.800, fut 6.233) | val loss 18.5430 (now 9.011, fut 9.532) | \n",
            "Epoch 073 |  train loss 11.8851 (now 5.721, fut 6.165) | val loss 18.8915 (now 9.277, fut 9.615) | \n",
            "Epoch 074 |  train loss 11.9041 (now 5.720, fut 6.184) | val loss 18.8773 (now 9.390, fut 9.488) | \n",
            "Epoch 075 |  train loss 11.8331 (now 5.692, fut 6.141) | val loss 19.2416 (now 9.599, fut 9.643) | \n",
            "Epoch 076 |  train loss 11.8231 (now 5.694, fut 6.129) | val loss 19.2723 (now 9.446, fut 9.827) | \n",
            "Epoch 077 |  train loss 11.8728 (now 5.704, fut 6.169) | val loss 19.0717 (now 9.422, fut 9.649) | \n",
            "Epoch 078 |  train loss 12.2341 (now 5.882, fut 6.353) | val loss 19.7269 (now 9.725, fut 10.002) | \n",
            "Epoch 079 |  train loss 12.2342 (now 5.886, fut 6.348) | val loss 19.2643 (now 9.408, fut 9.857) | \n",
            "Epoch 080 |  train loss 12.2833 (now 5.909, fut 6.375) | val loss 19.2762 (now 9.456, fut 9.820) | \n",
            "Epoch 081 |  train loss 12.1669 (now 5.818, fut 6.349) | val loss 19.1204 (now 9.362, fut 9.758) | \n",
            "Epoch 082 |  train loss 12.0894 (now 5.793, fut 6.297) | val loss 19.0775 (now 9.476, fut 9.602) | \n",
            "Epoch 083 |  train loss 12.0630 (now 5.788, fut 6.275) | val loss 18.9275 (now 9.260, fut 9.667) | \n",
            "Epoch 084 |  train loss 11.9586 (now 5.756, fut 6.202) | val loss 19.0755 (now 9.378, fut 9.698) | \n",
            "Epoch 085 |  train loss 11.9830 (now 5.752, fut 6.231) | val loss 19.4823 (now 9.583, fut 9.900) | \n",
            "Epoch 086 |  train loss 11.9765 (now 5.766, fut 6.211) | val loss 18.7214 (now 9.200, fut 9.522) | \n",
            "Epoch 087 |  train loss 11.9405 (now 5.745, fut 6.195) | val loss 19.5364 (now 9.506, fut 10.030) | \n",
            "Epoch 088 |  train loss 11.8643 (now 5.706, fut 6.158) | val loss 19.0569 (now 9.231, fut 9.826) | \n",
            "Epoch 089 |  train loss 11.8302 (now 5.717, fut 6.113) | val loss 18.5871 (now 9.097, fut 9.490) | \n",
            "Epoch 090 |  train loss 11.7725 (now 5.660, fut 6.112) | val loss 18.4923 (now 8.914, fut 9.578) | \n",
            "Epoch 091 |  train loss 11.7017 (now 5.630, fut 6.072) | val loss 19.2922 (now 9.443, fut 9.850) | \n",
            "Epoch 092 |  train loss 11.6826 (now 5.631, fut 6.051) | val loss 19.7126 (now 9.894, fut 9.819) | \n",
            "Epoch 093 |  train loss 11.6328 (now 5.598, fut 6.035) | val loss 18.7454 (now 9.151, fut 9.595) | \n",
            "Epoch 094 |  train loss 11.6522 (now 5.601, fut 6.051) | val loss 19.4475 (now 9.503, fut 9.944) | \n",
            "Epoch 095 |  train loss 11.6610 (now 5.612, fut 6.049) | val loss 19.7047 (now 9.629, fut 10.076) | \n",
            "Epoch 096 |  train loss 11.5356 (now 5.544, fut 5.991) | val loss 19.1183 (now 9.347, fut 9.771) | \n",
            "Epoch 097 |  train loss 11.5298 (now 5.547, fut 5.983) | val loss 19.2959 (now 9.483, fut 9.813) | \n",
            "Epoch 098 |  train loss 11.7144 (now 5.636, fut 6.078) | val loss 19.3378 (now 9.405, fut 9.933) | \n",
            "Epoch 099 |  train loss 11.7295 (now 5.634, fut 6.095) | val loss 19.0425 (now 9.232, fut 9.810) | \n",
            "Epoch 100 |  train loss 11.7437 (now 5.643, fut 6.101) | val loss 20.1730 (now 9.806, fut 10.367) | \n",
            "Best validation forecast MAE (S12 fold): 9.0563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 12\n",
        "\n",
        "Best val_MAE for S1-11, S13-15: 17.8257 (100 EPOCHS; reached at epoch 48)\n",
        "\n",
        "- now val_MAE: 8.769\n",
        "- fut val_MAE: 9.0563\n",
        "\n",
        "notes:\n",
        "- stagnation around high 11/low 12 mark for train_loss"
      ],
      "metadata": {
        "id": "krw8yYXeCHUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 12] - human error forgot to update so this is now the training loop for 12\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "UgncuLfcDvua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=12, # test: subject 12\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S12\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S12 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_CKy85pDr3U",
        "outputId": "4de29c4c-06c2-4332-b86e-c8e1f4d43a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S12] Train 47706 | Val 12005 | Test 3924 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 149.9472 (now 74.693, fut 75.255) | val loss 97.9553 (now 49.211, fut 48.744) | \n",
            "Epoch 002 | ** best ** train loss 40.4445 (now 20.038, fut 20.407) | val loss 54.1719 (now 27.177, fut 26.995) | \n",
            "Epoch 003 | ** best ** train loss 33.9909 (now 16.883, fut 17.108) | val loss 29.0196 (now 14.531, fut 14.489) | \n",
            "Epoch 004 | ** best ** train loss 28.7043 (now 14.069, fut 14.635) | val loss 27.4501 (now 13.832, fut 13.618) | \n",
            "Epoch 005 | ** best ** train loss 28.3835 (now 13.965, fut 14.418) | val loss 25.7551 (now 12.852, fut 12.903) | \n",
            "Epoch 006 | ** best ** train loss 28.1610 (now 13.875, fut 14.285) | val loss 24.6664 (now 12.430, fut 12.237) | \n",
            "Epoch 007 |  train loss 25.9201 (now 12.775, fut 13.146) | val loss 24.9954 (now 12.657, fut 12.339) | \n",
            "Epoch 008 | ** best ** train loss 25.3565 (now 12.475, fut 12.882) | val loss 23.2026 (now 11.604, fut 11.598) | \n",
            "Epoch 009 |  train loss 24.2619 (now 11.943, fut 12.319) | val loss 23.6332 (now 11.864, fut 11.770) | \n",
            "Epoch 010 |  train loss 23.9176 (now 11.769, fut 12.149) | val loss 23.8037 (now 12.063, fut 11.740) | \n",
            "Epoch 011 | ** best ** train loss 24.5025 (now 12.075, fut 12.428) | val loss 21.9028 (now 11.010, fut 10.893) | \n",
            "Epoch 012 |  train loss 22.3864 (now 11.040, fut 11.347) | val loss 21.7479 (now 10.835, fut 10.913) | \n",
            "Epoch 013 | ** best ** train loss 21.2966 (now 10.414, fut 10.883) | val loss 21.5203 (now 10.726, fut 10.794) | \n",
            "Epoch 014 | ** best ** train loss 21.4952 (now 10.544, fut 10.951) | val loss 20.7767 (now 10.309, fut 10.468) | \n",
            "Epoch 015 |  train loss 21.3622 (now 10.469, fut 10.893) | val loss 21.1495 (now 10.547, fut 10.602) | \n",
            "Epoch 016 | ** best ** train loss 20.0764 (now 9.825, fut 10.251) | val loss 20.3135 (now 10.033, fut 10.280) | \n",
            "Epoch 017 |  train loss 18.9214 (now 9.225, fut 9.696) | val loss 20.5617 (now 10.240, fut 10.322) | \n",
            "Epoch 018 |  train loss 19.0718 (now 9.357, fut 9.715) | val loss 20.6280 (now 10.313, fut 10.315) | \n",
            "Epoch 019 | ** best ** train loss 18.3168 (now 8.912, fut 9.405) | val loss 20.2182 (now 10.059, fut 10.160) | \n",
            "Epoch 020 | ** best ** train loss 18.8418 (now 9.191, fut 9.650) | val loss 19.7957 (now 9.872, fut 9.924) | \n",
            "Epoch 021 |  train loss 18.1984 (now 8.865, fut 9.334) | val loss 20.1335 (now 9.928, fut 10.206) | \n",
            "Epoch 022 |  train loss 20.2846 (now 9.916, fut 10.368) | val loss 23.8979 (now 11.944, fut 11.954) | \n",
            "Epoch 023 |  train loss 24.3434 (now 11.954, fut 12.389) | val loss 22.9641 (now 11.494, fut 11.470) | \n",
            "Epoch 024 |  train loss 22.6804 (now 11.132, fut 11.549) | val loss 22.1756 (now 11.292, fut 10.884) | \n",
            "Epoch 025 |  train loss 20.5665 (now 10.056, fut 10.510) | val loss 21.1440 (now 10.653, fut 10.491) | \n",
            "Epoch 026 |  train loss 19.4224 (now 9.503, fut 9.919) | val loss 21.8634 (now 10.867, fut 10.997) | \n",
            "Epoch 027 |  train loss 20.4136 (now 10.002, fut 10.412) | val loss 21.3160 (now 10.704, fut 10.612) | \n",
            "Epoch 028 |  train loss 19.1435 (now 9.355, fut 9.789) | val loss 21.1790 (now 10.546, fut 10.633) | \n",
            "Epoch 029 |  train loss 18.3563 (now 8.954, fut 9.402) | val loss 21.2066 (now 10.629, fut 10.578) | \n",
            "Epoch 030 |  train loss 17.7643 (now 8.633, fut 9.131) | val loss 21.0910 (now 10.543, fut 10.548) | \n",
            "Epoch 031 |  train loss 17.0623 (now 8.273, fut 8.789) | val loss 20.0670 (now 9.891, fut 10.176) | \n",
            "Epoch 032 |  train loss 16.5170 (now 8.029, fut 8.488) | val loss 20.1054 (now 10.006, fut 10.099) | \n",
            "Epoch 033 |  train loss 16.6508 (now 8.092, fut 8.559) | val loss 20.4171 (now 10.235, fut 10.182) | \n",
            "Epoch 034 |  train loss 16.4219 (now 7.976, fut 8.446) | val loss 20.0226 (now 9.958, fut 10.065) | \n",
            "Epoch 035 |  train loss 16.2606 (now 7.888, fut 8.373) | val loss 20.3987 (now 10.135, fut 10.263) | \n",
            "Epoch 036 |  train loss 16.0457 (now 7.783, fut 8.262) | val loss 20.1351 (now 10.013, fut 10.122) | \n",
            "Epoch 037 |  train loss 17.4255 (now 8.452, fut 8.973) | val loss 20.4759 (now 10.141, fut 10.335) | \n",
            "Epoch 038 | ** best ** train loss 15.6659 (now 7.594, fut 8.072) | val loss 19.5182 (now 9.642, fut 9.876) | \n",
            "Epoch 039 |  train loss 15.1865 (now 7.351, fut 7.835) | val loss 19.7721 (now 9.807, fut 9.965) | \n",
            "Epoch 040 | ** best ** train loss 15.0550 (now 7.270, fut 7.785) | val loss 19.4203 (now 9.574, fut 9.846) | \n",
            "Epoch 041 |  train loss 14.7576 (now 7.109, fut 7.648) | val loss 19.8609 (now 9.844, fut 10.017) | \n",
            "Epoch 042 |  train loss 14.6896 (now 7.091, fut 7.598) | val loss 19.8502 (now 9.768, fut 10.082) | \n",
            "Epoch 043 |  train loss 14.5407 (now 7.006, fut 7.535) | val loss 19.7684 (now 9.795, fut 9.974) | \n",
            "Epoch 044 |  train loss 14.3311 (now 6.891, fut 7.441) | val loss 19.7711 (now 9.756, fut 10.016) | \n",
            "Epoch 045 | ** best ** train loss 14.1443 (now 6.813, fut 7.331) | val loss 19.0225 (now 9.331, fut 9.691) | \n",
            "Epoch 046 |  train loss 14.0467 (now 6.769, fut 7.278) | val loss 19.3690 (now 9.630, fut 9.739) | \n",
            "Epoch 047 |  train loss 13.8712 (now 6.663, fut 7.208) | val loss 19.2047 (now 9.472, fut 9.733) | \n",
            "Epoch 048 |  train loss 13.6771 (now 6.566, fut 7.111) | val loss 19.5232 (now 9.576, fut 9.948) | \n",
            "Epoch 049 |  train loss 13.5627 (now 6.511, fut 7.052) | val loss 19.0862 (now 9.387, fut 9.699) | \n",
            "Epoch 050 |  train loss 13.4740 (now 6.492, fut 6.982) | val loss 19.5864 (now 9.656, fut 9.931) | \n",
            "Epoch 051 |  train loss 13.4243 (now 6.432, fut 6.993) | val loss 19.3716 (now 9.589, fut 9.783) | \n",
            "Epoch 052 |  train loss 13.2600 (now 6.379, fut 6.881) | val loss 19.5923 (now 9.682, fut 9.910) | \n",
            "Epoch 053 |  train loss 13.1773 (now 6.311, fut 6.867) | val loss 19.0271 (now 9.304, fut 9.723) | \n",
            "Epoch 054 |  train loss 13.0796 (now 6.246, fut 6.834) | val loss 19.7478 (now 9.862, fut 9.886) | \n",
            "Epoch 055 |  train loss 12.9870 (now 6.226, fut 6.761) | val loss 19.1521 (now 9.380, fut 9.772) | \n",
            "Epoch 056 | ** best ** train loss 12.8603 (now 6.161, fut 6.699) | val loss 18.9066 (now 9.323, fut 9.584) | \n",
            "Epoch 057 |  train loss 12.8578 (now 6.175, fut 6.683) | val loss 19.3239 (now 9.537, fut 9.786) | \n",
            "Epoch 058 | ** best ** train loss 12.7208 (now 6.105, fut 6.616) | val loss 18.9759 (now 9.521, fut 9.454) | \n",
            "Epoch 059 |  train loss 12.6836 (now 6.073, fut 6.610) | val loss 18.9019 (now 9.293, fut 9.609) | \n",
            "Epoch 060 |  train loss 12.7259 (now 6.094, fut 6.632) | val loss 19.2818 (now 9.398, fut 9.884) | \n",
            "Epoch 061 |  train loss 12.5535 (now 6.021, fut 6.532) | val loss 19.2494 (now 9.345, fut 9.904) | \n",
            "Epoch 062 |  train loss 12.5089 (now 6.018, fut 6.491) | val loss 19.4749 (now 9.629, fut 9.846) | \n",
            "Epoch 063 |  train loss 12.5042 (now 6.009, fut 6.495) | val loss 18.7351 (now 9.194, fut 9.541) | \n",
            "Epoch 064 |  train loss 12.4372 (now 5.952, fut 6.485) | val loss 19.3725 (now 9.559, fut 9.813) | \n",
            "Epoch 065 |  train loss 12.3875 (now 5.954, fut 6.433) | val loss 19.5162 (now 9.817, fut 9.700) | \n",
            "Epoch 066 |  train loss 12.3875 (now 5.952, fut 6.435) | val loss 18.7194 (now 9.185, fut 9.535) | \n",
            "Epoch 067 |  train loss 12.2829 (now 5.892, fut 6.391) | val loss 19.3446 (now 9.618, fut 9.727) | \n",
            "Epoch 068 |  train loss 12.2111 (now 5.867, fut 6.344) | val loss 19.0738 (now 9.477, fut 9.596) | \n",
            "Epoch 069 |  train loss 12.2237 (now 5.855, fut 6.368) | val loss 19.0005 (now 9.407, fut 9.594) | \n",
            "Epoch 070 |  train loss 12.1483 (now 5.823, fut 6.325) | val loss 18.8371 (now 9.280, fut 9.557) | \n",
            "Epoch 071 |  train loss 12.1057 (now 5.784, fut 6.322) | val loss 19.4922 (now 9.527, fut 9.966) | \n",
            "Epoch 072 |  train loss 12.0465 (now 5.762, fut 6.284) | val loss 18.8277 (now 9.214, fut 9.613) | \n",
            "Epoch 073 |  train loss 11.9950 (now 5.731, fut 6.264) | val loss 19.0178 (now 9.257, fut 9.761) | \n",
            "Epoch 074 | ** best ** train loss 11.9468 (now 5.719, fut 6.228) | val loss 18.6178 (now 9.229, fut 9.389) | \n",
            "Epoch 075 |  train loss 11.9265 (now 5.732, fut 6.194) | val loss 19.2589 (now 9.607, fut 9.652) | \n",
            "Epoch 076 |  train loss 11.9158 (now 5.709, fut 6.207) | val loss 18.7953 (now 9.321, fut 9.474) | \n",
            "Epoch 077 |  train loss 11.8331 (now 5.675, fut 6.158) | val loss 18.8894 (now 9.340, fut 9.549) | \n",
            "Epoch 078 |  train loss 11.8065 (now 5.647, fut 6.160) | val loss 19.5039 (now 9.661, fut 9.843) | \n",
            "Epoch 079 |  train loss 11.8072 (now 5.653, fut 6.154) | val loss 19.2590 (now 9.589, fut 9.670) | \n",
            "Epoch 080 |  train loss 11.7072 (now 5.631, fut 6.076) | val loss 18.6669 (now 9.171, fut 9.496) | \n",
            "Epoch 081 |  train loss 11.6943 (now 5.611, fut 6.083) | val loss 18.8031 (now 9.173, fut 9.630) | \n",
            "Epoch 082 |  train loss 11.6818 (now 5.591, fut 6.091) | val loss 19.4014 (now 9.490, fut 9.912) | \n",
            "Epoch 083 |  train loss 11.6040 (now 5.548, fut 6.056) | val loss 19.2280 (now 9.414, fut 9.815) | \n",
            "Epoch 084 |  train loss 11.6563 (now 5.581, fut 6.075) | val loss 19.2589 (now 9.384, fut 9.875) | \n",
            "Epoch 085 |  train loss 11.5655 (now 5.556, fut 6.010) | val loss 19.2032 (now 9.365, fut 9.838) | \n",
            "Epoch 086 |  train loss 11.5308 (now 5.522, fut 6.009) | val loss 18.6685 (now 9.063, fut 9.605) | \n",
            "Epoch 087 |  train loss 11.5221 (now 5.523, fut 5.999) | val loss 19.4066 (now 9.474, fut 9.933) | \n",
            "Epoch 088 |  train loss 11.5359 (now 5.509, fut 6.027) | val loss 18.8212 (now 9.154, fut 9.667) | \n",
            "Epoch 089 |  train loss 11.4488 (now 5.487, fut 5.962) | val loss 19.2272 (now 9.457, fut 9.770) | \n",
            "Epoch 090 |  train loss 11.4611 (now 5.495, fut 5.966) | val loss 19.5925 (now 9.737, fut 9.856) | \n",
            "Epoch 091 |  train loss 11.4321 (now 5.459, fut 5.973) | val loss 19.2472 (now 9.478, fut 9.770) | \n",
            "Epoch 092 |  train loss 11.3651 (now 5.439, fut 5.926) | val loss 19.6157 (now 9.593, fut 10.023) | \n",
            "Epoch 093 |  train loss 11.3722 (now 5.443, fut 5.930) | val loss 19.2317 (now 9.436, fut 9.796) | \n",
            "Epoch 094 |  train loss 11.3054 (now 5.419, fut 5.886) | val loss 19.2458 (now 9.418, fut 9.827) | \n",
            "Epoch 095 |  train loss 11.2672 (now 5.409, fut 5.858) | val loss 19.5968 (now 9.720, fut 9.876) | \n",
            "Epoch 096 |  train loss 11.2022 (now 5.383, fut 5.819) | val loss 19.4961 (now 9.568, fut 9.928) | \n",
            "Epoch 097 |  train loss 11.2314 (now 5.374, fut 5.857) | val loss 19.4833 (now 9.637, fut 9.847) | \n",
            "Epoch 098 |  train loss 11.2816 (now 5.432, fut 5.850) | val loss 19.7720 (now 9.672, fut 10.100) | \n",
            "Epoch 099 |  train loss 11.2194 (now 5.375, fut 5.844) | val loss 19.2841 (now 9.486, fut 9.798) | \n",
            "Epoch 100 |  train loss 11.1704 (now 5.364, fut 5.806) | val loss 19.6701 (now 9.503, fut 10.167) | \n",
            "Best validation forecast MAE (S12 fold): 9.3889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 12\n",
        "\n",
        "Best val_MAE for S1-11, S13-15: 18.6178 (100 EPOCHS; reached at epoch 74)\n",
        "\n",
        "- now val_MAE: 9.229\n",
        "- fut val_MAE: 9.3889\n",
        "\n",
        "notes:\n",
        "- overfitting beyond circa epoch 55"
      ],
      "metadata": {
        "id": "bjqTuWuOD0QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 13]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "_LhsgVBm2E3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=13, # test: subject 13\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S13\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S13 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPRXRv1m2SBH",
        "outputId": "bc0a6712-1923-4931-8de9-419cafb5e3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S13] Train 47217 | Val 11883 | Test 4535 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.5371 (now 73.675, fut 74.862) | val loss 98.1190 (now 48.127, fut 49.992) | \n",
            "Epoch 002 | ** best ** train loss 39.1535 (now 19.142, fut 20.011) | val loss 37.5101 (now 18.878, fut 18.633) | \n",
            "Epoch 003 |  train loss 34.6419 (now 18.068, fut 16.574) | val loss 44.9472 (now 22.835, fut 22.112) | \n",
            "Epoch 004 | ** best ** train loss 33.5700 (now 16.891, fut 16.679) | val loss 30.6985 (now 15.773, fut 14.925) | \n",
            "Epoch 005 | ** best ** train loss 30.0206 (now 15.262, fut 14.759) | val loss 28.8703 (now 14.959, fut 13.911) | \n",
            "Epoch 006 | ** best ** train loss 26.9987 (now 13.666, fut 13.333) | val loss 24.0821 (now 12.395, fut 11.687) | \n",
            "Epoch 007 | ** best ** train loss 22.9295 (now 11.355, fut 11.575) | val loss 22.1600 (now 11.320, fut 10.840) | \n",
            "Epoch 008 | ** best ** train loss 22.7381 (now 11.355, fut 11.383) | val loss 21.7409 (now 11.079, fut 10.662) | \n",
            "Epoch 009 | ** best ** train loss 21.4833 (now 10.617, fut 10.866) | val loss 22.1513 (now 11.532, fut 10.619) | \n",
            "Epoch 010 | ** best ** train loss 20.8502 (now 10.299, fut 10.552) | val loss 21.1753 (now 10.640, fut 10.535) | \n",
            "Epoch 011 | ** best ** train loss 19.1162 (now 9.328, fut 9.788) | val loss 19.5381 (now 9.646, fut 9.892) | \n",
            "Epoch 012 | ** best ** train loss 18.1784 (now 8.724, fut 9.454) | val loss 18.7528 (now 9.235, fut 9.518) | \n",
            "Epoch 013 |  train loss 18.3239 (now 8.834, fut 9.490) | val loss 19.1774 (now 9.501, fut 9.676) | \n",
            "Epoch 014 | ** best ** train loss 18.1531 (now 8.723, fut 9.430) | val loss 18.0657 (now 8.793, fut 9.273) | \n",
            "Epoch 015 |  train loss 17.3967 (now 8.266, fut 9.131) | val loss 18.0484 (now 8.766, fut 9.282) | \n",
            "Epoch 016 | ** best ** train loss 17.0350 (now 8.060, fut 8.975) | val loss 17.0473 (now 8.243, fut 8.805) | \n",
            "Epoch 017 |  train loss 16.4773 (now 7.751, fut 8.726) | val loss 17.7927 (now 8.596, fut 9.196) | \n",
            "Epoch 018 | ** best ** train loss 15.8404 (now 7.356, fut 8.484) | val loss 16.6046 (now 7.853, fut 8.752) | \n",
            "Epoch 019 |  train loss 15.1040 (now 6.921, fut 8.183) | val loss 16.8780 (now 8.060, fut 8.818) | \n",
            "Epoch 020 | ** best ** train loss 14.8438 (now 6.838, fut 8.006) | val loss 16.5688 (now 7.841, fut 8.728) | \n",
            "Epoch 021 |  train loss 14.5072 (now 6.629, fut 7.878) | val loss 17.2983 (now 8.265, fut 9.034) | \n",
            "Epoch 022 | ** best ** train loss 14.1784 (now 6.448, fut 7.731) | val loss 16.2139 (now 7.604, fut 8.610) | \n",
            "Epoch 023 |  train loss 14.1312 (now 6.431, fut 7.700) | val loss 16.3467 (now 7.712, fut 8.635) | \n",
            "Epoch 024 | ** best ** train loss 13.7713 (now 6.208, fut 7.564) | val loss 15.9007 (now 7.291, fut 8.609) | \n",
            "Epoch 025 | ** best ** train loss 13.6169 (now 6.118, fut 7.499) | val loss 15.7434 (now 7.201, fut 8.543) | \n",
            "Epoch 026 | ** best ** train loss 13.5207 (now 6.073, fut 7.447) | val loss 15.9611 (now 7.468, fut 8.493) | \n",
            "Epoch 027 | ** best ** train loss 13.3201 (now 5.997, fut 7.323) | val loss 14.9977 (now 6.775, fut 8.222) | \n",
            "Epoch 028 |  train loss 13.1587 (now 5.884, fut 7.275) | val loss 15.3838 (now 6.996, fut 8.388) | \n",
            "Epoch 029 |  train loss 13.1507 (now 5.888, fut 7.263) | val loss 15.3060 (now 6.916, fut 8.390) | \n",
            "Epoch 030 |  train loss 12.9685 (now 5.829, fut 7.140) | val loss 16.0841 (now 7.483, fut 8.601) | \n",
            "Epoch 031 |  train loss 12.8335 (now 5.708, fut 7.126) | val loss 16.0533 (now 7.595, fut 8.458) | \n",
            "Epoch 032 |  train loss 12.8327 (now 5.715, fut 7.118) | val loss 15.4426 (now 6.856, fut 8.586) | \n",
            "Epoch 033 |  train loss 12.6341 (now 5.592, fut 7.042) | val loss 15.6391 (now 7.050, fut 8.589) | \n",
            "Epoch 034 |  train loss 12.5569 (now 5.608, fut 6.949) | val loss 15.0850 (now 6.724, fut 8.361) | \n",
            "Epoch 035 |  train loss 12.4261 (now 5.503, fut 6.923) | val loss 15.0655 (now 6.678, fut 8.387) | \n",
            "Epoch 036 | ** best ** train loss 12.3204 (now 5.486, fut 6.835) | val loss 14.7858 (now 6.607, fut 8.178) | \n",
            "Epoch 037 |  train loss 12.2359 (now 5.434, fut 6.802) | val loss 15.2551 (now 6.744, fut 8.511) | \n",
            "Epoch 038 |  train loss 12.2116 (now 5.420, fut 6.792) | val loss 15.0577 (now 6.744, fut 8.314) | \n",
            "Epoch 039 |  train loss 12.1659 (now 5.383, fut 6.783) | val loss 15.2222 (now 6.878, fut 8.344) | \n",
            "Epoch 040 |  train loss 13.5629 (now 6.200, fut 7.362) | val loss 17.9074 (now 8.662, fut 9.245) | \n",
            "Epoch 041 |  train loss 17.7750 (now 8.473, fut 9.302) | val loss 18.7621 (now 9.411, fut 9.351) | \n",
            "Epoch 042 |  train loss 16.6701 (now 7.826, fut 8.844) | val loss 17.3249 (now 8.302, fut 9.023) | \n",
            "Epoch 043 |  train loss 16.5676 (now 7.778, fut 8.789) | val loss 17.9697 (now 8.816, fut 9.153) | \n",
            "Epoch 044 |  train loss 16.1259 (now 7.589, fut 8.537) | val loss 16.8922 (now 7.876, fut 9.016) | \n",
            "Epoch 045 |  train loss 13.5523 (now 6.129, fut 7.423) | val loss 14.7314 (now 6.296, fut 8.436) | \n",
            "Epoch 046 |  train loss 12.1385 (now 5.380, fut 6.759) | val loss 14.7029 (now 6.328, fut 8.375) | \n",
            "Epoch 047 |  train loss 12.0074 (now 5.292, fut 6.716) | val loss 14.7910 (now 6.469, fut 8.322) | \n",
            "Epoch 048 |  train loss 11.8875 (now 5.236, fut 6.651) | val loss 15.1595 (now 6.639, fut 8.521) | \n",
            "Epoch 049 |  train loss 11.6654 (now 5.145, fut 6.521) | val loss 14.9428 (now 6.657, fut 8.286) | \n",
            "Epoch 050 |  train loss 11.8142 (now 5.238, fut 6.576) | val loss 14.6363 (now 6.418, fut 8.219) | \n",
            "Epoch 051 |  train loss 11.8051 (now 5.220, fut 6.585) | val loss 14.8978 (now 6.437, fut 8.461) | \n",
            "Epoch 052 |  train loss 11.6354 (now 5.138, fut 6.498) | val loss 15.3149 (now 6.823, fut 8.491) | \n",
            "Epoch 053 |  train loss 11.5255 (now 5.079, fut 6.446) | val loss 14.7839 (now 6.552, fut 8.232) | \n",
            "Epoch 054 |  train loss 11.5886 (now 5.147, fut 6.442) | val loss 15.6146 (now 7.044, fut 8.570) | \n",
            "Epoch 055 |  train loss 11.4742 (now 5.053, fut 6.421) | val loss 15.5174 (now 6.811, fut 8.707) | \n",
            "Epoch 056 |  train loss 11.1982 (now 4.923, fut 6.276) | val loss 14.7467 (now 6.229, fut 8.518) | \n",
            "Epoch 057 |  train loss 11.1059 (now 4.869, fut 6.237) | val loss 14.7012 (now 6.292, fut 8.409) | \n",
            "Epoch 058 |  train loss 11.0002 (now 4.833, fut 6.168) | val loss 14.7888 (now 6.298, fut 8.491) | \n",
            "Epoch 059 |  train loss 10.9669 (now 4.801, fut 6.166) | val loss 15.0828 (now 6.486, fut 8.596) | \n",
            "Epoch 060 |  train loss 10.9322 (now 4.793, fut 6.139) | val loss 14.7272 (now 6.419, fut 8.308) | \n",
            "Epoch 061 |  train loss 10.8530 (now 4.751, fut 6.102) | val loss 14.8650 (now 6.419, fut 8.446) | \n",
            "Epoch 062 |  train loss 10.8604 (now 4.775, fut 6.085) | val loss 15.0229 (now 6.516, fut 8.507) | \n",
            "Epoch 063 |  train loss 10.7805 (now 4.741, fut 6.040) | val loss 14.6759 (now 6.288, fut 8.388) | \n",
            "Epoch 064 |  train loss 10.7236 (now 4.717, fut 6.006) | val loss 15.0769 (now 6.529, fut 8.548) | \n",
            "Epoch 065 |  train loss 10.7312 (now 4.702, fut 6.029) | val loss 15.2529 (now 6.560, fut 8.693) | \n",
            "Epoch 066 |  train loss 10.7451 (now 4.729, fut 6.016) | val loss 15.2232 (now 6.568, fut 8.655) | \n",
            "Epoch 067 |  train loss 10.6168 (now 4.684, fut 5.933) | val loss 14.9157 (now 6.409, fut 8.506) | \n",
            "Epoch 068 |  train loss 10.5926 (now 4.654, fut 5.939) | val loss 14.7010 (now 6.169, fut 8.532) | \n",
            "Epoch 069 |  train loss 10.5672 (now 4.637, fut 5.931) | val loss 14.7913 (now 6.280, fut 8.512) | \n",
            "Epoch 070 |  train loss 10.4873 (now 4.595, fut 5.892) | val loss 14.5117 (now 6.119, fut 8.393) | \n",
            "Epoch 071 |  train loss 10.4491 (now 4.598, fut 5.851) | val loss 14.8358 (now 6.265, fut 8.570) | \n",
            "Epoch 072 |  train loss 10.3794 (now 4.560, fut 5.819) | val loss 15.0188 (now 6.557, fut 8.461) | \n",
            "Epoch 073 |  train loss 10.3764 (now 4.552, fut 5.824) | val loss 15.0817 (now 6.501, fut 8.581) | \n",
            "Epoch 074 |  train loss 10.4610 (now 4.613, fut 5.848) | val loss 14.7591 (now 6.272, fut 8.487) | \n",
            "Epoch 075 |  train loss 10.3643 (now 4.564, fut 5.800) | val loss 14.9809 (now 6.267, fut 8.714) | \n",
            "Epoch 076 |  train loss 10.3363 (now 4.559, fut 5.777) | val loss 15.0927 (now 6.504, fut 8.589) | \n",
            "Epoch 077 |  train loss 10.3614 (now 4.581, fut 5.780) | val loss 15.6746 (now 7.027, fut 8.648) | \n",
            "Epoch 078 |  train loss 10.4803 (now 4.606, fut 5.875) | val loss 15.4050 (now 6.765, fut 8.640) | \n",
            "Epoch 079 |  train loss 10.3940 (now 4.579, fut 5.815) | val loss 15.2016 (now 6.442, fut 8.759) | \n",
            "Epoch 080 |  train loss 10.3956 (now 4.579, fut 5.816) | val loss 15.1752 (now 6.418, fut 8.758) | \n",
            "Epoch 081 |  train loss 10.4707 (now 4.651, fut 5.820) | val loss 15.4862 (now 6.844, fut 8.643) | \n",
            "Epoch 082 |  train loss 10.3351 (now 4.567, fut 5.768) | val loss 14.8185 (now 6.362, fut 8.457) | \n",
            "Epoch 083 |  train loss 10.2505 (now 4.528, fut 5.722) | val loss 14.9370 (now 6.320, fut 8.617) | \n",
            "Epoch 084 |  train loss 10.0914 (now 4.458, fut 5.633) | val loss 15.4134 (now 6.572, fut 8.841) | \n",
            "Epoch 085 |  train loss 10.1554 (now 4.492, fut 5.663) | val loss 15.4926 (now 6.531, fut 8.962) | \n",
            "Epoch 086 |  train loss 10.1002 (now 4.480, fut 5.620) | val loss 15.2865 (now 6.727, fut 8.560) | \n",
            "Epoch 087 |  train loss 10.2334 (now 4.551, fut 5.682) | val loss 15.1952 (now 6.575, fut 8.620) | \n",
            "Epoch 088 |  train loss 10.2458 (now 4.540, fut 5.705) | val loss 15.1632 (now 6.471, fut 8.692) | \n",
            "Epoch 089 |  train loss 10.4387 (now 4.634, fut 5.805) | val loss 15.1783 (now 6.578, fut 8.600) | \n",
            "Epoch 090 |  train loss 10.0952 (now 4.478, fut 5.617) | val loss 14.7107 (now 6.141, fut 8.570) | \n",
            "Epoch 091 |  train loss 10.0648 (now 4.467, fut 5.598) | val loss 15.3004 (now 6.437, fut 8.863) | \n",
            "Epoch 092 |  train loss 9.9471 (now 4.403, fut 5.544) | val loss 15.6125 (now 6.836, fut 8.776) | \n",
            "Epoch 093 |  train loss 9.9479 (now 4.386, fut 5.562) | val loss 15.2129 (now 6.539, fut 8.674) | \n",
            "Epoch 094 |  train loss 9.9638 (now 4.409, fut 5.555) | val loss 15.2272 (now 6.524, fut 8.703) | \n",
            "Epoch 095 |  train loss 9.9304 (now 4.391, fut 5.539) | val loss 15.1237 (now 6.502, fut 8.621) | \n",
            "Epoch 096 |  train loss 9.8598 (now 4.351, fut 5.509) | val loss 15.2851 (now 6.679, fut 8.606) | \n",
            "Epoch 097 |  train loss 9.8981 (now 4.380, fut 5.519) | val loss 15.3428 (now 6.414, fut 8.929) | \n",
            "Epoch 098 |  train loss 9.9914 (now 4.444, fut 5.547) | val loss 15.2561 (now 6.522, fut 8.734) | \n",
            "Epoch 099 |  train loss 10.3014 (now 4.615, fut 5.686) | val loss 16.1096 (now 6.994, fut 9.116) | \n",
            "Epoch 100 |  train loss 10.4119 (now 4.673, fut 5.739) | val loss 15.0747 (now 6.391, fut 8.684) | \n",
            "Best validation forecast MAE (S13 fold): 8.1784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 13 - highest performer\n",
        "\n",
        "Best val_MAE for S1-12, S14-15: 14.7858 (100 EPOCHS; reached at epoch 36)\n",
        "\n",
        "- now val_MAE: 6.607\n",
        "- fut val_MAE: 8.1784\n",
        "\n",
        "notes:\n",
        "- best performing - subject 13 could be particularly difficult to decipher. very high fitness (6) so maybe HR was less reactive to activity than other subjects\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "6SfLGwL92JMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 14]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "CqFORf462sEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=14, # test: subject 14\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S14\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S14 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1q5T6C523UH",
        "outputId": "e34e153c-2b30-4ef6-d77a-ad2ffe635ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S14] Train 47288 | Val 11901 | Test 4446 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.1065 (now 74.105, fut 74.001) | val loss 95.9489 (now 49.192, fut 46.757) | \n",
            "Epoch 002 | ** best ** train loss 40.8637 (now 20.458, fut 20.406) | val loss 32.5921 (now 16.430, fut 16.163) | \n",
            "Epoch 003 | ** best ** train loss 28.2389 (now 13.932, fut 14.307) | val loss 29.2577 (now 14.629, fut 14.629) | \n",
            "Epoch 004 | ** best ** train loss 24.5372 (now 12.042, fut 12.495) | val loss 25.4902 (now 12.771, fut 12.719) | \n",
            "Epoch 005 | ** best ** train loss 22.1321 (now 10.856, fut 11.276) | val loss 23.1301 (now 11.656, fut 11.474) | \n",
            "Epoch 006 | ** best ** train loss 20.6189 (now 10.131, fut 10.488) | val loss 21.6911 (now 10.949, fut 10.742) | \n",
            "Epoch 007 | ** best ** train loss 19.4747 (now 9.519, fut 9.956) | val loss 21.2511 (now 10.676, fut 10.575) | \n",
            "Epoch 008 | ** best ** train loss 18.4576 (now 8.982, fut 9.475) | val loss 20.1680 (now 10.152, fut 10.016) | \n",
            "Epoch 009 |  train loss 17.5156 (now 8.474, fut 9.042) | val loss 20.0787 (now 10.006, fut 10.073) | \n",
            "Epoch 010 | ** best ** train loss 16.9989 (now 8.186, fut 8.813) | val loss 19.5194 (now 9.673, fut 9.847) | \n",
            "Epoch 011 |  train loss 16.3878 (now 7.879, fut 8.509) | val loss 19.5560 (now 9.654, fut 9.902) | \n",
            "Epoch 012 | ** best ** train loss 16.0251 (now 7.664, fut 8.362) | val loss 18.2193 (now 8.952, fut 9.267) | \n",
            "Epoch 013 |  train loss 15.7024 (now 7.483, fut 8.219) | val loss 18.9141 (now 9.270, fut 9.644) | \n",
            "Epoch 014 |  train loss 15.3573 (now 7.319, fut 8.039) | val loss 18.8149 (now 9.170, fut 9.645) | \n",
            "Epoch 015 | ** best ** train loss 15.0805 (now 7.152, fut 7.928) | val loss 17.8198 (now 8.698, fut 9.122) | \n",
            "Epoch 016 |  train loss 14.8695 (now 7.033, fut 7.836) | val loss 18.2932 (now 9.050, fut 9.244) | \n",
            "Epoch 017 |  train loss 14.6913 (now 6.917, fut 7.774) | val loss 18.0145 (now 8.878, fut 9.137) | \n",
            "Epoch 018 |  train loss 14.4932 (now 6.850, fut 7.643) | val loss 18.0220 (now 8.803, fut 9.219) | \n",
            "Epoch 019 |  train loss 14.2832 (now 6.731, fut 7.552) | val loss 18.5491 (now 9.068, fut 9.481) | \n",
            "Epoch 020 | ** best ** train loss 14.0889 (now 6.633, fut 7.456) | val loss 17.6586 (now 8.609, fut 9.050) | \n",
            "Epoch 021 | ** best ** train loss 13.9401 (now 6.560, fut 7.380) | val loss 17.0012 (now 8.204, fut 8.798) | \n",
            "Epoch 022 |  train loss 13.7563 (now 6.434, fut 7.322) | val loss 17.4179 (now 8.407, fut 9.011) | \n",
            "Epoch 023 | ** best ** train loss 13.6585 (now 6.407, fut 7.252) | val loss 17.1878 (now 8.401, fut 8.787) | \n",
            "Epoch 024 |  train loss 13.4883 (now 6.303, fut 7.185) | val loss 17.0334 (now 8.189, fut 8.844) | \n",
            "Epoch 025 |  train loss 13.3448 (now 6.218, fut 7.127) | val loss 17.3957 (now 8.333, fut 9.063) | \n",
            "Epoch 026 |  train loss 13.2530 (now 6.185, fut 7.068) | val loss 17.2629 (now 8.216, fut 9.047) | \n",
            "Epoch 027 | ** best ** train loss 13.1152 (now 6.112, fut 7.004) | val loss 16.3239 (now 7.686, fut 8.638) | \n",
            "Epoch 028 |  train loss 13.0618 (now 6.075, fut 6.987) | val loss 16.9468 (now 8.005, fut 8.942) | \n",
            "Epoch 029 |  train loss 13.0041 (now 6.073, fut 6.932) | val loss 16.9274 (now 8.000, fut 8.927) | \n",
            "Epoch 030 |  train loss 13.0351 (now 6.071, fut 6.964) | val loss 17.1085 (now 8.087, fut 9.021) | \n",
            "Epoch 031 |  train loss 12.8330 (now 5.982, fut 6.851) | val loss 17.2754 (now 8.363, fut 8.912) | \n",
            "Epoch 032 |  train loss 12.7480 (now 5.929, fut 6.819) | val loss 16.5345 (now 7.833, fut 8.702) | \n",
            "Epoch 033 |  train loss 12.7577 (now 5.942, fut 6.815) | val loss 16.5559 (now 7.830, fut 8.725) | \n",
            "Epoch 034 |  train loss 12.6250 (now 5.868, fut 6.757) | val loss 16.9296 (now 8.184, fut 8.746) | \n",
            "Epoch 035 |  train loss 12.5715 (now 5.859, fut 6.713) | val loss 16.4909 (now 7.764, fut 8.727) | \n",
            "Epoch 036 |  train loss 12.5314 (now 5.852, fut 6.680) | val loss 16.8582 (now 8.086, fut 8.772) | \n",
            "Epoch 037 |  train loss 12.3966 (now 5.777, fut 6.619) | val loss 16.9070 (now 8.113, fut 8.794) | \n",
            "Epoch 038 | ** best ** train loss 12.4037 (now 5.783, fut 6.621) | val loss 16.3928 (now 7.755, fut 8.638) | \n",
            "Epoch 039 |  train loss 12.3106 (now 5.712, fut 6.599) | val loss 17.2655 (now 8.338, fut 8.927) | \n",
            "Epoch 040 |  train loss 12.2121 (now 5.676, fut 6.536) | val loss 17.1878 (now 8.266, fut 8.922) | \n",
            "Epoch 041 |  train loss 12.1562 (now 5.642, fut 6.514) | val loss 16.8704 (now 8.086, fut 8.784) | \n",
            "Epoch 042 |  train loss 12.1392 (now 5.641, fut 6.498) | val loss 16.6977 (now 7.928, fut 8.769) | \n",
            "Epoch 043 |  train loss 12.0192 (now 5.547, fut 6.472) | val loss 17.3207 (now 8.456, fut 8.865) | \n",
            "Epoch 044 |  train loss 12.0932 (now 5.609, fut 6.485) | val loss 16.5396 (now 7.816, fut 8.724) | \n",
            "Epoch 045 |  train loss 12.0445 (now 5.561, fut 6.484) | val loss 17.0270 (now 8.035, fut 8.992) | \n",
            "Epoch 046 |  train loss 11.9441 (now 5.537, fut 6.407) | val loss 17.3084 (now 8.285, fut 9.024) | \n",
            "Epoch 047 |  train loss 11.9014 (now 5.536, fut 6.365) | val loss 17.4256 (now 8.447, fut 8.978) | \n",
            "Epoch 048 |  train loss 11.8940 (now 5.522, fut 6.373) | val loss 17.0173 (now 8.130, fut 8.887) | \n",
            "Epoch 049 |  train loss 11.7582 (now 5.462, fut 6.297) | val loss 16.8224 (now 7.975, fut 8.848) | \n",
            "Epoch 050 |  train loss 11.7148 (now 5.433, fut 6.282) | val loss 17.1820 (now 8.093, fut 9.089) | \n",
            "Epoch 051 |  train loss 11.6877 (now 5.436, fut 6.251) | val loss 16.6952 (now 7.879, fut 8.816) | \n",
            "Epoch 052 |  train loss 11.7257 (now 5.439, fut 6.286) | val loss 17.0771 (now 8.087, fut 8.990) | \n",
            "Epoch 053 |  train loss 11.6461 (now 5.400, fut 6.246) | val loss 16.9065 (now 7.999, fut 8.908) | \n",
            "Epoch 054 |  train loss 11.6803 (now 5.414, fut 6.266) | val loss 17.3941 (now 8.407, fut 8.987) | \n",
            "Epoch 055 |  train loss 11.5589 (now 5.381, fut 6.178) | val loss 17.1348 (now 8.129, fut 9.006) | \n",
            "Epoch 056 |  train loss 11.5291 (now 5.352, fut 6.177) | val loss 17.1013 (now 8.166, fut 8.936) | \n",
            "Epoch 057 |  train loss 11.4528 (now 5.308, fut 6.144) | val loss 17.2931 (now 8.136, fut 9.158) | \n",
            "Epoch 058 |  train loss 11.4222 (now 5.266, fut 6.156) | val loss 16.9603 (now 8.026, fut 8.934) | \n",
            "Epoch 059 |  train loss 11.3823 (now 5.306, fut 6.076) | val loss 17.0109 (now 8.063, fut 8.948) | \n",
            "Epoch 060 |  train loss 11.4077 (now 5.311, fut 6.096) | val loss 17.0554 (now 8.060, fut 8.995) | \n",
            "Epoch 061 |  train loss 11.3081 (now 5.268, fut 6.040) | val loss 17.1094 (now 7.965, fut 9.145) | \n",
            "Epoch 062 |  train loss 11.2832 (now 5.221, fut 6.062) | val loss 17.6323 (now 8.411, fut 9.222) | \n",
            "Epoch 063 |  train loss 11.2368 (now 5.197, fut 6.040) | val loss 17.1961 (now 8.057, fut 9.140) | \n",
            "Epoch 064 |  train loss 11.1560 (now 5.166, fut 5.990) | val loss 17.1578 (now 8.138, fut 9.020) | \n",
            "Epoch 065 |  train loss 11.1869 (now 5.187, fut 6.000) | val loss 17.1460 (now 8.135, fut 9.011) | \n",
            "Epoch 066 |  train loss 11.0941 (now 5.138, fut 5.956) | val loss 17.1179 (now 8.126, fut 8.992) | \n",
            "Epoch 067 |  train loss 11.1271 (now 5.175, fut 5.953) | val loss 16.9259 (now 8.015, fut 8.910) | \n",
            "Epoch 068 |  train loss 11.0844 (now 5.163, fut 5.921) | val loss 16.8240 (now 7.785, fut 9.039) | \n",
            "Epoch 069 |  train loss 11.0431 (now 5.130, fut 5.913) | val loss 16.8475 (now 7.804, fut 9.043) | \n",
            "Epoch 070 |  train loss 10.9811 (now 5.091, fut 5.890) | val loss 16.9219 (now 7.956, fut 8.966) | \n",
            "Epoch 071 |  train loss 10.9958 (now 5.100, fut 5.896) | val loss 17.1333 (now 8.070, fut 9.063) | \n",
            "Epoch 072 |  train loss 10.9449 (now 5.060, fut 5.885) | val loss 16.6007 (now 7.741, fut 8.859) | \n",
            "Epoch 073 |  train loss 10.9297 (now 5.065, fut 5.865) | val loss 17.2144 (now 8.185, fut 9.030) | \n",
            "Epoch 074 |  train loss 10.9352 (now 5.094, fut 5.841) | val loss 16.9906 (now 8.088, fut 8.902) | \n",
            "Epoch 075 |  train loss 10.9020 (now 5.048, fut 5.854) | val loss 17.1151 (now 7.989, fut 9.126) | \n",
            "Epoch 076 |  train loss 10.8661 (now 5.043, fut 5.823) | val loss 17.2473 (now 8.185, fut 9.062) | \n",
            "Epoch 077 |  train loss 10.7802 (now 5.014, fut 5.766) | val loss 17.1821 (now 8.019, fut 9.163) | \n",
            "Epoch 078 |  train loss 10.8329 (now 5.010, fut 5.823) | val loss 17.1632 (now 7.995, fut 9.168) | \n",
            "Epoch 079 |  train loss 10.7772 (now 5.017, fut 5.760) | val loss 17.1907 (now 7.990, fut 9.201) | \n",
            "Epoch 080 |  train loss 10.7457 (now 4.976, fut 5.769) | val loss 17.0368 (now 7.978, fut 9.059) | \n",
            "Epoch 081 |  train loss 10.7329 (now 4.995, fut 5.738) | val loss 17.0176 (now 7.825, fut 9.193) | \n",
            "Epoch 082 |  train loss 10.6757 (now 4.946, fut 5.730) | val loss 16.8647 (now 7.808, fut 9.057) | \n",
            "Epoch 083 |  train loss 10.6609 (now 4.941, fut 5.720) | val loss 17.0291 (now 7.864, fut 9.165) | \n",
            "Epoch 084 |  train loss 10.6461 (now 4.937, fut 5.709) | val loss 17.2955 (now 8.167, fut 9.128) | \n",
            "Epoch 085 |  train loss 10.6092 (now 4.924, fut 5.685) | val loss 17.3260 (now 8.007, fut 9.319) | \n",
            "Epoch 086 |  train loss 10.5704 (now 4.894, fut 5.676) | val loss 17.6421 (now 8.296, fut 9.346) | \n",
            "Epoch 087 |  train loss 10.5525 (now 4.891, fut 5.661) | val loss 17.3116 (now 8.056, fut 9.256) | \n",
            "Epoch 088 |  train loss 10.5252 (now 4.882, fut 5.643) | val loss 17.3453 (now 8.281, fut 9.064) | \n",
            "Epoch 089 |  train loss 10.5561 (now 4.900, fut 5.657) | val loss 17.3186 (now 8.019, fut 9.299) | \n",
            "Epoch 090 |  train loss 10.5868 (now 4.918, fut 5.669) | val loss 17.0906 (now 7.922, fut 9.168) | \n",
            "Epoch 091 |  train loss 10.4484 (now 4.830, fut 5.618) | val loss 17.5847 (now 8.129, fut 9.455) | \n",
            "Epoch 092 |  train loss 10.4569 (now 4.862, fut 5.595) | val loss 17.3719 (now 8.131, fut 9.241) | \n",
            "Epoch 093 |  train loss 10.4934 (now 4.889, fut 5.604) | val loss 17.0519 (now 7.904, fut 9.148) | \n",
            "Epoch 094 |  train loss 10.4429 (now 4.858, fut 5.585) | val loss 17.0508 (now 7.918, fut 9.133) | \n",
            "Epoch 095 |  train loss 10.3797 (now 4.830, fut 5.550) | val loss 17.3454 (now 8.188, fut 9.157) | \n",
            "Epoch 096 |  train loss 10.4127 (now 4.841, fut 5.572) | val loss 17.8429 (now 8.407, fut 9.436) | \n",
            "Epoch 097 |  train loss 10.3420 (now 4.793, fut 5.549) | val loss 17.7214 (now 8.422, fut 9.300) | \n",
            "Epoch 098 |  train loss 10.4262 (now 4.852, fut 5.575) | val loss 17.5325 (now 8.202, fut 9.330) | \n",
            "Epoch 099 |  train loss 10.4385 (now 4.876, fut 5.563) | val loss 17.2310 (now 8.031, fut 9.200) | \n",
            "Epoch 100 |  train loss 10.3344 (now 4.813, fut 5.522) | val loss 17.0482 (now 8.003, fut 9.046) | \n",
            "Best validation forecast MAE (S14 fold): 8.6375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 14\n",
        "\n",
        "Best val_MAE for S1-13, S15: 16.3928 (100 EPOCHS; reached at epoch 38)\n",
        "\n",
        "- now val_MAE: 7.755\n",
        "- fut val_MAE: 8.6375\n",
        "\n",
        "notes:\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "KczUy5Va2wY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TEST: SUBJECT 15]\n",
        "\n",
        "- epochs: 100\n",
        "- warmup epochs: 5"
      ],
      "metadata": {
        "id": "IS9JqvrA2-Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "\n",
        "# build dataloaders\n",
        "fold = build_loso_fold_with_activity(\n",
        "    all_subjects_64hz=all_subjects_64hz,\n",
        "    activity_dir=ACTIVITY_DIR,\n",
        "    test_sid=15, # test: subject 15\n",
        "    H_segments=30,\n",
        "    val_frac=0.2,\n",
        "    embargo_seconds=8,\n",
        "    batch_train=128,\n",
        "    batch_eval=256\n",
        ")\n",
        "\n",
        "train_loader = fold[\"train_loader\"]\n",
        "val_loader = fold[\"val_loader\"]\n",
        "test_loader = fold[\"test_loader\"]\n",
        "\n",
        "# instantiate model\n",
        "model = MultiTaskGRU(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")\n",
        "\n",
        "# cfg\n",
        "cfg = TrainConfig(\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    lambda_fut=1.0,\n",
        "    max_epochs=100,\n",
        "    grad_clip=1.0,\n",
        "    use_amp=True,\n",
        "    early_stopping=False,\n",
        "    scheduler=\"cosine\",\n",
        "    warmup_epochs=5,\n",
        "    ckpt_dir=\"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        ")\n",
        "\n",
        "# train\n",
        "model, best_val = fit_fold(model, train_loader, val_loader, cfg, fold_tag=\"S15\", iterator=0)\n",
        "print(f\"Best validation forecast MAE (S15 fold): {best_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81_itoTh3HA1",
        "outputId": "019331f6-c0fa-45f4-dbf5-93fa231e1202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S15] Train 47697 | Val 12002 | Test 3936 | H=30 seg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1992006459.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.use_amp)\n",
            "/tmp/ipython-input-1212191027.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | ** best ** train loss 148.9155 (now 74.280, fut 74.635) | val loss 96.9368 (now 48.984, fut 47.953) | \n",
            "Epoch 002 | ** best ** train loss 37.9582 (now 18.845, fut 19.113) | val loss 38.1755 (now 19.160, fut 19.016) | \n",
            "Epoch 003 | ** best ** train loss 33.1620 (now 17.234, fut 15.928) | val loss 30.4458 (now 15.535, fut 14.911) | \n",
            "Epoch 004 |  train loss 33.6599 (now 17.145, fut 16.515) | val loss 33.7538 (now 17.517, fut 16.237) | \n",
            "Epoch 005 | ** best ** train loss 32.9916 (now 16.806, fut 16.186) | val loss 29.2647 (now 15.126, fut 14.139) | \n",
            "Epoch 006 | ** best ** train loss 29.4194 (now 14.944, fut 14.476) | val loss 27.9390 (now 14.351, fut 13.588) | \n",
            "Epoch 007 | ** best ** train loss 28.1121 (now 14.296, fut 13.816) | val loss 27.5364 (now 14.510, fut 13.026) | \n",
            "Epoch 008 | ** best ** train loss 25.5145 (now 12.796, fut 12.719) | val loss 26.6445 (now 13.664, fut 12.981) | \n",
            "Epoch 009 |  train loss 26.7577 (now 13.543, fut 13.215) | val loss 26.8039 (now 13.775, fut 13.029) | \n",
            "Epoch 010 | ** best ** train loss 27.0077 (now 13.661, fut 13.347) | val loss 26.4883 (now 13.763, fut 12.726) | \n",
            "Epoch 011 | ** best ** train loss 28.1328 (now 14.313, fut 13.820) | val loss 26.1106 (now 13.785, fut 12.326) | \n",
            "Epoch 012 |  train loss 27.3418 (now 13.894, fut 13.447) | val loss 26.4255 (now 13.950, fut 12.476) | \n",
            "Epoch 013 | ** best ** train loss 25.1330 (now 12.675, fut 12.458) | val loss 24.9411 (now 12.830, fut 12.111) | \n",
            "Epoch 014 | ** best ** train loss 23.8830 (now 11.983, fut 11.900) | val loss 24.5841 (now 12.782, fut 11.802) | \n",
            "Epoch 015 | ** best ** train loss 23.5783 (now 11.843, fut 11.735) | val loss 24.7422 (now 12.982, fut 11.760) | \n",
            "Epoch 016 | ** best ** train loss 22.3306 (now 11.226, fut 11.105) | val loss 23.7988 (now 12.293, fut 11.505) | \n",
            "Epoch 017 | ** best ** train loss 20.9132 (now 10.428, fut 10.485) | val loss 22.8799 (now 11.918, fut 10.962) | \n",
            "Epoch 018 |  train loss 20.3476 (now 10.145, fut 10.203) | val loss 22.4878 (now 11.513, fut 10.975) | \n",
            "Epoch 019 | ** best ** train loss 19.7005 (now 9.763, fut 9.938) | val loss 22.3735 (now 11.593, fut 10.780) | \n",
            "Epoch 020 |  train loss 20.8471 (now 10.465, fut 10.382) | val loss 24.0003 (now 12.400, fut 11.601) | \n",
            "Epoch 021 |  train loss 21.6334 (now 10.880, fut 10.754) | val loss 23.6525 (now 12.173, fut 11.479) | \n",
            "Epoch 022 |  train loss 20.9797 (now 10.501, fut 10.479) | val loss 22.7710 (now 11.953, fut 10.818) | \n",
            "Epoch 023 |  train loss 21.4564 (now 10.798, fut 10.659) | val loss 22.3548 (now 11.552, fut 10.802) | \n",
            "Epoch 024 |  train loss 21.3207 (now 10.708, fut 10.613) | val loss 22.8877 (now 11.925, fut 10.962) | \n",
            "Epoch 025 | ** best ** train loss 20.1737 (now 10.044, fut 10.130) | val loss 21.8252 (now 11.271, fut 10.554) | \n",
            "Epoch 026 | ** best ** train loss 19.1612 (now 9.486, fut 9.675) | val loss 21.5451 (now 10.996, fut 10.549) | \n",
            "Epoch 027 | ** best ** train loss 19.1383 (now 9.523, fut 9.615) | val loss 21.5564 (now 11.019, fut 10.537) | \n",
            "Epoch 028 |  train loss 18.9190 (now 9.350, fut 9.569) | val loss 21.7176 (now 11.156, fut 10.561) | \n",
            "Epoch 029 |  train loss 18.6351 (now 9.242, fut 9.393) | val loss 22.8794 (now 11.544, fut 11.336) | \n",
            "Epoch 030 |  train loss 18.6687 (now 9.257, fut 9.411) | val loss 22.8080 (now 11.804, fut 11.004) | \n",
            "Epoch 031 | ** best ** train loss 18.7506 (now 9.307, fut 9.444) | val loss 21.1490 (now 10.809, fut 10.340) | \n",
            "Epoch 032 |  train loss 19.0967 (now 9.561, fut 9.536) | val loss 22.3958 (now 11.436, fut 10.959) | \n",
            "Epoch 033 |  train loss 19.2818 (now 9.603, fut 9.678) | val loss 22.4447 (now 11.466, fut 10.978) | \n",
            "Epoch 034 |  train loss 18.8444 (now 9.365, fut 9.480) | val loss 22.0374 (now 11.483, fut 10.554) | \n",
            "Epoch 035 |  train loss 18.9550 (now 9.409, fut 9.546) | val loss 22.6143 (now 11.619, fut 10.995) | \n",
            "Epoch 036 |  train loss 18.9586 (now 9.439, fut 9.520) | val loss 22.1961 (now 11.374, fut 10.822) | \n",
            "Epoch 037 |  train loss 19.1620 (now 9.585, fut 9.577) | val loss 22.4563 (now 11.622, fut 10.834) | \n",
            "Epoch 038 |  train loss 19.0028 (now 9.499, fut 9.503) | val loss 22.5579 (now 11.778, fut 10.780) | \n",
            "Epoch 039 |  train loss 18.6466 (now 9.337, fut 9.310) | val loss 22.2598 (now 11.612, fut 10.647) | \n",
            "Epoch 040 |  train loss 18.5323 (now 9.231, fut 9.301) | val loss 22.1772 (now 11.346, fut 10.831) | \n",
            "Epoch 041 | ** best ** train loss 17.9827 (now 8.927, fut 9.056) | val loss 21.6009 (now 11.262, fut 10.339) | \n",
            "Epoch 042 |  train loss 17.9212 (now 8.905, fut 9.016) | val loss 21.7229 (now 10.999, fut 10.724) | \n",
            "Epoch 043 |  train loss 17.8069 (now 8.862, fut 8.945) | val loss 22.5058 (now 11.567, fut 10.939) | \n",
            "Epoch 044 |  train loss 17.5435 (now 8.728, fut 8.816) | val loss 22.0907 (now 11.347, fut 10.744) | \n",
            "Epoch 045 |  train loss 17.3664 (now 8.605, fut 8.761) | val loss 21.3313 (now 10.918, fut 10.414) | \n",
            "Epoch 046 |  train loss 17.6106 (now 8.758, fut 8.853) | val loss 21.3588 (now 10.922, fut 10.437) | \n",
            "Epoch 047 | ** best ** train loss 16.9247 (now 8.376, fut 8.549) | val loss 20.9319 (now 10.596, fut 10.336) | \n",
            "Epoch 048 | ** best ** train loss 16.6950 (now 8.248, fut 8.447) | val loss 21.0181 (now 10.702, fut 10.316) | \n",
            "Epoch 049 |  train loss 16.6415 (now 8.220, fut 8.421) | val loss 22.1818 (now 11.513, fut 10.669) | \n",
            "Epoch 050 | ** best ** train loss 16.3170 (now 8.081, fut 8.236) | val loss 20.7445 (now 10.482, fut 10.262) | \n",
            "Epoch 051 | ** best ** train loss 16.1980 (now 7.965, fut 8.233) | val loss 20.7030 (now 10.485, fut 10.218) | \n",
            "Epoch 052 | ** best ** train loss 16.3262 (now 8.084, fut 8.242) | val loss 20.3970 (now 10.388, fut 10.009) | \n",
            "Epoch 053 |  train loss 16.4211 (now 8.121, fut 8.300) | val loss 20.8342 (now 10.446, fut 10.388) | \n",
            "Epoch 054 |  train loss 16.2295 (now 8.019, fut 8.210) | val loss 20.6805 (now 10.541, fut 10.139) | \n",
            "Epoch 055 |  train loss 16.0601 (now 7.900, fut 8.160) | val loss 21.2624 (now 10.856, fut 10.406) | \n",
            "Epoch 056 |  train loss 16.1921 (now 7.995, fut 8.197) | val loss 21.0369 (now 10.712, fut 10.325) | \n",
            "Epoch 057 |  train loss 16.1954 (now 7.968, fut 8.227) | val loss 20.5858 (now 10.395, fut 10.191) | \n",
            "Epoch 058 |  train loss 15.9453 (now 7.817, fut 8.128) | val loss 20.9805 (now 10.674, fut 10.306) | \n",
            "Epoch 059 | ** best ** train loss 15.5714 (now 7.724, fut 7.847) | val loss 19.8031 (now 10.027, fut 9.776) | \n",
            "Epoch 060 |  train loss 15.6490 (now 7.746, fut 7.903) | val loss 21.0931 (now 10.780, fut 10.313) | \n",
            "Epoch 061 |  train loss 15.6709 (now 7.743, fut 7.928) | val loss 21.2648 (now 10.743, fut 10.522) | \n",
            "Epoch 062 |  train loss 15.9529 (now 7.883, fut 8.070) | val loss 20.8992 (now 10.489, fut 10.410) | \n",
            "Epoch 063 |  train loss 16.4071 (now 8.114, fut 8.293) | val loss 21.8854 (now 11.137, fut 10.748) | \n",
            "Epoch 064 |  train loss 16.4057 (now 8.068, fut 8.338) | val loss 22.0182 (now 11.173, fut 10.845) | \n",
            "Epoch 065 |  train loss 16.9596 (now 8.432, fut 8.527) | val loss 21.8235 (now 11.183, fut 10.640) | \n",
            "Epoch 066 |  train loss 17.8793 (now 8.904, fut 8.975) | val loss 21.2349 (now 10.954, fut 10.281) | \n",
            "Epoch 067 |  train loss 18.7942 (now 9.473, fut 9.322) | val loss 21.6223 (now 11.317, fut 10.305) | \n",
            "Epoch 068 |  train loss 19.0321 (now 9.539, fut 9.493) | val loss 23.9875 (now 12.489, fut 11.498) | \n",
            "Epoch 069 |  train loss 18.4215 (now 9.228, fut 9.194) | val loss 21.8102 (now 11.240, fut 10.570) | \n",
            "Epoch 070 |  train loss 18.1435 (now 9.029, fut 9.115) | val loss 21.3474 (now 10.907, fut 10.441) | \n",
            "Epoch 071 |  train loss 18.3282 (now 9.122, fut 9.206) | val loss 21.5683 (now 11.265, fut 10.304) | \n",
            "Epoch 072 |  train loss 18.8384 (now 9.451, fut 9.387) | val loss 22.1869 (now 11.488, fut 10.699) | \n",
            "Epoch 073 |  train loss 19.3689 (now 9.704, fut 9.665) | val loss 22.5493 (now 11.696, fut 10.854) | \n",
            "Epoch 074 |  train loss 19.3632 (now 9.681, fut 9.682) | val loss 22.9220 (now 12.059, fut 10.863) | \n",
            "Epoch 075 |  train loss 19.2809 (now 9.688, fut 9.593) | val loss 22.6310 (now 11.717, fut 10.914) | \n",
            "Epoch 076 |  train loss 18.5778 (now 9.219, fut 9.358) | val loss 23.6032 (now 12.340, fut 11.263) | \n",
            "Epoch 077 |  train loss 18.5357 (now 9.257, fut 9.278) | val loss 22.2323 (now 11.642, fut 10.590) | \n",
            "Epoch 078 |  train loss 18.7006 (now 9.360, fut 9.341) | val loss 22.9906 (now 11.946, fut 11.045) | \n",
            "Epoch 079 |  train loss 18.4452 (now 9.174, fut 9.272) | val loss 23.5563 (now 12.259, fut 11.297) | \n",
            "Epoch 080 |  train loss 18.4827 (now 9.236, fut 9.247) | val loss 22.1106 (now 11.404, fut 10.707) | \n",
            "Epoch 081 |  train loss 17.8037 (now 8.861, fut 8.943) | val loss 22.5005 (now 11.537, fut 10.963) | \n",
            "Epoch 082 |  train loss 17.2686 (now 8.553, fut 8.716) | val loss 22.4400 (now 11.553, fut 10.887) | \n",
            "Epoch 083 |  train loss 16.5779 (now 8.185, fut 8.393) | val loss 21.7490 (now 10.968, fut 10.781) | \n",
            "Epoch 084 |  train loss 16.7391 (now 8.263, fut 8.476) | val loss 21.5981 (now 11.085, fut 10.513) | \n",
            "Epoch 085 |  train loss 16.5413 (now 8.205, fut 8.336) | val loss 21.3890 (now 11.050, fut 10.339) | \n",
            "Epoch 086 |  train loss 17.0952 (now 8.465, fut 8.630) | val loss 21.8470 (now 11.252, fut 10.595) | \n",
            "Epoch 087 |  train loss 17.0204 (now 8.453, fut 8.568) | val loss 21.4808 (now 11.081, fut 10.400) | \n",
            "Epoch 088 |  train loss 17.0585 (now 8.461, fut 8.598) | val loss 22.1412 (now 11.233, fut 10.908) | \n",
            "Epoch 089 |  train loss 16.7917 (now 8.309, fut 8.482) | val loss 22.1263 (now 11.408, fut 10.718) | \n",
            "Epoch 090 |  train loss 16.9161 (now 8.374, fut 8.542) | val loss 21.7261 (now 11.191, fut 10.535) | \n",
            "Epoch 091 |  train loss 16.9509 (now 8.408, fut 8.543) | val loss 21.5833 (now 11.014, fut 10.570) | \n",
            "Epoch 092 |  train loss 16.4915 (now 8.171, fut 8.320) | val loss 21.4535 (now 10.948, fut 10.505) | \n",
            "Epoch 093 |  train loss 16.8066 (now 8.346, fut 8.461) | val loss 22.1692 (now 11.416, fut 10.753) | \n",
            "Epoch 094 |  train loss 17.2191 (now 8.589, fut 8.630) | val loss 22.2112 (now 11.525, fut 10.686) | \n",
            "Epoch 095 |  train loss 17.8681 (now 8.951, fut 8.917) | val loss 21.6484 (now 11.016, fut 10.632) | \n",
            "Epoch 096 |  train loss 17.6527 (now 8.823, fut 8.830) | val loss 21.9781 (now 11.282, fut 10.696) | \n",
            "Epoch 097 |  train loss 17.8587 (now 8.883, fut 8.976) | val loss 22.4965 (now 11.412, fut 11.085) | \n",
            "Epoch 098 |  train loss 17.3930 (now 8.641, fut 8.752) | val loss 21.7904 (now 11.191, fut 10.600) | \n",
            "Epoch 099 |  train loss 16.5917 (now 8.197, fut 8.394) | val loss 21.8885 (now 11.183, fut 10.706) | \n",
            "Epoch 100 |  train loss 16.4304 (now 8.144, fut 8.286) | val loss 21.5943 (now 11.096, fut 10.498) | \n",
            "Best validation forecast MAE (S15 fold): 9.7761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject 15\n",
        "\n",
        "Best val_MAE for S1-14: 19.8031 (100 EPOCHS; reached at epoch 59)\n",
        "\n",
        "- now val_MAE: 10.027\n",
        "- fut val_MAE: 9.7761\n",
        "\n",
        "notes:\n",
        "- overfitting"
      ],
      "metadata": {
        "id": "Sm_q5PjF3BzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------\n",
        "## VALIDATION SET RESULTS (bpm)\n",
        "\n",
        "### Average val_MAE: 17.7819\n",
        "### Average val now_MAE: 8.5187\n",
        "### Average val fut_MAE: 9.2630\n",
        "\n",
        "Thoughts:\n",
        "- There is natural variation in HR between individuals, whether they're performing physical activity or not.\n",
        "- Literature suggests that HR can differ by 20-30bpm when resting, submaximal activity 5-15bpm, and high-intensity activity by 10-20bpm.\n",
        "- Therefore, MAE of 8.5 and 9.3 respectively is very strong, as a majority of that error is due to intrinsic differences.\n",
        "\n",
        "- It would be interesting to implement an MAE per activity to get an understanding where a majority of the error is coming from.\n",
        "\n",
        "### Calibration\n",
        "\n",
        "Comparison to Consumer Wearables (vs ECG):\n",
        "- Reported MAE ~ 7-10 bpm\n",
        "- MAE at rest: ~ 3-5 bpm\n",
        "- MAE during exercise: ~ 10-15 bpm\n",
        "\n",
        "Comparison to Research-grade Wearables (vs ECG):\n",
        "- Reported MAE ~ 3-6 bpm\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "*consumer wearables include but are not limited to: Fitbit, Garmin, Apple Watch.\n",
        "\n",
        "*research-grade wearables include not are not limited to: polar, chest straps, clinical PPG."
      ],
      "metadata": {
        "id": "IuL_crQaFYgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "- All state_dicts from highest performing epochs from each training loop will now be ran on their respective test dataset, which is an entire new subject whose data wasn't exposed to during training.\n",
        "- The results across all test subjects will then be averaged.\n",
        "\n",
        "In reality, if we were to deploy this, we would train a new model on the same infrastructure etc. but train on all the subject data. The FOSO method is simply to get an understanding of the model's performance on unseen data (15 times), for which we'd expect that performance to replicate on the the deployed model."
      ],
      "metadata": {
        "id": "E2wu2Y8CTmy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for loading saved state_dict into model for evaluation"
      ],
      "metadata": {
        "id": "fYXFnaLRX1r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def load_model_from_ckpt(ckpt_path, model_cls, model_kwargs=None, device=None):\n",
        "  \"\"\"\n",
        "  ckpt_path: location of checkpoint\n",
        "  model_cls: class object\n",
        "  model_kwargs: dict of infrastructure for model's GRU and heads\n",
        "\n",
        "  loads saved state_dict into model for evaluation\n",
        "  \"\"\"\n",
        "  # asserting infrastructure and device\n",
        "  model_kwargs = model_kwargs or {}\n",
        "  device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # fresh model with same architecture used during training\n",
        "  model = model_cls(**model_kwargs).to(device) # ** converts the dictionary into keyword args\n",
        "\n",
        "  # load checkpoint\n",
        "  state = torch.load(ckpt_path, map_location=device)\n",
        "  model.load_state_dict(state[\"model\"], strict=True)\n",
        "  model.eval()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "sWwMSId5NPBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path and Model Infrastructure"
      ],
      "metadata": {
        "id": "pb9TaE75X9JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paths\n",
        "ACTIVITY_DIR = \"/content/drive/MyDrive/PPG_DaLia/activities\"\n",
        "CKPT_DIR = \"/content/drive/MyDrive/PPG_DaLia/checkpoints\"\n",
        "\n",
        "# training-time infrastructure\n",
        "model_kwargs = dict(\n",
        "    input_dim=6,\n",
        "    hidden_size=96,\n",
        "    num_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=False,\n",
        "    head_hidden=64,\n",
        "    head_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "PdskUXjUXZnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Folds and Models Dictionaries Setup"
      ],
      "metadata": {
        "id": "YptD4XkMYLCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "folds = {}\n",
        "models = {}\n",
        "\n",
        "# for every evaluation (test subject)\n",
        "for sid in range (1, 16):\n",
        "  # rebuild LOSO fold to retrieve test loader\n",
        "  fold = build_loso_fold_with_activity(\n",
        "      all_subjects_64hz=all_subjects_64hz,\n",
        "      activity_dir=ACTIVITY_DIR,\n",
        "      test_sid=sid,\n",
        "      H_segments=30,\n",
        "      val_frac=0.2,\n",
        "      embargo_seconds=8,\n",
        "      batch_train=128,\n",
        "      batch_eval=256\n",
        "  )\n",
        "\n",
        "  # store fold for each test\n",
        "  folds[sid] = fold\n",
        "\n",
        "  # retrieve specific checkpoint\n",
        "  ckpt_path = os.path.join(CKPT_DIR, f\"best_S{sid}_0.pt\")\n",
        "  if not os.path.exists(ckpt_path):\n",
        "    print(f\"[WARM] Missing checkpoint for S{sid}: {ckpt_path}\")\n",
        "    continue\n",
        "\n",
        "  # instantiate model with correct parameters\n",
        "  model = load_model_from_ckpt(\n",
        "      ckpt_path=ckpt_path,\n",
        "      model_cls=MultiTaskGRU,\n",
        "      model_kwargs=model_kwargs,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  models[sid] = model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9wDY8exYKW5",
        "outputId": "122bf667-58bb-4e0c-8300-49901aac3a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold S1] Train 47188 | Val 11874 | Test 4573 | H=30 seg\n",
            "[Fold S2] Train 47590 | Val 11976 | Test 4069 | H=30 seg\n",
            "[Fold S3] Train 47376 | Val 11922 | Test 4337 | H=30 seg\n",
            "[Fold S4] Train 47212 | Val 11881 | Test 4542 | H=30 seg\n",
            "[Fold S5] Train 47153 | Val 11867 | Test 4619 | H=30 seg\n",
            "[Fold S6] Train 48762 | Val 12269 | Test 2592 | H=30 seg\n",
            "[Fold S7] Train 47136 | Val 11861 | Test 4638 | H=30 seg\n",
            "[Fold S8] Train 47641 | Val 11987 | Test 4007 | H=30 seg\n",
            "[Fold S9] Train 47449 | Val 11939 | Test 4247 | H=30 seg\n",
            "[Fold S10] Train 46612 | Val 11732 | Test 5291 | H=30 seg\n",
            "[Fold S11] Train 47257 | Val 11891 | Test 4491 | H=30 seg\n",
            "[Fold S12] Train 47706 | Val 12005 | Test 3924 | H=30 seg\n",
            "[Fold S13] Train 47217 | Val 11883 | Test 4535 | H=30 seg\n",
            "[Fold S14] Train 47288 | Val 11901 | Test 4446 | H=30 seg\n",
            "[Fold S15] Train 47697 | Val 12002 | Test 3936 | H=30 seg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Function"
      ],
      "metadata": {
        "id": "8DaRwsU-awS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test(model, test_loader, device=None):\n",
        "\n",
        "  device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.eval().to(device)\n",
        "\n",
        "  total, sum_now, sum_fut = 0, 0.0, 0.0\n",
        "\n",
        "  # loop\n",
        "  for xb, y_now, y_fut in test_loader:\n",
        "    # to device\n",
        "    xb = xb.to(device, non_blocking=True)\n",
        "    y_now = y_now.to(device, non_blocking=True)\n",
        "    y_fut = y_fut.to(device, non_blocking=True)\n",
        "\n",
        "    # forward prop\n",
        "    y_now_pred, y_fut_pred = model(xb)\n",
        "\n",
        "    # find MAE loss and accumulate all losses\n",
        "    sum_now += torch.nn.functional.l1_loss(y_now_pred, y_now, reduction=\"sum\").item()\n",
        "    sum_fut += torch.nn.functional.l1_loss(y_fut_pred, y_fut, reduction=\"sum\").item()\n",
        "    total += xb.size(0)\n",
        "\n",
        "  return {\"mae_loss\": (sum_now / total) + (sum_fut / total), \"mae_now\": sum_now / total, \"mae_fut\": sum_fut / total}"
      ],
      "metadata": {
        "id": "hZxlGtAla0Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S1"
      ],
      "metadata": {
        "id": "Q-Y6RGvWd6T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S1 = evaluate_test(models[1], folds[1][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S1] \\nmae_loss: {test_metrics_S1[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S1[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S1[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Z3iDxjd9IU",
        "outputId": "5e6e1e36-ee44-46e4-ccf3-c59b74400690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S1] \n",
            "mae_loss: 24.0986\n",
            "mae_now: 11.3743\n",
            "mae_fut: 12.7242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S2"
      ],
      "metadata": {
        "id": "rWSC9y7Zn_oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S2 = evaluate_test(models[2], folds[2][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S2] \\nmae_loss: {test_metrics_S2[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S2[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S2[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSpUnH87oBO2",
        "outputId": "50468f78-84e1-4f03-bf72-6c3a590072ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S2] \n",
            "mae_loss: 31.0810\n",
            "mae_now: 15.4181\n",
            "mae_fut: 15.6629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S3"
      ],
      "metadata": {
        "id": "FbQ1gLrOoSEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S3 = evaluate_test(models[3], folds[3][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S3] \\nmae_loss: {test_metrics_S3[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S3[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S3[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7YUp9NxoN21",
        "outputId": "32126c0e-de98-44cd-deae-9aff05cbc3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S3] \n",
            "mae_loss: 23.8951\n",
            "mae_now: 12.0433\n",
            "mae_fut: 11.8518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S4"
      ],
      "metadata": {
        "id": "I_RoZ1f5p1lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S4 = evaluate_test(models[4], folds[4][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S4] \\nmae_loss: {test_metrics_S4[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S4[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S4[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLFHwUlYod0a",
        "outputId": "6d35b7fb-4303-4bfb-b3eb-b9b1ceb440b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S4] \n",
            "mae_loss: 23.7217\n",
            "mae_now: 11.4469\n",
            "mae_fut: 12.2749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S5 - worst performing\n",
        "\n",
        "mae_loss of 59.0583 indicates that something about S5's activity data is very different to other subjects - something unusual physiologically? or maybe didn't participate in the activities as properly..."
      ],
      "metadata": {
        "id": "lhpFnWgQp2sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S5 = evaluate_test(models[5], folds[5][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S5] \\nmae_loss: {test_metrics_S5[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S5[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S5[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR7qKrEvotGy",
        "outputId": "18697dbc-f21e-4d49-8d4e-5c93ad45ec22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S5] \n",
            "mae_loss: 59.0583\n",
            "mae_now: 28.3316\n",
            "mae_fut: 30.7267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S6\n",
        "\n",
        "- same as S5; S6 reports '1' fitness level"
      ],
      "metadata": {
        "id": "VI9AApWkp3eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S6 = evaluate_test(models[6], folds[6][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S6] \\nmae_loss: {test_metrics_S6[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S6[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S6[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVCmNOlFouAV",
        "outputId": "c1d77d2d-d9c3-40a2-9533-7803bc21fbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S6] \n",
            "mae_loss: 57.0260\n",
            "mae_now: 28.2792\n",
            "mae_fut: 28.7468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S7"
      ],
      "metadata": {
        "id": "Yu0wudESp4RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S7 = evaluate_test(models[7], folds[7][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S7] \\nmae_loss: {test_metrics_S7[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S7[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S7[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGvjecLYo8Th",
        "outputId": "af1602a4-a231-4d1a-c7cb-eec134150849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S7] \n",
            "mae_loss: 21.3194\n",
            "mae_now: 10.1404\n",
            "mae_fut: 11.1790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S8"
      ],
      "metadata": {
        "id": "-IMiwArip491"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S8 = evaluate_test(models[8], folds[8][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S8] \\nmae_loss: {test_metrics_S8[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S8[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S8[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vv2ArpIo8RJ",
        "outputId": "5a729f93-ba3d-42f5-c1ce-445658e5b504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S8] \n",
            "mae_loss: 23.5757\n",
            "mae_now: 11.0020\n",
            "mae_fut: 12.5737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S9 - best performing\n",
        "\n",
        "S9's physiology seems to best represent the variation amongst the 14 other subjects"
      ],
      "metadata": {
        "id": "0idwfkivp5rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S9 = evaluate_test(models[9], folds[9][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S9] \\nmae_loss: {test_metrics_S9[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S9[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S9[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJHss6Y8o8Oe",
        "outputId": "eeaab06c-43ec-4362-d8dc-0f5cc211d235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S9] \n",
            "mae_loss: 20.4885\n",
            "mae_now: 9.7095\n",
            "mae_fut: 10.7790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S10"
      ],
      "metadata": {
        "id": "5-POhhlRp6Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S10 = evaluate_test(models[10], folds[10][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S10] \\nmae_loss: {test_metrics_S10[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S10[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S10[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJpOMn5Jo8He",
        "outputId": "80198f31-45e1-44d2-ffad-e5d600107070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S10] \n",
            "mae_loss: 25.3724\n",
            "mae_now: 12.0220\n",
            "mae_fut: 13.3505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S11"
      ],
      "metadata": {
        "id": "wjZAUqzMp7PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S11 = evaluate_test(models[11], folds[11][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S11] \\nmae_loss: {test_metrics_S11[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S11[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S11[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEGXAAQapVr1",
        "outputId": "1a632e62-7ff9-4d23-b6fa-89afe090afb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S11] \n",
            "mae_loss: 35.8827\n",
            "mae_now: 17.7575\n",
            "mae_fut: 18.1252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S12"
      ],
      "metadata": {
        "id": "mcR5dWgSp73S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S12 = evaluate_test(models[12], folds[12][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S12] \\nmae_loss: {test_metrics_S12[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S12[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S12[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V_mUfclpcL9",
        "outputId": "5ff05b84-bc0a-45e4-b8b9-e8fc9f3d81fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S12] \n",
            "mae_loss: 47.7843\n",
            "mae_now: 23.8630\n",
            "mae_fut: 23.9213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S13\n",
        "\n",
        "- same as S9"
      ],
      "metadata": {
        "id": "fblR3I_Mp8na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S13 = evaluate_test(models[13], folds[13][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S13] \\nmae_loss: {test_metrics_S13[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S13[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S13[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QakVnQd8pcEe",
        "outputId": "6b0fda43-ddb3-47b7-be40-5f43ff832804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S13] \n",
            "mae_loss: 20.8863\n",
            "mae_now: 8.6240\n",
            "mae_fut: 12.2623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S14"
      ],
      "metadata": {
        "id": "FzgQPPetp9ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S14 = evaluate_test(models[14], folds[14][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S14] \\nmae_loss: {test_metrics_S14[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S14[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S14[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsE6upojpmi9",
        "outputId": "bbfa518d-fd04-4683-b236-c62b92d41e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S14] \n",
            "mae_loss: 28.6058\n",
            "mae_now: 13.8261\n",
            "mae_fut: 14.7797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: testing on S15"
      ],
      "metadata": {
        "id": "Sde9qFDip-Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_S15 = evaluate_test(models[15], folds[15][\"test_loader\"], device)\n",
        "\n",
        "print(f\"[TEST S15] \\nmae_loss: {test_metrics_S15[\"mae_loss\"]:.4f}\\nmae_now: {test_metrics_S15[\"mae_now\"]:.4f}\\nmae_fut: {test_metrics_S15[\"mae_fut\"]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tOYCt7fpsXc",
        "outputId": "fcd89cd2-3952-40b5-e935-a27a7c459087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST S15] \n",
            "mae_loss: 25.9765\n",
            "mae_now: 12.8689\n",
            "mae_fut: 13.1076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------\n",
        "## EVALUATION SET RESULTS (bpm)\n",
        "\n",
        "### Average test_MAE: 31.2515 (std 12.5022)\n",
        "### Average test now_MAE: 15.1138 (std 6.3127)\n",
        "### Average test fut_MAE: 16.1377 (std 6.2255)\n",
        "\n",
        "*no outliers as per Grubbs' test*\n",
        "\n",
        "Thoughts:\n",
        "- as expected, test_MAE is higher than val_MAE.\n",
        "\n",
        "- average test now_MAE is 15.1138, which is out of reported MAE range by consumer wearables (~ 7-10 bpm).\n",
        "\n",
        "% of time in activities of...\n",
        "\n",
        "- Rest (low motion, low artefacts): 40 mins -> 26.7%\n",
        "\n",
        "- Moderate (some movement, moderate artefacts): 40 mins -> 26.7%\n",
        "\n",
        "- High (strong artefacts): 23 mins (+ 47 mins from transient) -> 46.7%\n",
        "\n",
        "However, see that nearly half (46.7%) of the data was from high-motion activity, meaning that a now_MAE of 15.1 is more positive given the ~ 10-15 bpm MAE of Consumer Wearables.\n",
        "\n",
        "- Forecasting is a little more difficult, so + circa 1 for fut_MAE is positive as well.\n",
        "\n",
        "Things to consider:\n",
        "- Report per-activity MAE to confirm that it is indeed the high motion activities that are producing the higher MAE and not the lower MAE."
      ],
      "metadata": {
        "id": "OhPryuR9s_dJ"
      }
    }
  ]
}